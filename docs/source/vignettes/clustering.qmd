---
title: "Clustering of count data"
date: "2025-03-27"
format:
    html:
        embed-resources: true
        css: styles.css
        toc: true
        toc-location: left
bibliography: bib.bib
bibliographystyle: apa
execute:
    cache: true
nocite: |
  @joss_bastien
---


# Introduction

The Poisson lognormal (PLN) model can be extended to look for a clustering structure. Two different approaches are available, based on the available data:

- Supervised clustering: when clusters are known for a set of samples (`PlnLDA`)
- Unsupervised clustering: when no clusters is known (`PlnMixture`)


In this example, we analyze single-cell RNA-seq data provided by the
`load_scrna` function in the package. Each column in the dataset
represents a gene, while each row corresponds to a cell (i.e., an
individual). Cell types (`labels`) are also included, which will be our clusters for supervised clustering.
in this example. For simplicity, we limit the analysis to 20 variables (dimensions).

## Importing data

```{python}
from pyPLNmodels import load_scrna
rna = load_scrna(dim=20)
print('Data: ', rna.keys())
```

## Data Structure

### Count Matrix (`endog`)

```{python}
endog = rna["endog"]
print(endog.head())
```

### Cell Type (`labels`)

```{python}
cell_type = rna["labels"]
print('Possible cell types: ', cell_type.unique())
print(cell_type.head())
```

The number of cell types is quite balanced:

```{python}
print('Number of samples per cell type: ', cell_type.value_counts())
```




# Supervised clustering

The supervised way to cluster count data through a `Pln` based model is by
imposing a Linear Discriminant Analysis
([LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis)) on the
latent layer.

## Statistical background

We assume the memberships $c_i$ (i.e. cell types) are known for some samples:

$$
\begin{align}
Z_i &\sim \mathcal{N} \left(X_i^{\top}B + \sum_k \mu_k \mathbf{1}_{\{c_i = k\}}, \Sigma \right),\quad \text{for known cluster memberships } c_i \\
Y_{ij} \mid o_{ij} + Z_{ij} &\sim \mathcal{P}(\exp(o_{ij} + Z_{ij})).
\end{align}
$$

## Train test split

We split the data into train and test to assess the model:

```{python}
train_prop = 0.8
endog_train = endog[:int(endog.shape[0] * train_prop)]
endog_test = endog[int(endog.shape[0] * train_prop):]
cell_type_train = cell_type[:int(endog.shape[0] * train_prop)]
cell_type_test = cell_type[int(endog.shape[0] * train_prop):]
```


## Model fitting on train set

We start by fitting the model on the train set:

```{python}
from pyPLNmodels import PlnLDA
lda = PlnLDA(endog_train, clusters=cell_type_train).fit()
print(lda)
```

## Test set prediction

```{python}
# | echo: False
pred_test = lda.predict_clusters(endog_test)
```

We can now compare with the true labels:

```{python}
from pyPLNmodels import plot_confusion_matrix
plot_confusion_matrix(pred_test, cell_type_test)
```

The model could easily be improved by adding more variables in the dataset.

## Visualization

The visualization of the latent variables, adapted to the LDA model, is
done using the `.viz_transformed` method. It creates sub spaces adapted to the LDA.


```{python}
transformed_test_data = lda.transform_new(endog_test)
lda.viz_transformed(transformed_test_data, colors = cell_type_test)
```

One can also visualize the latent variables of the training set:

```{python}
transformed_train_data = lda.transform_new(endog_train)
lda.viz_transformed(transformed_train_data, colors = cell_type_train)
```

⚠️  Note:  One can add exogenous variables, like this:

```{python}
_ = PlnLDA(endog_train, exog = torch.randn(endog_train.shape[0], 2,) clusters=cell_type_train) # fake covariates
```

⚠️  Note: Ensure that the `exog` matrix has the correct rank, as the model includes an intercept to account for cluster membership. Using one hot encoded covariates will raise an issue.</p>

Note that exogenous variables can also be used in the model, but it will not be
used to separate clusters.


# Unsupervised clustering
When no clusters are known, the `PlnMixture` model captures subpopulation
structure in the data by modeling the latent variables as a mixture of Gaussians. The mixing proportions, denoted
as $\pi = (\pi_1, \pi_2, \dots, \pi_K)$, represent the probabilities of
belonging to each cluster, with the constraint that $\sum_{k=1}^K \pi_k = 1$.

## Statistical background

$$
\begin{align}
c_i | \pi & \sim \mathcal M(1, \pi),\\
Z_i \mid c_i = k & \sim \mathcal{N}(X_i^{\top} B + \mu_k, \Sigma_k),\\
Y_{ij} \mid Z_{ij} &\sim \mathcal{P}(\exp(o_{ij} + Z_{ij})).
\end{align}
$$

To handle high-dimensional data, the covariance matrices $\Sigma_k$ are assumed diagonal.

## Model fitting

We do not need to split the data into train and test sets, as the model is unsupervised. However, we need to choose the number of clusters. We use $3$ clusters for now.


```{python}
from pyPLNmodels import PlnMixture
mixture = PlnMixture(endog, n_cluster=3).fit()
print(mixture)
```

⚠️  Note:  One can add exogenous variables, like this:


```{python}
import torch
_ = PlnMixture(endog, exog = torch.randn(endog.shape[0], 2), n_cluster = 3) # fake covariates
```

⚠️  Note: Ensure that the `exog` matrix has the correct rank, as the model includes an intercept to account for cluster membership. Using one hot encoded covariates will raise an issue.</p>

The weights of the model are stored in the `weights` attribute:

```{python}
weights = mixture.weights
print(weights)
```

## Visualization

The inferred clusters can be accessed through the `clusters` attribute, as well as the probabilities of belonging to each cluster:

```{python}
clusters = mixture.clusters
latent_prob = mixture.latent_prob
```

Some insights on the clusters mean and covariances can be obtained using the `.show()` method:

```{python}
mixture.show()
```

One can visualize the latent variables using the `.viz()` method. By default,
the latent variables are colored by the inferred clusters. The `colors`
argument can be used to use other colors.

```{python}
mixture.viz()
```

One can compare the inferred clusters with the cell types using once again the `plot_confusion_matrix` function:


```{python}
plot_confusion_matrix(clusters, cell_type)
```

## Inferring new samples

One can infer new samples using the `.predict_clusters()` method:

```{python}
new_clusters = mixture.predict_clusters(endog)
```



## Choice of the number of clusters


The number of clusters is a crucial parameter in the `PlnMixture` model.
Fortunately, the package provides a way to fit multiple clusters and compare them, via the `PlnMixtureCollection` class:


```{python}
from pyPLNmodels import PlnMixtureCollection
collection = PlnMixtureCollection(endog, n_clusters=range(2, 5)).fit()
collection.show(figsize = (8,8))
```

The `silhouette` score is a good metric to assess the quality of the clusters,
the higher the better, as opposed to the 'Within Sum of Squares' (WSS) metric.
However, the WSS can only decrease with clusters numbers, as opposed to the
`silhouette` score. Based on the `silhouette` score, we can choose $2$ clusters.
