---
title: "Clustering of count data"
date: "2025-03-27"
format:
  html:
    embed-resources: true
    css: styles.css
    toc: true
    toc-location: left
bibliography: bib.bib
bibliographystyle: apa
execute:
  cache: true
nocite: |
  @joss_bastien
---

# Introduction

The Poisson lognormal (PLN) model can be extended to uncover clustering structures in count data. Two main approaches are available:

- **Supervised clustering**: when cluster labels are known for some samples (`PlnLDA`).
- **Unsupervised clustering**: when cluster labels are unknown (`PlnMixture`).

We illustrate both using a single-cell RNA-seq dataset from the `load_scrna` function. Each row represents a cell and each column a gene. Cell types are included as labels and used in the supervised setting.

# Data Loading and Structure

```{python}
from pyPLNmodels import load_scrna
rna = load_scrna(dim=20)
print("Data keys:", rna.keys())
```

### Count Matrix (`endog`)

```{python}
endog = rna["endog"]
print(endog.head())
```

### Cell Types (`labels`)

```{python}
cell_type = rna["labels"]
print("Cell types:", cell_type.unique())
print(cell_type.head())
print("Sample counts:", cell_type.value_counts())
```

# Supervised Clustering with `PlnLDA`

This method performs a Poisson lognormal discriminant analysis (PLN-LDA), similar to classical LDA but adapted to count data via a latent Gaussian layer.

## Model Formulation

Let $c_i$ denote the known cluster label for sample $i$:

$$
\begin{align}
Z_i| c_i = k &\sim \mathcal{N}(X_i^\top B + \mu_{k}, \Sigma), \\
Y_{ij} &\sim \mathcal{P}(\exp(o_{ij} + Z_{ij}))
\end{align}
$$

## Train-Test Split

```{python}
train_prop = 0.8
n_train = int(endog.shape[0] * train_prop)
endog_train, endog_test = endog[:n_train], endog[n_train:]
cell_type_train, cell_type_test = cell_type[:n_train], cell_type[n_train:]
```

## Fitting the PLN-LDA Model

```{python}
from pyPLNmodels import PlnLDA
lda = PlnLDA(endog_train, clusters=cell_type_train).fit()
print(lda)
```

## Predicting on Test Set

```{python}
pred_test = lda.predict_clusters(endog_test)
```

```{python}
from pyPLNmodels import plot_confusion_matrix
plot_confusion_matrix(pred_test, cell_type_test)
```

You can enhance performance by incorporating additional variables or increasing the number of samples in your dataset.

## Visualizing the Latent Space

The `.transform_new()` method transforms unseen endogenous data into the latent space via LDA.

```{python}
lda.viz_transformed(lda.transform_new(endog_test), colors=cell_type_test)
lda.viz_transformed(lda.transform_new(endog_train), colors=cell_type_train)
```

### Optional: Exogenous Variables

```{python}
import torch
_ = PlnLDA(endog_train, exog=torch.randn(endog_train.shape[0], 2), clusters=cell_type_train)
```

⚠️ Exogenous variables must be full rank, and an intercept will be added to
account for the cluser bias.. Avoid one-hot encodings without intercept removal.

# Unsupervised Clustering with `PlnMixture`

`PlnMixture` fits a latent mixture model to identify subpopulations without labels.

## Model Formulation

$$
\begin{align}
c_i &\sim \mathcal{M}(1, \pi), \\
Z_i \mid c_i=k &\sim \mathcal{N}(X_i^\top B + \mu_k, \Sigma_k), \\
Y_{ij} &\sim \mathcal{P}(\exp(o_{ij} + Z_{ij}))
\end{align}
$$

Covariance matrices $\Sigma_k$ are assumed diagonal.

## Model Fitting

```{python}
from pyPLNmodels import PlnMixture
mixture = PlnMixture(endog, n_cluster=3).fit()
print(mixture)
```

### Optional: With Covariates

```{python}
_ = PlnMixture(endog, exog=torch.randn(endog.shape[0], 2), n_cluster=3)
```

⚠️ Exogenous variables must be full rank, and an intercept will be added to
account for the cluser bias. Avoid one-hot encodings without intercept removal.

### Extracting Weights

```{python}
print(mixture.weights)
```

## Visualizing and Evaluating Clusters

```{python}
clusters = mixture.clusters
mixture.show()
mixture.viz()
plot_confusion_matrix(clusters, cell_type)
```

## Predicting on New Data

```{python}
new_clusters = mixture.predict_clusters(endog)
```

## Use `PlnMixtureCollection` for selecting the optimal number of clusters

To explore multiple cluster solutions, use `PlnMixtureCollection`:

```{python}
from pyPLNmodels import PlnMixtureCollection
collection = PlnMixtureCollection(endog, n_clusters=range(2, 8)).fit()
```

### Evaluate model quality

```{python}
collection.show(figsize=(8, 8))
```

This displays BIC, AIC, log-likelihood, and clustering metric (WCSS, silhouette) for each model.

Use the silhouette score (higher is better) or WCSS (lower is better) to
determine the optimal number of clusters. In this example, 2 clusters may be
appropriate.

⚠️  WSS always decreases with the number of clusters, so that an [ELBOW
method](https://en.wikipedia.org/wiki/Elbow_method_(clustering)) must be used.
The silhouette score is not sensitive to the number of clusters.

### Select the best model

```{python}
best_mixture = collection.best_model("silhouette") # Could be also BIC, AIC, or ICL.
print(best_mixture)
```
