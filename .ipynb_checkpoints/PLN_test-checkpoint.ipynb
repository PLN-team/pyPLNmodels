{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "secret-alcohol",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import gradient_descent, minibatch_class\n",
    "import utils\n",
    "from utils import Poisson_reg\n",
    "from utils import sample_PLN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm \n",
    "import time \n",
    "import sys \n",
    "from __future__ import print_function\n",
    "import psutil\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import scipy.linalg as SLA \n",
    "from scipy.linalg import toeplitz \n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "figured-waste",
   "metadata": {},
   "source": [
    "### Some functions to compare different  models  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "golden-hurricane",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models_batch(modele,modele_batch, batch_size): \n",
    "    '''\n",
    "    takes two models : modele and modele_batch and compare them. They are supposed to be trained.\n",
    "    the batch size is the batch size that has been used to train model_batch. \n",
    "    modele should not have been trained with batches. \n",
    "    '''\n",
    "    n,p = modele.Y.shape\n",
    "    d = modele.beta.shape[0]\n",
    "    ELBO = np.round(modele.current_ELBO,5)\n",
    "    name = 'batch size = {},n = {}, p = {}, d = {}, With batch : last ELBO : {}, time : {}, Without batch : last ELBO : {}, time : {}.jpg'.format(batch_size,\n",
    "                                         n,p,d,  np.round(modele_batch.current_ELBO,1), np.round(modele_batch.running_time,0),  np.round(modele.current_ELBO,1),np.round(modele.running_time,0))\n",
    "    abscisse = np.arange(len(modele.MSE_Sigma_list))\n",
    "    fig,ax = plt.subplots(4,1,figsize = (15,12))\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    plt.suptitle(name)\n",
    "    ax[0].plot(abscisse, modele_batch.MSE_Sigma_list, label = 'MSE with batch')\n",
    "    ax[0].plot(abscisse, modele.MSE_Sigma_list, label = 'MSE without batch')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ybound( lower=0, upper=0.7)\n",
    "    ax[0].set_title('MSE of Sigma. last MSE with batch :{} . last_MSE without batch : {}'.format(np.round(modele_batch.MSE_Sigma_list[-1],5), np.round(modele.MSE_Sigma_list[-1],5)))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax[1].plot(abscisse, modele_batch.MSE_beta_list, label = 'MSE with batch')\n",
    "    ax[1].plot(abscisse, modele.MSE_beta_list, label = 'MSE without batch')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_ybound( lower=0, upper=0.7)\n",
    "    ax[1].set_title('MSE of beta. last MSE with batch :{} . last_MSE without batch : {}'.format(np.round(modele_batch.MSE_beta_list[-1],5), np.round(modele.MSE_beta_list[-1],5)))\n",
    "    \n",
    "\n",
    "    ax[2].plot(abscisse, modele_batch.ELBO_list, label = 'ELBO with batch')\n",
    "    ax[2].plot(abscisse, modele.ELBO_list, label = 'ELBO without batch')\n",
    "    ax[2].legend()\n",
    "    ax[2].set_xscale('log')\n",
    "    ax[2].set_title('ELBO')\n",
    "    \n",
    "    \n",
    "    ax[3].plot(abscisse, np.array(modele.ELBO_list)-np.array(modele_batch.ELBO_list), label =  'ELBO without - ELBO with')\n",
    "    ax[3].legend()\n",
    "    ax[3].set_title('ELBO without batches - ELBO with batches')\n",
    "    ax[3].axhline(y=0, c = 'red')\n",
    "    ax[3].set_ybound( lower=-20, upper=20)\n",
    "    plt.savefig(name)\n",
    "\n",
    "    \n",
    "def compare_models_tol(modele, modele_little,big_tolerance, little_tolerance): \n",
    "    '''\n",
    "    same as above but for models that takes differents tolerance. \n",
    "    \n",
    "    modele_little shoudl have a lower tolerance than modele \n",
    "    '''\n",
    "    n,p = modele.Y.shape\n",
    "    d = modele.beta.shape[0]\n",
    "    ELBO = np.round(modele.current_ELBO,5)\n",
    "    \n",
    "    name = 'big tolerance = {}, smaller tolerance {},n = {}, p = {}, d = {}, precise step : last ELBO : {}, time : {}, inexact step : last ELBO : {}, time : {}.jpg'.format(big_tolerance,little_tolerance, \n",
    "                                         n,p,d,  np.round(modele_little.current_ELBO,1), np.round(modele_little.running_time,0),  np.round(modele.current_ELBO,1),np.round(modele.running_time,0))\n",
    "    abscisse = np.arange(len(modele.MSE_Sigma_list))\n",
    "    fig,ax = plt.subplots(4,1,figsize = (15,12))\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    plt.suptitle(name)\n",
    "    ax[0].plot(abscisse, modele_little.MSE_Sigma_list, label = 'MSE precise step')\n",
    "    ax[0].plot(abscisse, modele.MSE_Sigma_list, label = 'MSE inexact step')\n",
    "    ax[0].legend()\n",
    "    ax[0].set_ybound( lower=0, upper=0.7)\n",
    "    ax[0].set_title('MSE of Sigma. last MSE inexact step :{} . last_MSE precise step : {}'.format(np.round(modele.MSE_Sigma_list[-1],5),\n",
    "                                                                                        np.round(modele_little.MSE_Sigma_list[-1],5)))\n",
    "    \n",
    "    \n",
    "    ax[1].plot(abscisse, modele_little.MSE_beta_list, label = 'MSE precise step')\n",
    "    ax[1].plot(abscisse, modele.MSE_beta_list, label = 'MSE inexact step')\n",
    "    ax[1].legend()\n",
    "    ax[1].set_title('MSE of beta. last MSE inexact step :{} . last_MSE precise step : {}'.format(np.round(modele.MSE_beta_list[-1],5), np.round(modele_little.MSE_beta_list[-1],5)))\n",
    "    \n",
    "\n",
    "    ax[2].plot(abscisse, modele_little.ELBO_list, label = 'ELBO precise step')\n",
    "    ax[2].plot(abscisse, modele.ELBO_list, label = 'ELBO inexact step')\n",
    "    ax[2].legend()\n",
    "    ax[2].set_xscale('log')\n",
    "    ax[2].set_title('ELBO')\n",
    "    \n",
    "    \n",
    "    ax[3].plot(abscisse, np.array(modele.ELBO_list)-np.array(modele_little.ELBO_list), label =  'ELBO inexact step - ELBO precise step')\n",
    "    ax[3].legend()\n",
    "    ax[3].set_title('ELBO inexact step - ELBO precise ')\n",
    "    ax[3].axhline(y=0, c = 'red')\n",
    "    ax[3].set_ybound( lower=-20, upper=20)\n",
    "    plt.savefig(name)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "def print_model(modele): \n",
    "    'print some useful curves of the model modele'\n",
    "    \n",
    "    abscisse = np.arange(len(modele.MSE_Sigma_list))\n",
    "    fig,ax = plt.subplots(4,1,figsize = (12,12))\n",
    "    ax\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    ax[0].plot(abscisse, modele.MSE_Sigma_list)\n",
    "    Sigma_min = np.round(min(modele.MSE_Sigma_list),5)\n",
    "    Sigma_argmin = np.argmin(modele.MSE_Sigma_list)\n",
    "    ax[0].set_title('MSE of Sigma. last MSE :{} . best_MSE {}, argmin = {}'.format(np.round(modele.MSE_Sigma_list[-1],5), Sigma_min, Sigma_argmin))\n",
    "    \n",
    "    ax[1].plot(abscisse, modele.MSE_beta_list)\n",
    "    beta_min = np.round(min(modele.MSE_beta_list),5)\n",
    "    beta_argmin = np.argmin(modele.MSE_beta_list)\n",
    "    ax[1].set_title('MSE of beta. last MSE :{} . best_MSE {}, argmin = {}'.format(np.round(modele.MSE_beta_list[-1],5), beta_min, beta_argmin))\n",
    "    \n",
    "    ax[2].plot(abscisse, modele.ELBO_list)\n",
    "    ax[2].set_xscale('log')\n",
    "    ax[2].set_title('ELBO')\n",
    "    \n",
    "    ax[3].plot(abscisse, modele.MSE_Sigma_list)\n",
    "    ax[3].set_title('MSE of Sigma (x axis in log scale)')\n",
    "    ax[3].set_xscale('log')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "handy-navigator",
   "metadata": {},
   "source": [
    "# PLN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "parallel-patch",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLNmodel(): \n",
    "    '''\n",
    "    PLN model. The goal of this class is to compute the parameter beta and Sigma of the PLN model. \n",
    "    We use here variationnal approximation since we can't compute the log likelihood of the \n",
    "    latent variables given the data. \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    def __init__(self, Sigma_init, beta_init, M_init, S_init): \n",
    "        \n",
    "        '''\n",
    "            Initialization : \n",
    "            'Y' : the data, size (n,p). n is the number of samples we have and p the number of species. \n",
    "                  THE TYPE IS INT\n",
    "            'O': offset : additional offset. (not very important for comprehension). size (n,p)\n",
    "            'covariates' : covariates, size (n,d)\n",
    "            'Sigma_init' : initialization for Sigma. I plan to do a more advanced initialization. \n",
    "            'beta_init ' : Initialization for beta. I plan to do a more advanced initialization. \n",
    "            'M_init' : initialization for the variational parameter M\n",
    "            'S_init ': initialization for the variational parameter S\n",
    "        '''\n",
    "        \n",
    "        # model parameters\n",
    "        self.Sigma = torch.clone(Sigma_init)\n",
    "        #self.Sigma.requires_grad_(True)\n",
    "        self.beta = torch.clone(beta_init)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        #variational parameters\n",
    "        self.M = torch.clone(M_init)\n",
    "        self.M.requires_grad_(True)\n",
    "        self.S = torch.clone(S_init) \n",
    "        self.S.requires_grad_(True)\n",
    "        \n",
    "        # some useful variables\n",
    "        self.det_Sigma = torch.det(self.Sigma)\n",
    "        self.inv_Sigma = torch.inverse(self.Sigma)\n",
    "        \n",
    "        # optimizer for the VE_step\n",
    "        self.VE_step_optimizer = torch.optim.Adam([self.S,self.M], lr = 0.002)\n",
    "        self.VE_step_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.VE_step_optimizer, patience = 3, factor = 0.9)\n",
    "        \n",
    "        #optimizer for the M_step\n",
    "        self.M_step_optimizer = torch.optim.Adam([self.beta], lr = 0.01)\n",
    "        self.M_step_scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(self.M_step_optimizer, patience = 3, factor = 0.9)        \n",
    "        \n",
    "        #optimizer for a full gradient ascent\n",
    "        self.full_optimizer = torch.optim.Adam([self.S,self.M,self.beta])\n",
    "        \n",
    "        # just to stocke the parameters\n",
    "        self.params = {'M': self.M, 'S' : self.S, 'beta' : self.beta}\n",
    "        \n",
    "        # to stock the \n",
    "        self.MSE_Sigma_list = list()\n",
    "        self.MSE_beta_list = list()\n",
    "        self.ELBO_list = list()\n",
    "        \n",
    "        self.time = list()\n",
    "        self.losstime = list()\n",
    "        self.backtime = list()\n",
    "\n",
    "        \n",
    "    # we define here the gradients computed manually as a sanity check. We can check that they are equals \n",
    "    # to the gradients computed with the autodifferentiation of pytorch (diff = 1e-15)\n",
    "    def grad_Sigma(self): \n",
    "        with torch.no_grad():\n",
    "            self.inv_Sigma = torch.inverse(self.Sigma)\n",
    "            grad = -self.n/2*(self.inv_Sigma)# + torch.diag(torch.diagonal(self.inv_Sigma))) on a enlevé car avec ça ca match avec pytorch. \n",
    "            grad += 1/2*(sum([self.inv_Sigma@(torch.outer(self.M[i,:],self.M[i,:])+ torch.diag(self.S[i,:]))@self.inv_Sigma  for i in range(self.n)]))\n",
    "        return grad\n",
    "    def grad_M(self): \n",
    "        with torch.no_grad():\n",
    "            grad = -torch.mm(self.M,self.inv_Sigma)\n",
    "            grad -= torch.exp(self.O + torch.mm(self.covariates,self.beta) + self.M + torch.pow(self.S,2)/2)\n",
    "            grad += self.Y \n",
    "        return grad \n",
    "    def grad_S(self):\n",
    "        with torch.no_grad():\n",
    "            grad = -1/2*torch.mm(torch.ones((self.n,self.p)), torch.diag(torch.diag(self.inv_Sigma)))\n",
    "            grad-= torch.mul(self.S,torch.exp(self.O + torch.mm(self.covariates,self.beta) + self.M + torch.pow(self.S,2)/2))\n",
    "            grad += 1/2*torch.div(1,self.S)\n",
    "        return grad \n",
    "    def grad_beta(self): \n",
    "        with torch.no_grad():\n",
    "            grad = - torch.mm(self.covariates.T,torch.exp( self.O + self.M + torch.pow(self.S,2)/2 + torch.mm(self.covariates,self.beta)))\n",
    "            grad += torch.mm(self.covariates.T,self.Y.double())\n",
    "        return grad \n",
    "    \n",
    "        \n",
    "    def extract_data(self,data): \n",
    "        '''\n",
    "        function to extract the data. This function is just here to have a code more compact. \n",
    "        \n",
    "        args : \n",
    "              'data': list with 3 elements : Y, O and covariates in this order. \n",
    "        '''\n",
    "        #known variables\n",
    "        self.Y = data[0];self.O = data[1];self.covariates = data[2]\n",
    "        self.n, self.p = self.Y.shape\n",
    "    \n",
    "    def train_step(self, optimizer, Y_,covariates_, O_,M_,S_,Sigma_, beta_): \n",
    "        '''\n",
    "        do one step on gradient ascent. We will optimize only the parametrs of the optimizer. \n",
    "        \n",
    "        args : \n",
    "                'optimizer' torch.optim.optimizer object. this function will update the parameters of this optimizer. \n",
    "                \n",
    "                'Y_' size (batch_size, p). \n",
    "                'covariates' size (batch_size, d)\n",
    "                'O_' size (batch_size, p)\n",
    "                'M_' size (batch_size, p)\n",
    "                'S_' size (batch_size, p)\n",
    "                'Sigma_' size (p,p)\n",
    "                'beta_' size(batch_size, p)\n",
    "                \n",
    "        returns : \n",
    "        \n",
    "                'loss' float. the loss computed. correspond to -ELBO \n",
    "        '''\n",
    "        optimizer.zero_grad()\n",
    "        loss = -self.compute_ELBO(Y_, covariates_, O_, M_, S_,Sigma_,beta_)\n",
    "        loss.backward()\n",
    "        if torch.isnan(loss).item() == True : \n",
    "            print('NAN')\n",
    "        else : self.last_param = self.params \n",
    "        optimizer.step()\n",
    "        return loss \n",
    "\n",
    "   \n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent. the batches are generate for the\n",
    "        variables Y covariates 0 M and S (only those who depends on n). It is a generator to handle memory. \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one since the rest of the division is\n",
    "                    not always an integer)\n",
    "        '''\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        nb_full_batch, last_batch_size  = self.n//batch_size, self.n % batch_size  \n",
    "        self.batch_size = batch_size\n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*batch_size: (i+1)*batch_size]], \n",
    "                    self.covariates[indices[i*batch_size: (i+1)*batch_size]],\n",
    "                    self.O[indices[i*batch_size: (i+1)*batch_size]], \n",
    "                    self.M[indices[i*batch_size: (i+1)*batch_size]], \n",
    "                    self.S[indices[i*batch_size: (i+1)*batch_size]])\n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]],\n",
    "                    self.M[indices[-last_batch_size:]], \n",
    "                    self.S[indices[-last_batch_size:]])\n",
    "            \n",
    "    \n",
    "\n",
    "    def print_stats(self, loss, params, optimizer): \n",
    "        '''\n",
    "        small function that print some stats. \n",
    "        \n",
    "        It will print the actual learning rate of the optimizer, the actual log likelihood \n",
    "        and the norms of each parameter's gradient. The norm of the parameter's gradient should be low\n",
    "        when we are close to the optimum. \n",
    "        '''\n",
    "        print('---------------------------------lr :', optimizer.param_groups[0]['lr'])\n",
    "        print('---------------------------------log likelihood :', - loss.item())\n",
    "        for param_name, param in params.items(): \n",
    "            print('---------------------------------grad_{}_norm : '.format(param_name), round(torch.norm(param.grad).item(), 3))\n",
    "    \n",
    "    \n",
    "    def VEM(self,data, number_VEM_step ,batch_size,   tolerance = 0.1, beginning_VE_step_lr = 0.002, beginning_M_step_lr = 0.01, \n",
    "                requires_init = False,N_epoch_VE = 50, N_epoch_M = 75, verbose = False): \n",
    "        '''\n",
    "        function to optimize both the variational parameters and the model parameters.\n",
    "        We alternate between two steps : Variational step (VE_step) and Maximization step (M_step). \n",
    "        \n",
    "        \n",
    "        args : \n",
    "            'number_VEM_step' : int . Number of times we want to do the VEM step, i.e. alternate between VE step and M step. \n",
    "                                The greater the better the approximation, the greater the longer time it takes. \n",
    "            \n",
    "            'beginning_VE_step_lr' : float. the beginning of the learning for the VE_step. The VE will start with this lr. \n",
    "            'beginning_M_step_lr' : float. Same for beta, the M step will start with this lr. \n",
    "            \n",
    "       returns : \n",
    "               M_S_lr, beta_lr : the learning rates of both steps, so that we can continue after that with the appropriate learning rates.  \n",
    "        ''' \n",
    "        \n",
    "        self.running_time = time.time()\n",
    "        \n",
    "        # we first extract the data. \n",
    "        self.extract_data(data)\n",
    "        \n",
    "        self.MSE_Sigma_list.append(torch.mean((self.Sigma-true_Sigma)**2).item())\n",
    "        self.MSE_beta_list.append(torch.mean((self.beta-true_beta)**2).item())\n",
    "        self.ELBO_list.append(1)\n",
    "        \n",
    "        if requires_init : \n",
    "            print('Initialisation ... ')\n",
    "            clf = Poisson_reg()\n",
    "            for j in range(p): \n",
    "                Y_j = self.Y[:,j]   \n",
    "                O_j = self.O[:,j]\n",
    "                \n",
    "                clf.fit_torch(O_j,self.covariates,Y_j, verbose = False , Niter_max = 500, lr = 0.1)\n",
    "                with torch.no_grad():\n",
    "                    self.beta[:,j] = clf.beta\n",
    "            print('Initialisation finished')\n",
    "            \n",
    "        # we do as many VEM_step we are asked to. \n",
    "        for i in range(number_VEM_step): \n",
    "            #M STEP-------UPDATE------- None N_epoch :VE  100\n",
    "            # closed form for Sigma, we don't need to optimize\n",
    "            with torch.no_grad(): \n",
    "                self.Sigma = 1/self.n*(torch.sum(torch.stack([torch.outer(self.M[i,:],self.M[i,:]) + torch.diag(self.S[i,:])  for i in range(self.n)]), axis = 0))\n",
    "                #self.beta = torch.mm(torch.inverse(torch.mm(self.covariates.T,self.covariates)),torch.mm(self.covariates.T,self.M))\n",
    "            #gradient ascent for beta \n",
    "            self.torch_gradient_ascent(self.M_step_optimizer, self.M_step_scheduler, \n",
    "                                       lr = beginning_M_step_lr, tolerance = tolerance, N_epoch= N_epoch_M, verbose= verbose, batch_size = batch_size )\n",
    "            #VE STEP\n",
    "            #gradient ascent for M and S \n",
    "            self.torch_gradient_ascent(self.VE_step_optimizer, self.VE_step_scheduler, lr = beginning_VE_step_lr,\n",
    "                                    tolerance= tolerance, N_epoch= N_epoch_VE, verbose= verbose)\n",
    "            \n",
    "            # we append the MSE to keep track of the progression. Note that true_Sigma and true_betais unknown in practice. \n",
    "            self.MSE_Sigma_list.append(torch.mean((self.Sigma-true_Sigma)**2).item())\n",
    "            self.MSE_beta_list.append(torch.mean((self.beta-true_beta)**2).item())\n",
    "            self.ELBO_list.append(self.current_ELBO)\n",
    "            if i %  100 == 0 : \n",
    "                print('-------UPDATE-------')\n",
    "                print(' MSE with Sigma : ', np.round(self.MSE_Sigma_list[-1],5))\n",
    "                print(' MSE with beta : ', np.round(self.MSE_beta_list[-1],5))\n",
    "                print('ELBO : ', np.round(self.current_ELBO,5))\n",
    "        \n",
    "        #keep track of the runningtime \n",
    "        self.running_time = time.time()- self.running_time\n",
    "        return self.VE_step_optimizer.param_groups[0]['lr'],self.M_step_optimizer.param_groups[0]['lr']        \n",
    "    \n",
    "    def torch_gradient_ascent(self,optimizer, scheduler, lr = None, tolerance = 2, N_epoch = 500, verbose = True, batch_size = None ): \n",
    "        '''\n",
    "        gradient ascent function. We compute the gradients thanks to the autodifferentiation of pytorch. \n",
    "        \n",
    "        args : \n",
    "                'optimizer' : torch.optim.optimizer. the optimizer for the parameters. \n",
    "                'scheduler' : torch.optim.lr_scheduler.  scheduler for the optimizer above. \n",
    "                \n",
    "                'lr' : float.  a learning rate if we want to set the optimizer learning rate to a certain lr. If None, \n",
    "                      it will take the actual learning_rate of the optimizer. \n",
    "                'tolerance': float. the threshold we set to stop the algorithm. It will stop if the norm of each gradient's parameter \n",
    "                             is lower than this threshold, or if we are not improving the loss more than tolerance. \n",
    "                'N_epoch': int. the Maximum number of epoch we are ready to do. \n",
    "                \n",
    "                'Verbose' : bool. if True, will print some messages useful to interpret the gradient ascent. If False, nothing will be printed. \n",
    "                \n",
    "                'batch_size' : int or None. If None, the batch size will be n, so it will be a classical vanilla algorithm. \n",
    "                              if int, we will split the data set in batch size and do a gradient step for each mini_batch. \n",
    "                              \n",
    "        \n",
    "        returns : the parameters optimized. \n",
    "        '''\n",
    "        \n",
    "        \n",
    "        # we set the gradient to zero just to make sure the gradients are properly calculated\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if lr is not None : # if we want to set a threshold, we set it. Ohterwise, we skip this condition and keep the actual learning_rate\n",
    "            optimizer.param_groups[0]['lr'] = lr \n",
    "            \n",
    "        #if batch_size is None, we take n. \n",
    "        if batch_size == None : \n",
    "            batch_size = self.Y.shape[0]\n",
    "        \n",
    "        stop_condition = False \n",
    "        i = 0\n",
    "        old_epoch_loss = 1.\n",
    "        \n",
    "        while i < N_epoch and stop_condition == False: \n",
    "            epoch_loss = 0.\n",
    "            # we run through the whole dataset. if batch_size was None, this loop contains only one element. \n",
    "            t0 = time.time()\n",
    "            for Y_b, covariates_b, O_b, M_b, S_b in self.get_batch(batch_size): \n",
    "                epoch_loss += self.train_step(optimizer, Y_b, covariates_b, O_b, M_b, S_b, self.Sigma,self.beta)\n",
    "            self.time.append(time.time()-t0)\n",
    "            if verbose and i % 25 == 0 : \n",
    "                self.print_stats(epoch_loss, self.params, optimizer)\n",
    "            i += 1\n",
    "            scheduler.step(epoch_loss)\n",
    "            # condition to see if we have reach the tolerance threshold\n",
    "            if  abs(epoch_loss.item() - old_epoch_loss) < tolerance : #and i > 10 and max([torch.norm(param.grad) for param in params]) < tolerance  or\n",
    "                #if max([torch.norm(param.grad) for param in params]) < tolerance  or abs(loss.item()- old_loss)>  tolerance :\n",
    "                stop_condition = True \n",
    "            old_epoch_loss = epoch_loss\n",
    "            \n",
    "        #keep track of the ELBO \n",
    "        self.current_ELBO = -epoch_loss.item()\n",
    "        \n",
    "        if verbose : # just print some stats if we want to \n",
    "            if stop_condition : \n",
    "                print('---------------------------------Tolerance {} reached in {} iterations'.format(tolerance, i))\n",
    "            else : \n",
    "                print('---------------------------------Maximum number of iterations reached : ', N_epoch)\n",
    "            self.print_stats(epoch_loss, self.params, optimizer)\n",
    "        return self.last_param\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    def compute_ELBO(self, Y, covariates,O,M,S,Sigma,beta): \n",
    "        ''' \n",
    "        computes the ELBO. We simply apply the formula given above. \n",
    "        '''\n",
    "        batch_size,p = Y.shape\n",
    "        \n",
    "        # we store some matrices to avoid computing it two times\n",
    "        inv_Sigma = torch.inverse(Sigma)\n",
    "        Gram_matrix = torch.mm(covariates,beta) \n",
    "        help_calculus = O + Gram_matrix + M \n",
    "        tmp = -batch_size/2*torch.log(torch.det(Sigma)) #-1/2*( torch.sum(torch.mm(torch.mm(M,inv_Sigma),M.T).diagonal()))\n",
    "        \n",
    "        #term = -torch.exp(help_calculus+ torch.pow(S,2)/2)\n",
    "        #mask = torch.isnan(term)\n",
    "        #print('nb elements nonzero', torch.sum(mask))\n",
    "        tmp += torch.sum(-torch.exp(help_calculus+ torch.pow(S,2)/2) + torch.multiply(Y, help_calculus))\n",
    "        tmp -= 1/2*torch.trace(torch.mm(torch.mm(M.T, M) + torch.diag(torch.sum(S, dim = 0)), inv_Sigma))\n",
    "        #tmp-= batch_size*p/2\n",
    "        tmp += 1/2*torch.sum(torch.log(torch.prod(S, dim = 1)))\n",
    "        return tmp\n",
    "\n",
    "    def full_grad_ascent(self, data, lr = None, tolerance = 2, N_iter = 500, verbose = True): \n",
    "        '''\n",
    "        gradient ascent function. We compute the gradients thanks to the autodifferentiation of pytorch. \n",
    "        \n",
    "        args :  'data' the data i.e. a list of 3 elements Y O and covariates in this order\n",
    "                \n",
    "                'lr' : float.  a learning rate if we want to set the optimizer learning rate to a certain lr. If None, \n",
    "                      it will take the actual learning_rate of the optimizer. \n",
    "                'tolerance': float. the threshold we set to stop the algorithm. It will stop if the norm of each gradient's parameter \n",
    "                             is lower than this threshold, or if we are not improving the loss more than tolerance. \n",
    "                'N_iter': int. the Maximum number of iterations we are ready to do. \n",
    "                \n",
    "                'Verbose' : bool. if True, will print some messages useful to interpret the gradient ascent. If False, nothing will be printed. \n",
    "\n",
    "        \n",
    "        returns : the parameters optimized. \n",
    "        '''\n",
    "        self.extract_data(data)\n",
    "        \n",
    "        # we set the gradient to zero just to make sure the gradients are properly calculated\n",
    "        self.full_optimizer.zero_grad()\n",
    "        \n",
    "        if lr is not None : # if we want to set a threshold, we set it. Ohterwise, we skip this condition and keep the actual learning_rate\n",
    "            self.full_optimizer.param_groups[0]['lr'] = lr \n",
    "            \n",
    "        \n",
    "        stop_condition = False \n",
    "        i = 0\n",
    "        old_loss = 0 \n",
    "        \n",
    "        while i < N_iter and stop_condition == False: \n",
    "            \n",
    "            loss = self.train_step(self.full_optimizer, self.Y, self.covariates, self.O, self.M, self.S, self.Sigma, self.beta)\n",
    "            i += 1 \n",
    "            \n",
    "            # condition to see if we have reach the tolerance threshold\n",
    "            if max([torch.norm(param.grad) for param in self.params.values()]) < tolerance  or abs(loss - old_loss) < tolerance : #and i > 10:\n",
    "                stop_condition = True \n",
    "            old_loss = loss \n",
    "            \n",
    "            #update Sigma with the closed form. \n",
    "            with torch.no_grad(): \n",
    "                self.Sigma = 1/self.n*(torch.sum(torch.stack([torch.outer(self.M[i,:],self.M[i,:]) + torch.diag(self.S[i,:])  for i in range(self.n)]), axis = 0))\n",
    "            \n",
    "            self.MSE_Sigma_list.append(torch.mean((self.Sigma-true_Sigma)**2).item())\n",
    "            self.MSE_beta_list.append(torch.mean((self.beta-true_beta)**2).item())\n",
    "            self.ELBO_list.append(-loss)\n",
    "            \n",
    "            if verbose and i % 100 == 0 : \n",
    "                print('iteration number: ', i)\n",
    "                self.print_stats(loss, self.params, self.full_optimizer)\n",
    "                print('-------UPDATE-------')\n",
    "                print(' MSE with Sigma : ', np.round(self.MSE_Sigma_list[-1],5))\n",
    "                print(' MSE with beta : ', np.round(self.MSE_beta_list[-1],5))\n",
    "                print('ELBO : ', np.round(-loss.item(),5))\n",
    "            \n",
    "\n",
    "\n",
    "        if verbose : # just print some stats if we want to \n",
    "            if stop_condition : \n",
    "                print('---------------------------------Tolerance reached in {} iterations'.format(i))\n",
    "            else : \n",
    "                print('---------------------------------Maximum number of iterations reached')\n",
    "            self.print_stats(loss,self.params, self.full_optimizer)   \n",
    "        \n",
    "        return self.params\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forty-ecuador",
   "metadata": {},
   "source": [
    "## Generation des données. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "negative-collaboration",
   "metadata": {},
   "outputs": [],
   "source": [
    "#extraction des données \n",
    "Y_tricho = torch.from_numpy(read_csv('trichoptera.csv', sep=',').to_numpy())\n",
    "O_tricho = torch.outer(Y_tricho.sum(1), torch.ones(Y_tricho.shape[1]))/1000\n",
    "\n",
    "n_tricho,p_tricho = Y_tricho.shape \n",
    "d_tricho = 4\n",
    "covariates_tricho = torch.ones((n_tricho,d_tricho)) # attention prendre que des 1 n'est pas une bonne solution\n",
    "\n",
    "data_tricho = [Y_tricho,O_tricho, covariates_tricho]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "mounted-movement",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 1000; p = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "tested-quick",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Sigma = torch.from_numpy(toeplitz(0.5**np.arange(p)))\n",
    "true_beta = torch.randn(d, p)\n",
    "\n",
    "covariates = torch.rand((n,d))\n",
    "O = 1 + torch.zeros((n,p))\n",
    "\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled = torch.from_numpy(sample_model.sample(true_Sigma,true_beta, O, covariates)) \n",
    "\n",
    "data = [Y_sampled, O, covariates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "domestic-centre",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "noise = torch.randn(p,p) \n",
    "Sigma_init =  (noise+ noise.T)\n",
    "beta_init = torch.rand((d, p))\n",
    "\n",
    "M_init = torch.ones((n,p))/100# some random values to initialize we divide to avoid nan values \n",
    "S_init = torch.ones((n,p))/8 # some random values to initializ. we divise to avoid nan values "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chronic-implement",
   "metadata": {},
   "source": [
    "# Inférence"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "placed-resort",
   "metadata": {},
   "source": [
    "### VEM algorithm without batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "freelance-singapore",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.04674\n",
      " MSE with beta :  1.00881\n",
      "ELBO :  102643.77374\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.20796\n",
      " MSE with beta :  0.62711\n",
      "ELBO :  108403.99089\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.17093\n",
      " MSE with beta :  0.5514\n",
      "ELBO :  108420.54251\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.143\n",
      " MSE with beta :  0.49875\n",
      "ELBO :  108432.7208\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.12458\n",
      " MSE with beta :  0.45573\n",
      "ELBO :  108441.75705\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.11345\n",
      " MSE with beta :  0.41449\n",
      "ELBO :  108447.88156\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.10679\n",
      " MSE with beta :  0.37449\n",
      "ELBO :  108451.79511\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.10255\n",
      " MSE with beta :  0.33816\n",
      "ELBO :  108454.39096\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09971\n",
      " MSE with beta :  0.30794\n",
      "ELBO :  108456.23308\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09761\n",
      " MSE with beta :  0.28266\n",
      "ELBO :  108457.57262\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09626\n",
      " MSE with beta :  0.2638\n",
      "ELBO :  108458.46784\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.0953\n",
      " MSE with beta :  0.2505\n",
      "ELBO :  108459.1441\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09483\n",
      " MSE with beta :  0.23945\n",
      "ELBO :  108459.65625\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09432\n",
      " MSE with beta :  0.2335\n",
      "ELBO :  108459.53982\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09448\n",
      " MSE with beta :  0.22974\n",
      "ELBO :  108459.74735\n",
      "CPU times: user 47.8 s, sys: 43.1 ms, total: 47.8 s\n",
      "Wall time: 12 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.02, 0.1)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = PLNmodel(Sigma_init, beta_init, M_init, S_init)\n",
    "%time model.VEM(data, number_VEM_step = 1500,tolerance= 0.1, batch_size = None, requires_init = False, N_epoch_VE = 100, N_epoch_M = 100, beginning_VE_step_lr=0.02, beginning_M_step_lr = 0.1 , verbose = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "established-trademark",
   "metadata": {},
   "source": [
    "### gradient descent on all the parameters \n",
    "\n",
    "(except $\\Sigma$ that is updated with a closed form) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "plain-weekly",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration number:  100\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 106346.1562092691\n",
      "---------------------------------grad_M_norm :  822.822\n",
      "---------------------------------grad_S_norm :  4.151\n",
      "---------------------------------grad_beta_norm :  507.626\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.22919\n",
      " MSE with beta :  0.9146\n",
      "ELBO :  106346.15621\n",
      "iteration number:  200\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108174.20814725183\n",
      "---------------------------------grad_M_norm :  41.819\n",
      "---------------------------------grad_S_norm :  1.642\n",
      "---------------------------------grad_beta_norm :  33.075\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.53715\n",
      " MSE with beta :  0.7801\n",
      "ELBO :  108174.20815\n",
      "iteration number:  300\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108265.62070756617\n",
      "---------------------------------grad_M_norm :  20.042\n",
      "---------------------------------grad_S_norm :  1.7\n",
      "---------------------------------grad_beta_norm :  23.35\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.62568\n",
      " MSE with beta :  0.68566\n",
      "ELBO :  108265.62071\n",
      "iteration number:  400\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108300.02907440298\n",
      "---------------------------------grad_M_norm :  14.327\n",
      "---------------------------------grad_S_norm :  2.327\n",
      "---------------------------------grad_beta_norm :  20.332\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.65595\n",
      " MSE with beta :  0.61782\n",
      "ELBO :  108300.02907\n",
      "iteration number:  500\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108320.73804092748\n",
      "---------------------------------grad_M_norm :  12.01\n",
      "---------------------------------grad_S_norm :  0.505\n",
      "---------------------------------grad_beta_norm :  18.388\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.65591\n",
      " MSE with beta :  0.56576\n",
      "ELBO :  108320.73804\n",
      "iteration number:  600\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108335.44454568024\n",
      "---------------------------------grad_M_norm :  10.765\n",
      "---------------------------------grad_S_norm :  2.839\n",
      "---------------------------------grad_beta_norm :  16.797\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.63722\n",
      " MSE with beta :  0.52474\n",
      "ELBO :  108335.44455\n",
      "iteration number:  700\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108346.70439050611\n",
      "---------------------------------grad_M_norm :  10.01\n",
      "---------------------------------grad_S_norm :  1.656\n",
      "---------------------------------grad_beta_norm :  15.475\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.60616\n",
      " MSE with beta :  0.49156\n",
      "ELBO :  108346.70439\n",
      "iteration number:  800\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108355.84914572726\n",
      "---------------------------------grad_M_norm :  9.471\n",
      "---------------------------------grad_S_norm :  4.042\n",
      "---------------------------------grad_beta_norm :  14.434\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.56691\n",
      " MSE with beta :  0.46386\n",
      "ELBO :  108355.84915\n",
      "iteration number:  900\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108363.74664288035\n",
      "---------------------------------grad_M_norm :  9.111\n",
      "---------------------------------grad_S_norm :  2.405\n",
      "---------------------------------grad_beta_norm :  13.612\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.52264\n",
      " MSE with beta :  0.43997\n",
      "ELBO :  108363.74664\n",
      "iteration number:  1000\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108370.92791036189\n",
      "---------------------------------grad_M_norm :  8.87\n",
      "---------------------------------grad_S_norm :  3.563\n",
      "---------------------------------grad_beta_norm :  12.991\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.47596\n",
      " MSE with beta :  0.41882\n",
      "ELBO :  108370.92791\n",
      "iteration number:  1100\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108377.73238511452\n",
      "---------------------------------grad_M_norm :  8.744\n",
      "---------------------------------grad_S_norm :  3.844\n",
      "---------------------------------grad_beta_norm :  12.437\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.42893\n",
      " MSE with beta :  0.39978\n",
      "ELBO :  108377.73239\n",
      "iteration number:  1200\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108384.35136018645\n",
      "---------------------------------grad_M_norm :  8.623\n",
      "---------------------------------grad_S_norm :  1.477\n",
      "---------------------------------grad_beta_norm :  12.067\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.38313\n",
      " MSE with beta :  0.3825\n",
      "ELBO :  108384.35136\n",
      "iteration number:  1300\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108390.86929936524\n",
      "---------------------------------grad_M_norm :  8.534\n",
      "---------------------------------grad_S_norm :  2.869\n",
      "---------------------------------grad_beta_norm :  11.696\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.33974\n",
      " MSE with beta :  0.36675\n",
      "ELBO :  108390.8693\n",
      "iteration number:  1400\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108397.34433012466\n",
      "---------------------------------grad_M_norm :  8.438\n",
      "---------------------------------grad_S_norm :  1.502\n",
      "---------------------------------grad_beta_norm :  11.371\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.29957\n",
      " MSE with beta :  0.35241\n",
      "ELBO :  108397.34433\n",
      "iteration number:  1500\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108403.76565391039\n",
      "---------------------------------grad_M_norm :  8.308\n",
      "---------------------------------grad_S_norm :  1.568\n",
      "---------------------------------grad_beta_norm :  11.033\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.26316\n",
      " MSE with beta :  0.3394\n",
      "ELBO :  108403.76565\n",
      "iteration number:  1600\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108410.09853189178\n",
      "---------------------------------grad_M_norm :  8.136\n",
      "---------------------------------grad_S_norm :  4.776\n",
      "---------------------------------grad_beta_norm :  10.692\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.23088\n",
      " MSE with beta :  0.32767\n",
      "ELBO :  108410.09853\n",
      "iteration number:  1700\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108416.30886123331\n",
      "---------------------------------grad_M_norm :  7.859\n",
      "---------------------------------grad_S_norm :  1.929\n",
      "---------------------------------grad_beta_norm :  10.322\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.20291\n",
      " MSE with beta :  0.31718\n",
      "ELBO :  108416.30886\n",
      "iteration number:  1800\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108422.2679942908\n",
      "---------------------------------grad_M_norm :  7.522\n",
      "---------------------------------grad_S_norm :  2.246\n",
      "---------------------------------grad_beta_norm :  9.942\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.17933\n",
      " MSE with beta :  0.30788\n",
      "ELBO :  108422.26799\n",
      "iteration number:  1900\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108427.86593043253\n",
      "---------------------------------grad_M_norm :  7.103\n",
      "---------------------------------grad_S_norm :  1.903\n",
      "---------------------------------grad_beta_norm :  9.506\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.16002\n",
      " MSE with beta :  0.29969\n",
      "ELBO :  108427.86593\n",
      "iteration number:  2000\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108432.96892176113\n",
      "---------------------------------grad_M_norm :  6.629\n",
      "---------------------------------grad_S_norm :  1.915\n",
      "---------------------------------grad_beta_norm :  9.037\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.14468\n",
      " MSE with beta :  0.29248\n",
      "ELBO :  108432.96892\n",
      "iteration number:  2100\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108437.47407249127\n",
      "---------------------------------grad_M_norm :  6.099\n",
      "---------------------------------grad_S_norm :  2.279\n",
      "---------------------------------grad_beta_norm :  8.541\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.13286\n",
      " MSE with beta :  0.28607\n",
      "ELBO :  108437.47407\n",
      "iteration number:  2200\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108441.32905058961\n",
      "---------------------------------grad_M_norm :  5.547\n",
      "---------------------------------grad_S_norm :  1.721\n",
      "---------------------------------grad_beta_norm :  8.035\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.12395\n",
      " MSE with beta :  0.28024\n",
      "ELBO :  108441.32905\n",
      "iteration number:  2300\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108444.54984970762\n",
      "---------------------------------grad_M_norm :  5.004\n",
      "---------------------------------grad_S_norm :  1.713\n",
      "---------------------------------grad_beta_norm :  7.493\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.11733\n",
      " MSE with beta :  0.27481\n",
      "ELBO :  108444.54985\n",
      "iteration number:  2400\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108447.19298914052\n",
      "---------------------------------grad_M_norm :  4.479\n",
      "---------------------------------grad_S_norm :  3.127\n",
      "---------------------------------grad_beta_norm :  6.943\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.11243\n",
      " MSE with beta :  0.26965\n",
      "ELBO :  108447.19299\n",
      "iteration number:  2500\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108449.36136610224\n",
      "---------------------------------grad_M_norm :  3.994\n",
      "---------------------------------grad_S_norm :  2.051\n",
      "---------------------------------grad_beta_norm :  6.394\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.10878\n",
      " MSE with beta :  0.26468\n",
      "ELBO :  108449.36137\n",
      "iteration number:  2600\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108451.1304767814\n",
      "---------------------------------grad_M_norm :  3.55\n",
      "---------------------------------grad_S_norm :  3.139\n",
      "---------------------------------grad_beta_norm :  5.862\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.10602\n",
      " MSE with beta :  0.25992\n",
      "ELBO :  108451.13048\n",
      "iteration number:  2700\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108452.57808402146\n",
      "---------------------------------grad_M_norm :  3.146\n",
      "---------------------------------grad_S_norm :  3.01\n",
      "---------------------------------grad_beta_norm :  5.367\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.10389\n",
      " MSE with beta :  0.25538\n",
      "ELBO :  108452.57808\n",
      "iteration number:  2800\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108453.77562979385\n",
      "---------------------------------grad_M_norm :  2.766\n",
      "---------------------------------grad_S_norm :  1.956\n",
      "---------------------------------grad_beta_norm :  4.896\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.10221\n",
      " MSE with beta :  0.25111\n",
      "ELBO :  108453.77563\n",
      "iteration number:  2900\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108454.75704309151\n",
      "---------------------------------grad_M_norm :  2.418\n",
      "---------------------------------grad_S_norm :  2.536\n",
      "---------------------------------grad_beta_norm :  4.476\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.10087\n",
      " MSE with beta :  0.24715\n",
      "ELBO :  108454.75704\n",
      "iteration number:  3000\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108455.57090212361\n",
      "---------------------------------grad_M_norm :  2.06\n",
      "---------------------------------grad_S_norm :  5.358\n",
      "---------------------------------grad_beta_norm :  4.114\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09978\n",
      " MSE with beta :  0.24355\n",
      "ELBO :  108455.5709\n",
      "iteration number:  3100\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108456.25678140832\n",
      "---------------------------------grad_M_norm :  1.838\n",
      "---------------------------------grad_S_norm :  1.758\n",
      "---------------------------------grad_beta_norm :  3.75\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09889\n",
      " MSE with beta :  0.24031\n",
      "ELBO :  108456.25678\n",
      "iteration number:  3200\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108456.82458463115\n",
      "---------------------------------grad_M_norm :  1.588\n",
      "---------------------------------grad_S_norm :  3.018\n",
      "---------------------------------grad_beta_norm :  3.461\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09814\n",
      " MSE with beta :  0.23745\n",
      "ELBO :  108456.82458\n",
      "iteration number:  3300\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108457.30602717066\n",
      "---------------------------------grad_M_norm :  1.385\n",
      "---------------------------------grad_S_norm :  3.488\n",
      "---------------------------------grad_beta_norm :  3.199\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09752\n",
      " MSE with beta :  0.23496\n",
      "ELBO :  108457.30603\n",
      "iteration number:  3400\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108457.71633709075\n",
      "---------------------------------grad_M_norm :  1.203\n",
      "---------------------------------grad_S_norm :  1.901\n",
      "---------------------------------grad_beta_norm :  2.957\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09699\n",
      " MSE with beta :  0.23284\n",
      "ELBO :  108457.71634\n",
      "iteration number:  3500\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108458.0680675063\n",
      "---------------------------------grad_M_norm :  1.042\n",
      "---------------------------------grad_S_norm :  2.127\n",
      "---------------------------------grad_beta_norm :  2.743\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09655\n",
      " MSE with beta :  0.23105\n",
      "ELBO :  108458.06807\n",
      "iteration number:  3600\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108458.36774990085\n",
      "---------------------------------grad_M_norm :  0.907\n",
      "---------------------------------grad_S_norm :  2.041\n",
      "---------------------------------grad_beta_norm :  2.559\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09617\n",
      " MSE with beta :  0.22958\n",
      "ELBO :  108458.36775\n",
      "iteration number:  3700\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108458.62814980147\n",
      "---------------------------------grad_M_norm :  0.76\n",
      "---------------------------------grad_S_norm :  3.823\n",
      "---------------------------------grad_beta_norm :  2.357\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09584\n",
      " MSE with beta :  0.22839\n",
      "ELBO :  108458.62815\n",
      "iteration number:  3800\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108458.85439765117\n",
      "---------------------------------grad_M_norm :  0.697\n",
      "---------------------------------grad_S_norm :  1.817\n",
      "---------------------------------grad_beta_norm :  2.177\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09557\n",
      " MSE with beta :  0.22746\n",
      "ELBO :  108458.8544\n",
      "iteration number:  3900\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.04864658543\n",
      "---------------------------------grad_M_norm :  0.615\n",
      "---------------------------------grad_S_norm :  2.039\n",
      "---------------------------------grad_beta_norm :  2.155\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09533\n",
      " MSE with beta :  0.22676\n",
      "ELBO :  108459.04865\n",
      "iteration number:  4000\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.21717751918\n",
      "---------------------------------grad_M_norm :  0.538\n",
      "---------------------------------grad_S_norm :  2.759\n",
      "---------------------------------grad_beta_norm :  1.871\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09513\n",
      " MSE with beta :  0.22626\n",
      "ELBO :  108459.21718\n",
      "iteration number:  4100\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.36540587971\n",
      "---------------------------------grad_M_norm :  0.472\n",
      "---------------------------------grad_S_norm :  3.49\n",
      "---------------------------------grad_beta_norm :  1.744\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09496\n",
      " MSE with beta :  0.22593\n",
      "ELBO :  108459.36541\n",
      "iteration number:  4200\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.49208137029\n",
      "---------------------------------grad_M_norm :  0.524\n",
      "---------------------------------grad_S_norm :  2.388\n",
      "---------------------------------grad_beta_norm :  2.648\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09482\n",
      " MSE with beta :  0.22574\n",
      "ELBO :  108459.49208\n",
      "iteration number:  4300\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.60465641794\n",
      "---------------------------------grad_M_norm :  0.375\n",
      "---------------------------------grad_S_norm :  3.224\n",
      "---------------------------------grad_beta_norm :  1.438\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.0947\n",
      " MSE with beta :  0.22567\n",
      "ELBO :  108459.60466\n",
      "iteration number:  4400\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.69914530347\n",
      "---------------------------------grad_M_norm :  0.349\n",
      "---------------------------------grad_S_norm :  3.068\n",
      "---------------------------------grad_beta_norm :  1.407\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.0946\n",
      " MSE with beta :  0.22569\n",
      "ELBO :  108459.69915\n",
      "iteration number:  4500\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.77936211595\n",
      "---------------------------------grad_M_norm :  0.282\n",
      "---------------------------------grad_S_norm :  2.643\n",
      "---------------------------------grad_beta_norm :  1.207\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09452\n",
      " MSE with beta :  0.22577\n",
      "ELBO :  108459.77936\n",
      "iteration number:  4600\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.8462066238\n",
      "---------------------------------grad_M_norm :  0.295\n",
      "---------------------------------grad_S_norm :  2.017\n",
      "---------------------------------grad_beta_norm :  1.268\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09445\n",
      " MSE with beta :  0.2259\n",
      "ELBO :  108459.84621\n",
      "iteration number:  4700\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.90281004524\n",
      "---------------------------------grad_M_norm :  0.244\n",
      "---------------------------------grad_S_norm :  2.684\n",
      "---------------------------------grad_beta_norm :  1.045\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09439\n",
      " MSE with beta :  0.22606\n",
      "ELBO :  108459.90281\n",
      "iteration number:  4800\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.95422384563\n",
      "---------------------------------grad_M_norm :  0.212\n",
      "---------------------------------grad_S_norm :  1.801\n",
      "---------------------------------grad_beta_norm :  1.002\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09435\n",
      " MSE with beta :  0.22624\n",
      "ELBO :  108459.95422\n",
      "iteration number:  4900\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108459.96880303726\n",
      "---------------------------------grad_M_norm :  1.012\n",
      "---------------------------------grad_S_norm :  2.866\n",
      "---------------------------------grad_beta_norm :  6.719\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09431\n",
      " MSE with beta :  0.22643\n",
      "ELBO :  108459.9688\n",
      "iteration number:  5000\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.01517610376\n",
      "---------------------------------grad_M_norm :  0.64\n",
      "---------------------------------grad_S_norm :  3.337\n",
      "---------------------------------grad_beta_norm :  2.789\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09428\n",
      " MSE with beta :  0.22661\n",
      "ELBO :  108460.01518\n",
      "iteration number:  5100\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.05281900009\n",
      "---------------------------------grad_M_norm :  0.125\n",
      "---------------------------------grad_S_norm :  2.12\n",
      "---------------------------------grad_beta_norm :  0.651\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09426\n",
      " MSE with beta :  0.22679\n",
      "ELBO :  108460.05282\n",
      "iteration number:  5200\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.07444007137\n",
      "---------------------------------grad_M_norm :  0.135\n",
      "---------------------------------grad_S_norm :  1.943\n",
      "---------------------------------grad_beta_norm :  0.642\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09424\n",
      " MSE with beta :  0.22694\n",
      "ELBO :  108460.07444\n",
      "iteration number:  5300\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.0816974083\n",
      "---------------------------------grad_M_norm :  0.761\n",
      "---------------------------------grad_S_norm :  2.266\n",
      "---------------------------------grad_beta_norm :  3.659\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09423\n",
      " MSE with beta :  0.22708\n",
      "ELBO :  108460.0817\n",
      "iteration number:  5400\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.10683828691\n",
      "---------------------------------grad_M_norm :  0.095\n",
      "---------------------------------grad_S_norm :  1.956\n",
      "---------------------------------grad_beta_norm :  0.448\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09421\n",
      " MSE with beta :  0.22721\n",
      "ELBO :  108460.10684\n",
      "iteration number:  5500\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.11687153428\n",
      "---------------------------------grad_M_norm :  0.21\n",
      "---------------------------------grad_S_norm :  2.539\n",
      "---------------------------------grad_beta_norm :  1.262\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.0942\n",
      " MSE with beta :  0.22731\n",
      "ELBO :  108460.11687\n",
      "iteration number:  5600\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.11922086577\n",
      "---------------------------------grad_M_norm :  0.162\n",
      "---------------------------------grad_S_norm :  3.971\n",
      "---------------------------------grad_beta_norm :  0.895\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09419\n",
      " MSE with beta :  0.2274\n",
      "ELBO :  108460.11922\n",
      "iteration number:  5700\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.13024136932\n",
      "---------------------------------grad_M_norm :  0.119\n",
      "---------------------------------grad_S_norm :  2.503\n",
      "---------------------------------grad_beta_norm :  0.508\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09419\n",
      " MSE with beta :  0.22747\n",
      "ELBO :  108460.13024\n",
      "iteration number:  5800\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.12720769562\n",
      "---------------------------------grad_M_norm :  0.429\n",
      "---------------------------------grad_S_norm :  5.023\n",
      "---------------------------------grad_beta_norm :  2.022\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09419\n",
      " MSE with beta :  0.22752\n",
      "ELBO :  108460.12721\n",
      "iteration number:  5900\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.13139409745\n",
      "---------------------------------grad_M_norm :  0.657\n",
      "---------------------------------grad_S_norm :  3.439\n",
      "---------------------------------grad_beta_norm :  2.917\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09419\n",
      " MSE with beta :  0.22756\n",
      "ELBO :  108460.13139\n",
      "iteration number:  6000\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.13911412809\n",
      "---------------------------------grad_M_norm :  0.522\n",
      "---------------------------------grad_S_norm :  2.245\n",
      "---------------------------------grad_beta_norm :  2.541\n",
      "-------UPDATE-------\n",
      " MSE with Sigma :  0.09419\n",
      " MSE with beta :  0.2276\n",
      "ELBO :  108460.13911\n",
      "---------------------------------Maximum number of iterations reached\n",
      "---------------------------------lr : 0.02\n",
      "---------------------------------log likelihood : 108460.13911412809\n",
      "---------------------------------grad_M_norm :  0.522\n",
      "---------------------------------grad_S_norm :  2.245\n",
      "---------------------------------grad_beta_norm :  2.541\n",
      "CPU times: user 1min 27s, sys: 74.3 ms, total: 1min 27s\n",
      "Wall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "model_full = PLNmodel(Sigma_init, beta_init, M_init, S_init)\n",
    "%time param_full = model_full.full_grad_ascent(data, lr = 0.02, tolerance = 0.00, N_iter = 6000, verbose = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-accordance",
   "metadata": {},
   "source": [
    "### VEM algorithm with batch size "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "distributed-nutrition",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------UPDATE------- 50 N_epoch :VE  100\n",
      " MSE with Sigma :  0.04674\n",
      " MSE with beta :  0.26354\n",
      "ELBO :  724836.76834\n",
      "CPU times: user 1min 52s, sys: 76.3 ms, total: 1min 52s\n",
      "Wall time: 39.8 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.005, 0.022876792454961013)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_batch  = PLNmodel(Sigma_init, beta_init, M_init, S_init)\n",
    "%time model_batch.VEM(data, number_VEM_step = 2,tolerance= 0.01, batch_size = 50, requires_init = False, N_epoch_VE = 100, N_epoch_M = 100, beginning_VE_step_lr=0.005, beginning_M_step_lr = 0.1 , verbose = False) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foreign-bankruptcy",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$\\Sigma :  (p,p)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$\n",
    "\n",
    "$M : (n,p)$\n",
    "\n",
    "$S : (n,p)$ . Should be seen as $(n,p,p)$ but since all the $n$  matrix $(p,p)$ are diagonal, we only need $p$ points to encode it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decent-acceptance",
   "metadata": {},
   "source": [
    "## The Poisson lognormal (PLN) model\n",
    "\n",
    "\n",
    "- Consider $n$ sites $(i=1 \\ldots n)$\n",
    "\n",
    "- Measure $x_{i}=\\left(x_{i h}\\right)_{1 \\leq h \\leq d}$ :\n",
    "$x_{i h}=$ given environmental descriptor (covariate) for site $i$\n",
    "(altitude, temperature, latitude, ...)\n",
    "\n",
    "- Consider $p$ species $(j=1 \\ldots p)$ Measure $Y=\\left(Y_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p}$ :\n",
    "\n",
    "- Measure $Y = Y_{i j}=$ number of observed individuals from species $j$ in site $i$ (abundance). \n",
    "\n",
    "- Associate a random vector $Z_{i}$ with each site Assume that the unknown $\\left(Z_{i}\\right)_{1 \\leq i \\leq n}$ are iid (no spatial structure):\n",
    "$$\n",
    "Z_{i} \\sim \\mathcal{N}_{p}(0, \\Sigma)\n",
    "$$\n",
    "- Assume that the observed abundances $\\left(Y_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p}$ are independent conditionally on the $Z=\\left(Z_{i}\\right)_{i}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\left(Y_{i j} \\mid Z_{i j}\\right) \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+x_{i}^{\\top} \\beta_{j}+Z_{i j}\\right)\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "The parameter of the model is $\\theta = (\\beta, \\Sigma)$. \n",
    "\n",
    "Since the model depends on latent variables, we want to apply the EM algortihm. However, EM requires to compute the following : \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[p_{\\theta}\\left(Z_{i} \\mid Y\\right)\\right]=\\mathbb{E}_{\\theta}\\left[p_{\\theta}\\left(Z_{i} \\mid Y_{i}\\right)\\right] \\propto \\int_{\\mathbb{R}^{p}} p_{\\Sigma}\\left(Z_{i}\\right) \\prod_{j} p_{\\theta}\\left(Y_{i j} \\mid Z_{i j}\\right) \\mathrm{d} Z_{i}\n",
    "$$\n",
    "\n",
    "Which is intractable in practice. \n",
    "\n",
    "We have two alternatives here. The first one is an approximation of this integral via MCMC methods. The second one is the variationnal approach. We adopt here the second approach. \n",
    "\n",
    "The goal of variationnal approach is to approximate $p_{\\theta}(Z \\mid Y)$ with some law $q^{\\star}(Z)$ from which we can compute the expectation with respect to $\\theta$. \n",
    "\n",
    "\n",
    "We find such a law by maximizing the Evidence Lower BOund (ELBO), that is : \n",
    "$$ \n",
    "q^{\\star} = \\underset{q \\in \\mathcal{Q}}{\\operatorname{argmax}} J_{\\theta,q}(Y) \n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\begin{align} J_{\\theta, q}(Y)& =\\log p_{\\theta}(Y)-K L\\left[q(Z) \\| p_{\\theta}(Z \\mid Y)\\right]                                    \\\\ \n",
    "                              & = \\mathbb{E}_{q}\\left[\\log p_{\\theta}(Y, Z)\\right] \\underbrace{-\\mathbb{E}_{q}[\\log q(Z)]}_{\\text {entropy } \\mathcal{H}(q)}    \\end{align}\n",
    "$$\n",
    "\n",
    "\n",
    "Where $\\mathcal{Q}$ is a set of distributions. \n",
    "The Variational EM (VEM) consists in alternate between two steps : \n",
    "- VE step: update $q$\n",
    "$$\n",
    "q^{h+1}=\\underset{q \\in \\mathcal{Q}}{\\arg \\max } J_{\\theta^{h}, q}(Y)=\\underset{q \\in \\mathcal{Q}}{\\arg \\min } K L\\left[q(Z) \\| p_{\\theta^{h}}(Z \\mid Y)\\right]\n",
    "$$\n",
    "- M step: update $\\theta$\n",
    "$$\n",
    "\\theta^{h+1}=\\underset{\\theta}{\\arg \\max } J_{\\theta, q^{h+1}}(Y)=\\underset{\\theta}{\\arg \\max } \\mathbb{E}_{q^{h+1}}\\left[\\log p_{\\theta}(Y, Z)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incomplete-istanbul",
   "metadata": {},
   "source": [
    "# VEM for PLN \n",
    "\n",
    "We consider \n",
    "$$\n",
    "\\mathcal{Q}_{\\text {Gauss }}=\\{q: q=\\mathcal{N}(m, S)\\}\n",
    "$$\n",
    "\n",
    "The parameters $M = (m_i)_{1\\leq i \\leq n} \\in \\mathbb{R}^{n\\times p}$  and $ S = (S_i)_{1\\leq i \\leq n} \\in \\mathbb{R}^{n\\times p} $ are called variational parameters whereas $\\theta = (\\beta,\\Sigma)$ is called model parameter.  \n",
    "\n",
    "The ELBO can be computed as : \n",
    "\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_{\\theta, q}(Y)=& \\sum_{i} \\mathbb{E}_{q_{i}}\\left[\\log p_{\\theta}\\left(Z_{i}\\right)\\right]+\\sum_{i} \\mathbb{E}_{q_{i}}\\left[\\log p_{\\theta}\\left(Y_{i} \\mid Z_{i}\\right)\\right]+\\sum_{i} \\mathcal{H}\\left[\\mathcal{N}\\left(m_{i}, S_{i}\\right)\\right] \\\\\n",
    "=&-\\frac{n}{2} \\log |\\Sigma|-\\frac{1}{2} \\sum_{i} \\mathbb{E}_{\\mathcal{N}\\left(\\cdot m_{i}, S_{i}\\right)}\\left[Z_{i}^{\\top} \\Sigma^{-1} Z_{i}\\right] \\\\\n",
    "&+\\sum_{i, j} \\mathbb{E}_{\\mathcal{N}\\left(\\cdot ; m_{i}, S_{i}\\right)}\\left[-\\exp \\left(o_{i j}+x_{i}^{\\top} \\beta_{j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+x_{i}^{\\top} \\beta_{j}+Z_{i j}\\right)\\right] \\\\\n",
    "&+\\frac{1}{2} \\sum_{i} \\log \\left|S_{i}\\right|+\\mathrm{cst}\n",
    "\\end{aligned}\n",
    "$$\n",
    "We can evaluate some moments of $Z$ : \n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}_{\\mathcal{N}\\left(\\cdot ; m_{i}, s_{i}\\right)}\\left(Z_{i}\\right) &=m_{i}, & \\mathbb{E}_{\\mathcal{N}\\left(\\cdot ; m_{i}, S_{i}\\right)}\\left(Z_{i}^{\\top} \\Sigma^{-1} Z_{i}\\right)=m_{i}^{\\top} \\Sigma^{-1} m_{i}+\\operatorname{tr}\\left(\\Sigma^{-1} S_{i}\\right) \\\\\n",
    "\\mathbb{E}_{\\mathcal{N}\\left(\\cdot ; m_{i}, S_{i}\\right)}\\left(e^{Z_{i j}}\\right) &=\\exp \\left(m_{i j}+\\left[S_{i}\\right]_{j j}^{2} / 2\\right) &\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We then get : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "J_{\\theta, q}(y) &=-\\frac{n}{2} \\log |\\Sigma|-\\frac{1}{2} \\sum_{i} m_{i}^{\\top} \\Sigma^{-1} m_{i}+tr\\left(\\Sigma^{-1} S_{i}\\right) \\\\\n",
    "&+\\sum_{i, j}-\\exp \\left(o_{i j}+x_{i}^{T} \\beta_{j}+m_{i j}+\\left[S_{i}\\right]_{j j}^{2} / 2\\right)+Y_{i j}\\left(o_{i j}+x_{i}^{T} \\beta_{j}+m_{i j}\\right) \\\\\n",
    "&+\\frac{1}{2} \\sum_{i} \\log \\left|S_{i}\\right|+c s t .\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "% the problem is convex if the S_i are diagonals. %\n",
    "\n",
    "We have a closed form for the $\\Sigma$ step, that is : \n",
    "\n",
    "$$\n",
    "\\widehat{\\Sigma}=\\frac{1}{n} \\sum_{i}\\left(m_{i} m_{i}^{\\top}+S_{i}\\right)\n",
    "$$\n",
    "\n",
    "For $\\beta$, we have the following : \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\nabla _{\\beta}=-X^{\\top} \\exp \\left(0+M+\\frac{S \\odot S}{2}+X \\beta\\right)+\\frac{1}{2} X^{\\top} Y\n",
    "$$\n",
    "\n",
    "However, we don't have any closed form for the VE step. \n",
    "\n",
    "We consider now $S$ as $(n,p)$ matrix. Indeed, the $S_i$ are supposed diagonal matrices so we only need $p$ points to encode each of them. Considering $S$ this way is simpler for calculus. Here are the gradients with respect to the variational parameters. \n",
    "\n",
    "$$\n",
    "\\nabla_{M} J=-M \\Sigma^{-1}-\\exp \\left(O+x \\beta+M+\\frac{S \\odot S}{2}\\right)+Y\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{S} J=-\\frac{1}{2}\\mathbb{1}_{n}\\mathbb{1}_{p}^{T} D_{\\Sigma^{-1}}-S\\odot  \\exp \\left(0+X \\beta+M+\\frac{S \\odot  S}{2}\\right)+\\frac{1}{2} \\frac{1}{S}\n",
    "$$\n",
    "\n",
    "\n",
    "We denote $D_{\\Sigma^{-1}}$ the diagonal matrix composed of the diagonal of $\\Sigma^{-1}$. The exponential is applied component-wise, as well as the division $\\frac 1 S$. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advance-investigation",
   "metadata": {},
   "source": [
    "## What I did \n",
    "\n",
    "I first implemented the VEM algorithm. I learned how to use pytorch and the autodifferentiation to compute the gradients efficiently. I also implemented the gradients myself as a sanity check. If pytorch is used, "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
