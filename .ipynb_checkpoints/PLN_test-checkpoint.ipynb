{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "weighted-gravity",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import gradient_descent, minibatch_class\n",
    "import utils\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import scipy.linalg as SLA \n",
    "from scipy.linalg import toeplitz \n",
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "Y = torch.from_numpy(read_csv('trichoptera.csv', sep=',').to_numpy())\n",
    "O = torch.outer(Y.sum(1), torch.ones(Y.shape[1]))/1000\n",
    "\n",
    "#data = utils.format_data(counts=Y, offsets=np.log(O))\n",
    "\n",
    "n,p = Y.shape \n",
    "d = 2 # nb of cavariates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "independent-excerpt",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$\\Sigma :  (p,p)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$\n",
    "\n",
    "$M : (n,p)$\n",
    "\n",
    "$S : (n,p)$ . Should be seen as $(n,p,p)$ but since all the $n$  matrix $(p,p)$ are diagonal, we only need $p$ points to encode it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "suspended-xerox",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLNmodel(): \n",
    "    def __init__(self, Y, O,covariates, Sigma_init, beta_init, M_init, S_init): \n",
    "        \n",
    "        '''Initialization : \n",
    "            \n",
    "            Sigma_init is the initilization for Sigma as well as beta_init\n",
    "            I plan to do an initialization step more advanced. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y\n",
    "        self.O = O\n",
    "        self.covariates = covariates\n",
    "        \n",
    "        self.Sigma = torch.clone(Sigma_init)\n",
    "        #self.Sigma.requires_grad_(True)\n",
    "        self.beta = torch.clone(beta_init)\n",
    "        self.beta.requires_grad_(True)\n",
    "        self.M = torch.clone(M_init)\n",
    "        self.M.requires_grad_(True)\n",
    "        self.S = torch.clone(S_init) \n",
    "        self.S.requires_grad_(True)\n",
    "        \n",
    "        self.n, self.p = Y.shape\n",
    "        self.det_Sigma = torch.det(self.Sigma)\n",
    "        self.inv_Sigma = torch.inverse(self.Sigma)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def forward(self): \n",
    "        ''' \n",
    "        computes the ELBO J(theta,q) \n",
    "        \n",
    "        S is of size (n,p) but it size should be (n,p,p). The size is (n,p) since it \n",
    "        represents n diagonals matrix of size (p,p), which we can encode with only (n,p) numbers. \n",
    "        '''\n",
    "        \n",
    "        tmp = -self.n/2*math.log(self.det_Sigma) # premiere formule ou on prend le log du det de Sigma \n",
    "    \n",
    "        tmp -=1/2*( \n",
    "            \n",
    "                    torch.sum(torch.mm(torch.mm(self.M,self.inv_Sigma),self.M.T).diagonal()) # we can simplify here, takes too much time \n",
    "                   +                                                  # we should remove the diagonal and do a more efficient multiplication\n",
    "                   torch.sum(torch.tensor([torch.trace(torch.multiply(self.inv_Sigma,self.S[i,:])) for i in range(self.n)]))\n",
    "                    ) # formula with the quadratic function and the trace \n",
    "        \n",
    "        \n",
    "        Gram_matrix = torch.mm(self.covariates,self.beta) # matrix with term (i,j): <x_i,beta_j>\n",
    "        \n",
    "        Exp_moment = torch.exp(self.M + torch.pow(self.S,2)/2)\n",
    "        \n",
    "        tmp += torch.sum(-torch.exp(self.O + Gram_matrix + self.M + torch.pow(self.S,2)/2) + torch.multiply(self.Y, self.O + Gram_matrix + self.M))\n",
    "        \n",
    "        tmp+= 1/2*torch.sum(torch.tensor([math.log(self.S[i,:].prod()) for i in range(self.n)]))\n",
    "        tmp.backward()\n",
    "        return tmp\n",
    "    \n",
    "    \n",
    "    def grad_m(self): \n",
    "        pass\n",
    "    \n",
    "    def update(self,lr = 0.01): \n",
    "        '''fait une étape de descente de gradient sur le parametre S. On va généraliser \n",
    "        à M,beta et Sigma ensuite '''\n",
    "        print('current value :',self.forward().item())\n",
    "        with torch.no_grad():\n",
    "            self.S += lr*self.S.grad\n",
    "        self.S.grad.zero_()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 562,
   "id": "assumed-chicken",
   "metadata": {},
   "outputs": [],
   "source": [
    "Sigma_init = torch.from_numpy(toeplitz(0.5**np.arange(p)))\n",
    "beta_init = torch.ones((d, p))\n",
    "M_init = torch.ones((n,p))/100# some random values to initialize we divide to avoid nan values \n",
    "S_init = torch.ones((n,p))/8 # some random values to initializ. we divise to avoid nan values \n",
    "\n",
    "covariates = torch.zeros((n,d))# pas encore de covariates dc je prend 0 pr l'instant "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "arabic-coverage",
   "metadata": {},
   "source": [
    "Model_bad ne marche pas, model_good marche. La seule chose qui change entre les deux modèles est l'initialisation de S. Celle de model_bad est 8 fois plus petite que celle de model_good. Si tu retranches le gradient ( i.e. faire self.S -= lr*self.s.grad dans la fonction update) alors les deux tendances s'échangent. \n",
    "\n",
    "J'ai regardé ce qu'il se passait quand on update M, cette fois ci ça marche avec n'importe quelle initialisation. J'ai essayé d'update Sigma et beta mais il y a un bug avec requires_grad que j'ai donc mis en commentaire, je suis entrain de debugger ça. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "gothic-documentary",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_bad = PLNmodel(Y,O, covariates, Sigma_init, beta_init, M_init, torch.ones((n,p))/8 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "functioning-mozambique",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current value : 10050.185035569646\n",
      "current value : 10045.94234560788\n",
      "current value : 10041.263323694968\n",
      "current value : 10036.28951374982\n",
      "current value : 10031.112203976729\n",
      "current value : 10025.791159702494\n",
      "current value : 10020.366033206612\n",
      "current value : 10014.863473876114\n",
      "current value : 10009.30163202494\n",
      "current value : 10003.693046797816\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10 ) : \n",
    "    model_bad.update()\n",
    "    #print(torch.min(torch.abs(model.S)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "continental-relevance",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_good = PLNmodel(Y,O, covariates, Sigma_init, beta_init, M_init, torch.ones((n,p)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "spectacular-oasis",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current value : 9495.356539235709\n",
      "current value : 9671.187273212672\n",
      "current value : 9746.773648736002\n",
      "current value : 9797.377524515361\n",
      "current value : 9836.06876480069\n",
      "current value : 9867.705341506662\n",
      "current value : 9894.637753890287\n",
      "current value : 9918.180509782485\n",
      "current value : 9939.139796361907\n",
      "current value : 9958.043173250315\n"
     ]
    }
   ],
   "source": [
    "for _ in range(10) : \n",
    "    model_good.update()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
