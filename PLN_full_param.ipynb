{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "chemical-dietary",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import gradient_descent, minibatch_class\n",
    "import utils\n",
    "from utils import Poisson_reg\n",
    "from utils import sample_PLN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm \n",
    "import time \n",
    "import sys \n",
    "from __future__ import print_function\n",
    "import psutil\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import scipy.linalg as SLA \n",
    "from scipy.linalg import toeplitz \n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dimensional-creature",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "olive-tumor",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLN_full():\n",
    "    def __init__(self, C_init, beta_init, M_init, S_init): \n",
    "        '''\n",
    "            Initialization : \n",
    "            'Y' : the data, size (n,p). n is the number of samples we have and p the number of species. \n",
    "                  THE TYPE IS INT\n",
    "            'O': offset : additional offset. (not very important for comprehension). size (n,p)\n",
    "            'covariates' : covariates, size (n,d)\n",
    "            'C_init' : initialization for C. I plan to do a more advanced initialization. \n",
    "            'beta_init ' : Initialization for beta. I plan to do a more advanced initialization. \n",
    "            'M_init' : initialization for the variational parameter M\n",
    "            'S_init ': initialization for the variational parameter S\n",
    "        '''\n",
    "        # model parameters\n",
    "        self.C = torch.clone(C_init)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.clone(beta_init)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        #variational parameters\n",
    "        self.M = torch.clone(M_init)\n",
    "        self.M.requires_grad_(True)\n",
    "        self.S = torch.clone(S_init) \n",
    "        self.S.requires_grad_(True)\n",
    "        self.params = {'C' : self.C, 'beta' : self.beta, 'S' : self.S,'M' : self.M}\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.params.values(), lr = 0.002)\n",
    "        \n",
    "        # some list to store some stats\n",
    "        self.MSE_Sigma_list = list()\n",
    "        self.MSE_beta_list = list()\n",
    "        self.ELBO_list = list()\n",
    "        self.running_times = list()\n",
    "\n",
    "        \n",
    "    def extract_data(self,data): \n",
    "        '''\n",
    "        function to extract the data. This function is just here to have a code more compact. \n",
    "        \n",
    "        args : \n",
    "              'data': list with 3 elements : Y, O and covariates in this order. \n",
    "        '''\n",
    "        #known variables\n",
    "        self.Y = data[0];self.O = data[1];self.covariates = data[2]\n",
    "        self.n, self.p = self.Y.shape\n",
    "        \n",
    "    def compute_ELBO(self): \n",
    "        return ELBO(self.Y, self.covariates,self.O ,self.M ,self.S ,self.C ,self.beta)\n",
    "    \n",
    "    def full_grad_ascent(self,data, lr = 0.1, tolerance = 0, N_epoch = 1000, verbose = True ): \n",
    "        self.extract_data(data)\n",
    "        self.last_params, self.running_times, self.MSE_Sigma_list, self.MSE_beta_list, self.ELBO_list = torch_gradient_ascent(self.optimizer, \n",
    "                            self.compute_ELBO,self.params, lr = lr, tolerance = tolerance, N_epoch = N_epoch, verbose = verbose)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "increasing-kuwait",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_gradient_ascent(optimizer, compute_gain,params,  lr , tolerance , N_epoch , verbose ): \n",
    "    '''\n",
    "    gradient ascent function. We compute the gradients thanks to the autodifferentiation of pytorch. \n",
    "\n",
    "    args : \n",
    "            'optimizer' : torch.optim.optimizer. the optimizer for the parameters. \n",
    "            \n",
    "            'compute_gain' : function. It should call the parameters by itself. i.e. we wil call compute_gain() without any parameter \n",
    "                            in argument. \n",
    "\n",
    "            'lr' : float.  a learning rateM if we want to set the optimizer learning rate to a certain lr. If None, \n",
    "                  it will take the actual learning_rate of the optimizer. \n",
    "            'tolerance': float. the threshold we set to stop the algorithm. It will stop if the norm of each gradient's parameter \n",
    "                         is lower than this threshold, or if we are not improving the loss more than tolerance. \n",
    "            'N_epoch': int. the Maximum number of epoch we are ready to do. \n",
    "\n",
    "            'Verbose' : bool. if True, will print some messages useful to interpret the gradient ascent. If False, nothing will be printed. \n",
    "            \n",
    "            we have ot yet implement it so thtat it can takes batches. \n",
    "\n",
    "\n",
    "    returns : the parameters optimized. \n",
    "    '''\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    MSE_Sigma_list = list()\n",
    "    MSE_beta_list = list()\n",
    "    ELBO_list = list()\n",
    "    running_times = list()\n",
    "    # we set the gradient to zero just to make sure the gradients are properly calculated\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if lr is not None : # if we want to set a threshold, we set it. Ohterwise, we skip this condition and keep the actual learning_rate\n",
    "        optimizer.param_groups[0]['lr'] = lr \n",
    "\n",
    "    '''\n",
    "    #if batch_size is None, we take n. \n",
    "    if batch_size == None : \n",
    "        batch_size = self.Y.shape[0]\n",
    "    '''\n",
    "\n",
    "    stop_condition = False \n",
    "    i = 0\n",
    "    old_loss = 1.\n",
    "\n",
    "    while i < N_epoch and stop_condition == False: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -compute_gain()\n",
    "        loss.backward()\n",
    "        if torch.isnan(loss).item() == True : \n",
    "            print('NAN')\n",
    "        else : last_params = params \n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        \n",
    "        # condition to see if we have reach the tolerance threshold\n",
    "        if  abs(loss.item() - old_loss) < tolerance : \n",
    "            #if max([torch.norm(param.grad) for param in params]) < tolerance  or abs(loss.item()- old_loss)>  tolerance :\n",
    "            stop_condition = True \n",
    "        old_loss = loss.item()\n",
    "        \n",
    "        running_times.append(time.time()-t0)\n",
    "        MSE_Sigma_list.append(torch.mean((torch.mm(params['C'],params['C'].T)-true_Sigma)**2).item())\n",
    "        MSE_beta_list.append(torch.mean((params['beta']-true_beta)**2).item())\n",
    "        ELBO_list.append(-loss.item())\n",
    "        \n",
    "        \n",
    "        if i%100 == 0 and verbose : \n",
    "            print('iteration number: ', i)\n",
    "            print('-------UPDATE-------')\n",
    "            print(' MSE with Sigma : ', np.round(MSE_Sigma_list[-1],5))\n",
    "            print(' MSE with beta : ', np.round(MSE_beta_list[-1],5))\n",
    "            print('ELBO : ', np.round(-loss.item(),5))\n",
    "            print_stats(loss, last_params, optimizer)\n",
    "            \n",
    "    \n",
    "    if verbose : # just print some stats if we want to \n",
    "        if stop_condition : \n",
    "            print('---------------------------------Tolerance {} reached in {} iterations'.format(tolerance, i))\n",
    "        else : \n",
    "            print('---------------------------------Maximum number of iterations reached : ', N_epoch)\n",
    "        print_stats(loss, last_params, optimizer)\n",
    "    print(' MSE with Sigma : ', np.round(MSE_Sigma_list[-1],5))\n",
    "    print(' MSE with beta : ', np.round(MSE_beta_list[-1],5))\n",
    "    \n",
    "    return last_params, running_times, MSE_Sigma_list, MSE_beta_list, ELBO_list\n",
    "\n",
    "def print_stats(loss, params, optimizer): \n",
    "    '''\n",
    "    small function that print some stats. \n",
    "\n",
    "    It will print the actual learning rate of the optimizer, the actual log likelihood \n",
    "    and the norms of each parameter's gradient. The norm of the parameter's gradient should be low\n",
    "    when we are close to the optimum. \n",
    "    '''\n",
    "    print('---------------------------------lr :', optimizer.param_groups[0]['lr'])\n",
    "    print('---------------------------------log likelihood :', - loss.item())\n",
    "    for param_name, param in params.items(): \n",
    "        print('---------------------------------grad_{}_norm : '.format(param_name), round(torch.norm(param.grad).item(), 3))\n",
    "\n",
    "\n",
    "def ELBO(Y, covariates,O ,M ,S ,C ,beta): \n",
    "    n = Y.shape[0]\n",
    "    SrondS = torch.multiply(S,S)\n",
    "    OplusM = O+M\n",
    "    MmoinsXB = M-torch.mm(covariates, beta) \n",
    "    tmp = torch.sum(  torch.multiply(Y, OplusM)  -1/2*(torch.exp(OplusM+SrondS/2)-torch.log(SrondS)))\n",
    "    tmp -= 1/2*torch.trace(  \n",
    "                            torch.mm(  \n",
    "                                        torch.inverse(torch.mm(C,C.T)), \n",
    "                                        torch.diag(torch.sum(SrondS, dim = 0))+ torch.mm(MmoinsXB.T, MmoinsXB)\n",
    "                                    )\n",
    "                          )\n",
    "    #tmp-= n*torch.sum(torch.log(torch.diag(torch.abs(C))))\n",
    "    tmp-= n*torch.log(torch.det(C))\n",
    "    return tmp \n",
    "\n",
    "def grad_beta(Y, covariates,O ,M ,S ,C ,beta) :  \n",
    "    #not yet finished\n",
    "    grad = - torch.mm(covariates.T,torch.exp( O + M + torch.pow(S,2)/2 + torch.mm(covariates,beta)))\n",
    "    grad += torch.mm(covariates.T,Y.double())\n",
    "    return grad "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pharmaceutical-enhancement",
   "metadata": {},
   "source": [
    "$$\\boxed{\\begin{align} J_{\\theta, q}(Y) &=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S\\odot S}{2}\\right) + \\frac 12 \\log (S \\odot S) \\right)\\mathbb{1}_p \\\\\n",
    "& \\quad  - \\frac 12\\operatorname{tr}\\left((CC^{\\top})^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right) \\\\\n",
    "& \\quad  - n \\log |C|+ cst \n",
    "\\end{align}}$$\n",
    "\n",
    "$$\\boxed{\\begin{align} J_{\\theta, q}(Y) &=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S\\odot S}{2}\\right) + \\frac 12 \\log (S \\odot S) \\right)\\mathbb{1}_p \\\\\n",
    "& \\quad  - \\frac 12\\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right) \\\\\n",
    "& \\quad  - \\frac n2 \\log |\\Sigma|+ cst \n",
    "\\end{align}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "japanese-auditor",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 1000; p = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "spiritual-wagon",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Sigma = torch.from_numpy(toeplitz(0.5**np.arange(p)))\n",
    "true_beta = torch.randn(d, p)\n",
    "\n",
    "covariates = torch.rand((n,d))\n",
    "O = 1 + torch.zeros((n,p))\n",
    "\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled = torch.from_numpy(sample_model.sample(true_Sigma,true_beta, O, covariates)) \n",
    "\n",
    "data = [Y_sampled, O, covariates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "brutal-bones",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "noise = torch.randn(p) \n",
    "Sigma_init =  torch.diag(noise**2)\n",
    "C_init = torch.cholesky(Sigma_init)\n",
    "beta_init = torch.rand((d, p))\n",
    "\n",
    "M_init = torch.ones((n,p))/100# some random values to initialize we divide to avoid nan values \n",
    "S_init = torch.ones((n,p))/8 # some random values to initializ. we divise to avoid nan values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "about-vocabulary",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PLN_full(C_init, beta_init, M_init, S_init)\n",
    "%time model.full_grad_ascent(data, N_epoch = 10000,verbose=True, lr = 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-grace",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-trinity",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-processing",
   "metadata": {},
   "source": [
    "Here we have a PLN model described as the following : \n",
    "\n",
    "- Consider $n$ sites $(i=1 \\ldots n)$\n",
    "\n",
    "- Measure $x_{i}=\\left(x_{i h}\\right)_{1 \\leq h \\leq d}$ :\n",
    "$x_{i h}=$ given environmental descriptor (covariate) for site $i$\n",
    "(altitude, temperature, latitude, ...)\n",
    "\n",
    "- Consider $p$ species $(j=1 \\ldots p)$ Measure $Y=\\left(Y_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p}$ :\n",
    "\n",
    "- Measure $Y = Y_{i j}=$ number of observed individuals from species $j$ in site $i$ (abundance). \n",
    "\n",
    "- Associate a random vector $Z_{i}$ with each site Assume that the unknown $\\left(Z_{i}\\right)_{1 \\leq i \\leq n}$ are independant such that:\n",
    "$$\n",
    "Z_{i} \\sim \\mathcal{N}_{p}(x_i \\beta, \\Sigma) \\quad \\Sigma = CC^{\\top}\n",
    "$$\n",
    "\n",
    "and $C$ is a lower triangular matrix. \n",
    "- Assume that the observed abundances $\\left(Y_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p}$ are independent conditionally on the $Z=\\left(Z_{i}\\right)_{i}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\left(Y_{i j} \\mid Z_{i j}\\right) \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "$$\n",
    "\n",
    "Where $O = (o_{ij})_{1\\leq i\\leq n, 1\\leq j\\leq p}$ are known offsets. \n",
    "\n",
    "The unknown parameter is $\\theta = (C,\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fewer-closing",
   "metadata": {},
   "source": [
    "$Z$ being a latent variable, we want to use the EM algorithm to derive the maximum likelihood estimator. However, it requires to compute \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[p_{\\theta}\\left(Z_{i} \\mid Y\\right)\\right]=\\mathbb{E}_{\\theta}\\left[p_{\\theta}\\left(Z_{i} \\mid Y_{i}\\right)\\right] \\propto \\int_{\\mathbb{R}^{p}} p_{\\theta}\\left(Z_{i}\\right) \\prod_{j} p_{\\theta}\\left(Y_{i j} \\mid Z_{i j}\\right) \\mathrm{d} Z_{i}\n",
    "$$ which is intractable in practice. \n",
    "\n",
    "We thus choose the variationnal approximation. We set\n",
    "\n",
    "$$ \n",
    "q^{\\star} = \\underset{q \\in \\mathcal{Q_{gauss}}}{\\operatorname{argmax}} J_{\\theta,q}(Y) \n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\begin{align} J_{\\theta, q}(Y)& =\\log p_{\\theta}(Y)-K L\\left[q(Z) \\| p_{\\theta}(Z \\mid Y)\\right]                                    \\\\ \n",
    "                              & = \\mathbb{E}_{q}\\left[\\log p_{\\theta}(Y, Z)\\right] \\underbrace{-\\mathbb{E}_{q}[\\log q(Z)]}_{\\text {entropy } \\mathcal{H}(q)}    \\end{align}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\mathcal{Q}_{\\text {Gauss }}=\\{\n",
    "q=\\left(q_{1}, \\ldots q_{n}\\right), q_{i} \\sim \\mathcal{N}\\left(M_{i}, \\operatorname{diag} (S_{i}\\odot S_i ), M_i \\in \\mathbb{S} ^p, S_i \\in \\mathbb{R} ^p\\right)\\}\n",
    "$$\n",
    "\n",
    "\n",
    "The Variational EM (VEM) consists in alternate between two steps : \n",
    "- VE step: update $q$\n",
    "$$\n",
    "q^{h+1}=\\underset{q \\in \\mathcal{Q_{gauss}}}{\\arg \\max } J_{\\theta^{h}, q}(Y)=\\underset{q \\in \\mathcal{Q_{gauss}}}{\\arg \\min } K L\\left[q(Z) \\| p_{\\theta^{h}}(Z \\mid Y)\\right]\n",
    "$$\n",
    "- M step: update $\\theta$\n",
    "$$\n",
    "\\theta^{h+1}=\\underset{\\theta}{\\arg \\max } J_{\\theta, q^{h+1}}(Y)=\\underset{\\theta}{\\arg \\max } \\mathbb{E}_{q^{h+1}}\\left[\\log p_{\\theta}(Y, Z)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "agricultural-spice",
   "metadata": {},
   "source": [
    "Let's compute the ELBO $J_{\\theta, q}(Y)$\n",
    "\n",
    "\n",
    "$$\n",
    "J_{\\sigma, q}(Y)=\\underbrace{\\mathbb{E}_{q}\\left[\\log p_{\\theta}(Y \\mid Z)\\right]}_{(1)}+\\underbrace{E_{q}\\left[\\log p_{\\theta}(Z)\\right]}_{(2)}+\\underbrace{H(q)}_{(3)}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "(1)& =\\sum_{i} \\mathbb{E}_{q}\\left[\\log p_{\\theta}\\left(Y_{i} \\mid Z\\right)\\right] \\\\\n",
    "&=\\sum_{i} \\mathbb{E}_{q}\\left[\\log p_{\\theta}\\left(Y_{i} \\mid Z_{i}\\right)\\right] \\\\\n",
    "&=\\sum_{i, j} \\mathbb{E}_{q}\\left[\\log p_{\\theta}\\left(Y_{i j} \\mid Z_{i j}\\right)\\right] \\\\\n",
    "& =\\sum_{i, j} \\mathbb{E}_{q}\\left[Y_{i j}\\left(o_{i j}+Z_{i j}\\right)-\\exp \\left(o_{i j}+Z_{i j}\\right)\\right] + cst\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We need to compute some moments of $Z$ under $q$. \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q}\\left[Z_{i j}\\right]=M_{i j} \\quad \\quad E_{q}\\left[\\operatorname{exp}\\left(Z_{i j}\\right)\\right]=\\frac{1}{2} \\operatorname{exp}\\left(M_{i j}+\\frac{(S_{ij})^2}{2}\\right)\n",
    "$$\n",
    "\n",
    "So that \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(1) &=\\sum_{i, j} Y_{i j}\\left(o_{i j}+M_{i j}\\right)-\\frac{1}{2} \\exp \\left(o_{i j}+M_{i j}+\\frac{(S_{i_{jj}})^2}{2}\\right) + cst \\\\\n",
    "&=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S \\odot S}{2}\\right)\\right)\\mathbb{1}_p + cst \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "thick-rescue",
   "metadata": {},
   "source": [
    "Where we have denoted $M = (M_{ij})_{1\\leq i\\leq n, 1\\leq j\\leq p}$ and $ S = (S_{ij})_{1\\leq i\\leq n, 1\\leq j\\leq p}$. The exponential is applied pointwise on the last equation. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(3)=H(q) &=\\sum_{i} H\\left(q_{i}\\right) \\\\\n",
    "&=\\sum_{i} \\log \\left(\\sqrt{(2 \\pi e)^{p}\\left|S_{i}\\right|}\\right) \\\\\n",
    "&=\\frac 12\\sum_{i} \\log \\left|(S_{i} \\odot S_i)^2 \\right|+cst \\\\\n",
    "&=\\sum_{i j} \\log S_{i j}+cst \\\\\n",
    "& =  \\mathbb{1}_n ^{\\top}(\\log S )\\mathbb{1}_p+cst\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where the log is applied pointwise at the last equation. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(2)=\\mathbb{E}_{q}\\left[\\log p_{\\theta}(Z)\\right] &=\\sum_{1} E_{q}\\left[\\log p_{\\theta}\\left(Z_{i}\\right)\\right] \\\\\n",
    "&=-\\frac{n}{2} \\log |\\Sigma|+\\sum_{i} \\mathbb{E}_{q}\\left[-\\frac{1}{2}\\left(Z_{i}-X_{i} \\beta\\right)^{\\top} \\Sigma^{-1}\\left(Z_{i} - X_{i} \\beta\\right)\\right] +cst\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text { Let } V \\sim \\mathcal{N} \\left(\\mu, \\Lambda), \\mu \\in \\mathbb{R}^{p}, \\Lambda \\in \\mathcal S _p ^{++}\\right.  \\\\\n",
    "\\text { Let's compute } \\; \\mathbb{E}\\left[V^{\\top} \\Sigma^{-1} V\\right]\n",
    "$\n",
    "\n",
    "We denote $\\Sigma ^{-1 / 2}$ the square root Matrix of $\\Sigma^{-1}$. It exists since $\\Sigma ^{-1} \\in \\mathcal{S}_p^{++}$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[V^{\\top} \\Sigma^{-1} V\\right] &=\\mathbb{E}\\left[V^{\\top}\\Sigma ^{-1 / 2} \\Sigma^{-1 / 2} V\\right]\\\\\n",
    "&=\\mathbb{E}\\left[\\left(\\Sigma^{-1 / 2} V\\right)^{\\top}\\left(\\Sigma^{-1 / 2} V\\right)\\right] \\\\\n",
    "&=\\mathbb{E} \\|\\Sigma^{-1 / 2} V \\|_{2}^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text {Let } \\tilde{V}=\\Sigma^{-1 / 2} V, \\quad \\tilde{V} \\sim \\mathcal{N}\\left(\\Sigma^{-1 / 2} \\mu,  \\Sigma^{-1 / 2} \\Lambda \\Sigma^{-\\frac{1}{2}}\\right)\n",
    "$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[V^{\\top} \\Sigma^{-1} V\\right] &=\\mathbb{E}\\|\\widetilde{V}\\|_{2}^{2} \\\\\n",
    "&=\\sum \\mathbb E \\widetilde{V}_{j}^{2} \\\\\n",
    "&=\\sum \\operatorname{var}\\left(\\widetilde{V}_{j}\\right)^{2}+\\mathbb{E}\\left[\\widetilde{V}_{j}\\right]^{2}\\\\\n",
    "&=\\sum_{j}\\left(\\Sigma^{-1 / 2} \\Lambda \\Sigma^{-1 / 2}\\right)_{j j}+\\left(\\Sigma^{-1 / 2} \\mu\\right)^{2}_j\\\\\n",
    "&= \\operatorname{tr}\\left(\\Sigma^{-1 / 2} \\Lambda \\Sigma^{-1 / 2}\\right)+\\sum_{j}\\left(\\left(\\Sigma_{j,.}^{-1 / 2}\\right)^{\\top} \\mu\\right)^{2}\\\\\n",
    "&=\\operatorname{tr}\\left(\\Sigma^{-1} \\Lambda\\right)+\\sum_{j}\\left(\\left(\\Sigma_{j,.}^{-1/2}\\right)^{\\top} \\mu\\right)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since under $q$, $Z_{i}-X_{i} \\beta \\sim \\mathcal N (M_i - X_i \\beta, S_i \\odot S_i ) $\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(2) &=- \\frac 12 \\sum_{i} \\operatorname{tr}\\left(\\Sigma^{-1} (S_{i} \\odot S_i) \\right) - \\frac 12 \\sum_{i, j}\\left(\\left(\\Sigma_{j,.}^{-1 / 2} \\right) ^{\\top}\\left(M_{i}-X_{i} \\beta\\right)\\right)^2  - \\frac n2 \\log |\\Sigma|+ cst  \\\\\n",
    "&= - \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\sum_{i} S_{i}\\odot S_i \\right)\\right) - \\frac 12 \\sum_{i, j}\\left(\\Sigma^{-1 / 2}(M-X \\beta)\\right)_{j, i}^{\\top}\\left(\\Sigma\n",
    "^{-1 / 2}(M-X \\beta)\\right)_{i, j}  - \\frac n2 \\log |\\Sigma|+ cst  \\\\\n",
    "&=- \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\sum_{i} S_{i} \\odot S_i  \\right)\\right)- \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1 / 2}(M-X \\beta)^{\\top}(M-X \\beta) \\Sigma^{-1 / 2}\\right)  - \\frac n2 \\log |\\Sigma|+ cst  \\\\\n",
    "&=- \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right)  - \\frac n2 \\log |\\Sigma|+ cst \n",
    "\\end{aligned}\n",
    "$$\n",
    "We then have : \n",
    "\n",
    "\n",
    "$$\\boxed{\\begin{align} J_{\\theta, q}(Y) &=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S\\odot S}{2}\\right) + \\frac 12 \\log (S \\odot S) \\right)\\mathbb{1}_p \\\\\n",
    "& \\quad  - \\frac 12\\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right) \\\\\n",
    "& \\quad  - \\frac n2 \\log |\\Sigma|+ cst \n",
    "\\end{align}}$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
