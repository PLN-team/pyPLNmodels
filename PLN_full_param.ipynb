{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "willing-communication",
   "metadata": {},
   "outputs": [],
   "source": [
    "from gradient_descent import gradient_descent, minibatch_class\n",
    "import utils\n",
    "from utils import Poisson_reg\n",
    "from utils import sample_PLN\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from pandas import read_csv\n",
    "import math\n",
    "from timeit import default_timer as timer\n",
    "from tqdm import tqdm \n",
    "import time \n",
    "import sys \n",
    "\n",
    "from __future__ import print_function\n",
    "import psutil\n",
    "import multiprocessing\n",
    "import concurrent.futures\n",
    "import threading\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn\n",
    "import scipy.linalg as SLA \n",
    "from scipy.linalg import toeplitz \n",
    "torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "threaded-treatment",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PLN_full():\n",
    "    def __init__(self, C_init, beta_init, M_init, S_init): \n",
    "        '''\n",
    "            Initialization : \n",
    "            'Y' : the data, size (n,p). n is the number of samples we have and p the number of species. \n",
    "                  THE TYPE IS INT\n",
    "            'O': offset : additional offset. (not very important for comprehension). size (n,p)\n",
    "            'covariates' : covariates, size (n,d)\n",
    "            'C_init' : initialization for C. I plan to do a more advanced initialization. \n",
    "            'beta_init ' : Initialization for beta. I plan to do a more advanced initialization. \n",
    "            'M_init' : initialization for the variational parameter M\n",
    "            'S_init ': initialization for the variational parameter S\n",
    "        '''\n",
    "        # model parameters\n",
    "        self.C = torch.clone(C_init)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.clone(beta_init)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        #variational parameters\n",
    "        self.M = torch.clone(M_init)\n",
    "        self.M.requires_grad_(True)\n",
    "        self.S = torch.clone(S_init) \n",
    "        self.S.requires_grad_(True)\n",
    "        self.params = {'C' : self.C, 'beta' : self.beta, 'S' : self.S,'M' : self.M}\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.params.values(), lr = 0.002)\n",
    "        \n",
    "        # some list to store some stats\n",
    "        self.MSE_Sigma_list = list()\n",
    "        self.MSE_beta_list = list()\n",
    "        self.ELBO_list = list()\n",
    "        self.running_times = list()\n",
    "\n",
    "        \n",
    "    def extract_data(self,data): \n",
    "        '''\n",
    "        function to extract the data. This function is just here to have a code more compact. \n",
    "        \n",
    "        args : \n",
    "              'data': list with 3 elements : Y, O and covariates in this order. \n",
    "        '''\n",
    "        #known variables\n",
    "        self.Y = data[0];self.O = data[1];self.covariates = data[2]\n",
    "        self.n, self.p = self.Y.shape\n",
    "        \n",
    "    def compute_ELBO(self): \n",
    "        return ELBO(self.Y,self.O , self.covariates,self.M ,self.S ,self.C ,self.beta)\n",
    "    \n",
    "    def full_grad_ascent(self,data, lr = 0.1, tolerance = 0, N_epoch = 1000, verbose = True ): \n",
    "        self.extract_data(data)\n",
    "        self.last_params, self.running_times, self.MSE_Sigma_list, self.MSE_beta_list, self.ELBO_list = torch_gradient_ascent(self.optimizer, \n",
    "                            self.compute_ELBO,self.params, lr = lr, tolerance = tolerance, N_epoch = N_epoch, verbose = verbose)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "global-mileage",
   "metadata": {},
   "outputs": [],
   "source": [
    "def torch_gradient_ascent(optimizer, compute_gain,params,  lr , tolerance , N_epoch , verbose ): \n",
    "    '''\n",
    "    gradient ascent function. We compute the gradients thanks to the autodifferentiation of pytorch. \n",
    "\n",
    "    args : \n",
    "            'optimizer' : torch.optim.optimizer. the optimizer for the parameters. \n",
    "            \n",
    "            'compute_gain' : function. It should call the parameters by itself. i.e. we wil call compute_gain() without any parameter \n",
    "                            in argument. \n",
    "\n",
    "            'lr' : float.  a learning rateM if we want to set the optimizer learning rate to a certain lr. If None, \n",
    "                  it will take the actual learning_rate of the optimizer. \n",
    "            'tolerance': float. the threshold we set to stop the algorithm. It will stop if the norm of each gradient's parameter \n",
    "                         is lower than this threshold, or if we are not improving the loss more than tolerance. \n",
    "            'N_epoch': int. the Maximum number of epoch we are ready to do. \n",
    "\n",
    "            'Verbose' : bool. if True, will print some messages useful to interpret the gradient ascent. If False, nothing will be printed. \n",
    "            \n",
    "            we have ot yet implement it so thtat it can takes batches. \n",
    "\n",
    "\n",
    "    returns : the parameters optimized. \n",
    "    '''\n",
    "\n",
    "    t0 = time.time()\n",
    "    \n",
    "    MSE_Sigma_list = list()\n",
    "    MSE_beta_list = list()\n",
    "    ELBO_list = list()\n",
    "    running_times = list()\n",
    "    # we set the gradient to zero just to make sure the gradients are properly calculated\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    if lr is not None : # if we want to set a threshold, we set it. Ohterwise, we skip this condition and keep the actual learning_rate\n",
    "        optimizer.param_groups[0]['lr'] = lr \n",
    "\n",
    "    '''\n",
    "    #if batch_size is None, we take n. \n",
    "    if batch_size == None : \n",
    "        batch_size = self.Y.shape[0]\n",
    "    '''\n",
    "\n",
    "    stop_condition = False \n",
    "    i = 0\n",
    "    old_loss = 1.\n",
    "\n",
    "    while i < N_epoch and stop_condition == False: \n",
    "        optimizer.zero_grad()\n",
    "        loss = -compute_gain()\n",
    "        loss.backward()\n",
    "        #print('true grad : ', params['C'].grad)\n",
    "        #print('my_grad, ',grad_C(data[0], data[1],data[2] ,params['M'] ,params['S'] ,params['C'] ,params['beta']) )\n",
    "        print('sanity check : ', torch.norm(params['C'].grad+grad_C(data[0], data[1],data[2] ,params['M'] ,params['S'] ,params['C'] ,params['beta']) ))\n",
    "        if torch.isnan(loss).item() == True : \n",
    "            print('NAN')\n",
    "        else : last_params = params \n",
    "        optimizer.step()\n",
    "        i += 1\n",
    "        \n",
    "        # condition to see if we have reach the tolerance threshold\n",
    "        if  abs(loss.item() - old_loss) < tolerance : \n",
    "            #if max([torch.norm(param.grad) for param in params]) < tolerance  or abs(loss.item()- old_loss)>  tolerance :\n",
    "            stop_condition = True \n",
    "        old_loss = loss.item()\n",
    "        \n",
    "        running_times.append(time.time()-t0)\n",
    "        MSE_Sigma_list.append(torch.mean((torch.mm(params['C'],params['C'].T)-true_Sigma)**2).item())\n",
    "        MSE_beta_list.append(torch.mean((params['beta']-true_beta)**2).item())\n",
    "        ELBO_list.append(-loss.item())\n",
    "        \n",
    "        if i%100 == 0 and verbose : \n",
    "            print('iteration number: ', i)\n",
    "            print('-------UPDATE-------')\n",
    "            print(' MSE with Sigma : ', np.round(MSE_Sigma_list[-1],5))\n",
    "            print(' MSE with beta : ', np.round(MSE_beta_list[-1],5))\n",
    "            print('ELBO : ', np.round(-loss.item(),5))\n",
    "            print_stats(loss, last_params, optimizer)\n",
    "            \n",
    "    \n",
    "    if verbose : # just print some stats if we want to \n",
    "        if stop_condition : \n",
    "            print('---------------------------------Tolerance {} reached in {} iterations'.format(tolerance, i))\n",
    "        else : \n",
    "            print('---------------------------------Maximum number of iterations reached : ', N_epoch)\n",
    "        print_stats(loss, last_params, optimizer)\n",
    "    print(' MSE with Sigma : ', np.round(MSE_Sigma_list[-1],5))\n",
    "    print(' MSE with beta : ', np.round(MSE_beta_list[-1],5))\n",
    "    \n",
    "    return last_params, running_times, MSE_Sigma_list, MSE_beta_list, ELBO_list\n",
    "\n",
    "def print_stats(loss, params, optimizer): \n",
    "    '''\n",
    "    small function that print some stats. \n",
    "\n",
    "    It will print the actual learning rate of the optimizer, the actual log likelihood \n",
    "    and the norms of each parameter's gradient. The norm of the parameter's gradient should be low\n",
    "    when we are close to the optimum. \n",
    "    '''\n",
    "    print('---------------------------------lr :', optimizer.param_groups[0]['lr'])\n",
    "    print('---------------------------------log likelihood :', - loss.item())\n",
    "    for param_name, param in params.items(): \n",
    "        print('---------------------------------grad_{}_norm : '.format(param_name), round(torch.norm(param.grad).item(), 3))\n",
    "\n",
    "\n",
    "def ELBO(Y, O,covariates ,M ,S ,C ,beta): \n",
    "    n = Y.shape[0]\n",
    "    SrondS = torch.multiply(S,S)\n",
    "    OplusM = O+M\n",
    "    MmoinsXB = M-torch.mm(covariates, beta) \n",
    "    \n",
    "    tmp = torch.sum(  torch.multiply(Y, OplusM)  -torch.exp(OplusM+SrondS/2) +1/2*torch.log(SrondS))\n",
    "    \n",
    "    tmp -= 1/2*torch.trace(  \n",
    "                            torch.mm(  \n",
    "                                        torch.inverse(torch.mm(C,C.T)), \n",
    "                                        torch.diag(torch.sum(SrondS, dim = 0))+ torch.mm(MmoinsXB.T, MmoinsXB)\n",
    "                                    )\n",
    "                          )\n",
    "    tmp-= n*torch.log(torch.det(C))\n",
    "    return tmp \n",
    "\n",
    "def grad_beta(Y, O, covariates ,M ,S ,C ,beta) : \n",
    "    grad = torch.mm(torch.mm(covariates.T, M-torch.mm(covariates, beta) ), torch.inverse(torch.mm(C,C.T)))\n",
    "    return grad \n",
    "\n",
    "def grad_M(Y, O, covariates ,M ,S ,C ,beta):\n",
    "    grad = Y - torch.exp(O+M+torch.multiply(S,S)/2)-torch.mm(M-torch.mm(covariates,beta), torch.inverse(torch.mm(C,C.T)))\n",
    "    return grad \n",
    "def grad_S(Y, O, covariates ,M ,S ,C ,beta): \n",
    "    return torch.div(1,S)-torch.multiply(S, torch.exp(O+M+torch.multiply(S,S)/2))-torch.mm(S, torch.diag(torch.diag(torch.inverse(torch.mm(C,C.T)))))\n",
    "\n",
    "def grad_C(Y, O, covariates ,M ,S ,C ,beta): \n",
    "    n = Y.shape[0]\n",
    "    CCT = torch.mm(C,C.T)\n",
    "    MmoinsXB = M-torch.mm(covariates, beta) \n",
    "    big_mat = torch.diag(torch.sum(torch.multiply(S,S), dim = 0))+ torch.mm(MmoinsXB.T, MmoinsXB)\n",
    "    return torch.mm(torch.mm(torch.inverse(C),(big_mat +big_mat.T)/2.),torch.inverse(CCT)).T - Y.shape[0]*torch.inverse(C).T\n",
    "\n",
    "def grad_test(Y_, O_, covariates_,M_ ,S_ ,C_ ,beta_): \n",
    "    Y = torch.clone(Y_)\n",
    "    O = torch.clone(O_)\n",
    "    covariates = torch.clone(covariates_)\n",
    "    M = torch.clone(M_)\n",
    "    S = torch.clone(S_)\n",
    "    C = torch.clone(C_)\n",
    "    beta = torch.clone(beta_)\n",
    "    print(C_init)\n",
    "    for i in range(500): \n",
    "        grad = grad_C(Y, O, covariates ,M ,S ,C ,beta)\n",
    "        C+=0.00000015*grad \n",
    "        #M+=0.00015*grad \n",
    "        #beta+=0.00000015*grad\n",
    "        if torch.isnan(ELBO(Y, O, covariates ,M ,S ,C ,beta)) == True : \n",
    "            print('nan')\n",
    "        if i % 200 == 0: \n",
    "            print('ELBO : ', ELBO(Y, O, covariates ,M ,S ,C ,beta))\n",
    "        #print('norm grad : ', torch.norm(grad))\n",
    "#grad_test(Y_sampled, O, covariates, M_init,S_init, C_init, beta_init )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "uniform-olympus",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test : tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11])\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n"
     ]
    }
   ],
   "source": [
    "def make_lower_trig(vectC,n): \n",
    "    return torch.reshape(vectC,(n,-1))\n",
    "test = torch.arange(12)\n",
    "print('test :', test)\n",
    "print(make_lower_trig(test,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "pregnant-center",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "grad can be implicitly created only for scalar outputs",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-7bfe753d1c01>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mother\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mother\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads)\u001b[0m\n\u001b[1;32m     48\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"grad can be implicitly created only for scalar outputs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m                 \u001b[0mnew_grads\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: grad can be implicitly created only for scalar outputs"
     ]
    }
   ],
   "source": [
    "test = torch.arange(12,dtype =torch.float, requires_grad = True)\n",
    "optimizer = torch.optim.Adam(params=[test])\n",
    "other = test*2\n",
    "other.backward(retain_graph=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "compact-lending",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "other :  tensor([ 0.0000,  1.9398,  3.9398,  5.9398,  7.9397,  9.9397, 11.9397, 13.9397,\n",
      "        15.9397, 17.9397, 19.9397, 21.9397], dtype=torch.float32,\n",
      "       grad_fn=<MulBackward0>)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-06fbe78766d3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'other : '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    243\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    244\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 245\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    246\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    247\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    143\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time, but the saved intermediate results have already been freed. Specify retain_graph=True when calling .backward() or autograd.grad() the first time."
     ]
    }
   ],
   "source": [
    "for i in range(10): \n",
    "    print('other : ', other)\n",
    "    loss = torch.norm(other)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "assured-capital",
   "metadata": {},
   "source": [
    "\n",
    "$$\\boxed{\\begin{align} J_{\\theta, q}(Y) &=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S\\odot S}{2}\\right) + \\frac 12 \\log (S \\odot S) \\right)\\mathbb{1}_p \\\\\n",
    "& \\quad  - \\frac 12\\operatorname{tr}\\left((CC^{\\top})^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right) \\\\\n",
    "& \\quad  - n \\log |C|+ cst \n",
    "\\end{align}}$$\n",
    "\n",
    "$$\\boxed{\\begin{align} J_{\\theta, q}(Y) &=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S\\odot S}{2}\\right) + \\frac 12 \\log (S \\odot S) \\right)\\mathbb{1}_p \\\\\n",
    "& \\quad  - \\frac 12\\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right) \\\\\n",
    "& \\quad  - \\frac n2 \\log |\\Sigma|+ cst \n",
    "\\end{align}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "handled-instruction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sanity check :  tensor(1.0281e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.1105e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0382e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.2265e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.1456e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0595e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0207e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(9.1386e-14, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0117e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.1218e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(9.4662e-14, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0940e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(9.8204e-14, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0215e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.2767e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.3180e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.2892e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.2314e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0904e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.3422e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.0847e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.5102e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.5851e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.3556e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.5957e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.8039e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.5319e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.8512e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(1.9392e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(3.3498e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(2.6361e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(2.8218e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(2.9788e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(2.7288e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(2.8691e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(4.0941e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(3.6887e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(4.9614e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.5831e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(3.2127e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(4.7692e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(7.0695e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(4.3587e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(4.5120e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.8540e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.1248e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.4786e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.1853e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.1371e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(8.0438e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.1106e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.2875e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.9191e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.3408e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.4940e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.7252e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.5121e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(4.7967e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.1821e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.6765e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(7.5525e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.4687e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.5004e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.1577e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.3838e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.3939e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.7393e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.4964e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.3207e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.1777e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.9578e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.4247e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.3161e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.4285e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.5103e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.4934e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.9432e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.3172e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.8058e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.0387e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.9418e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.5308e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.7464e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.4252e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.3970e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.6269e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.6986e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.9335e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.8949e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.4165e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.1415e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.2904e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.5784e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(6.1608e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(7.0651e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.6137e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.3064e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.7380e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.0475e-13, grad_fn=<CopyBackwards>)\n",
      "sanity check :  tensor(5.4382e-13, grad_fn=<CopyBackwards>)\n",
      " MSE with Sigma :  0.23647\n",
      " MSE with beta :  1.00747\n",
      "CPU times: user 198 ms, sys: 8.14 ms, total: 206 ms\n",
      "Wall time: 196 ms\n"
     ]
    }
   ],
   "source": [
    "model = PLN_full(C_init, beta_init, M_init, S_init)\n",
    "%time model.full_grad_ascent(data, N_epoch = 100,verbose=False, lr = 0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "driven-peoples",
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 50; p = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "ahead-integer",
   "metadata": {},
   "outputs": [],
   "source": [
    "true_Sigma = torch.from_numpy(toeplitz(0.5**np.arange(p)))\n",
    "true_C = torch.cholesky(true_Sigma)\n",
    "true_beta = torch.randn(d, p)\n",
    "\n",
    "covariates = torch.rand((n,d))\n",
    "O =  1+torch.zeros((n,p))\n",
    "\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled = torch.from_numpy(sample_model.sample(true_Sigma,true_beta, O, covariates)) \n",
    "\n",
    "data = [Y_sampled.double(), O, covariates]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "resistant-omaha",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "\n",
    "noise = torch.randn(p) \n",
    "Sigma_init =  torch.diag(noise**2)\n",
    "Sigma_init = torch.from_numpy(toeplitz(0.4**np.arange(p)))\n",
    "C_init = torch.cholesky(Sigma_init)\n",
    "beta_init = torch.rand((d, p))\n",
    "\n",
    "M_init = torch.ones((n,p))/100# some random values to initialize we divide to avoid nan values \n",
    "S_init = torch.ones((n,p))/8 # some random values to initializ. we divise to avoid nan values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "united-emperor",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1000, 4])"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "covariates.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "dietary-outreach",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30])"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "beta_init.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "willing-assignment",
   "metadata": {},
   "source": [
    "Here we have a PLN model described as the following : \n",
    "\n",
    "- Consider $n$ sites $(i=1 \\ldots n)$\n",
    "\n",
    "- Measure $x_{i}=\\left(x_{i h}\\right)_{1 \\leq h \\leq d}$ :\n",
    "$x_{i h}=$ given environmental descriptor (covariate) for site $i$\n",
    "(altitude, temperature, latitude, ...)\n",
    "\n",
    "- Consider $p$ species $(j=1 \\ldots p)$ Measure $Y=\\left(Y_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p}$ :\n",
    "\n",
    "- Measure $Y = Y_{i j}=$ number of observed individuals from species $j$ in site $i$ (abundance). \n",
    "\n",
    "- Associate a random vector $Z_{i}$ with each site Assume that the unknown $\\left(Z_{i}\\right)_{1 \\leq i \\leq n}$ are independant such that:\n",
    "$$\n",
    "Z_{i} \\sim \\mathcal{N}_{p}(x_i \\beta, \\Sigma) \\quad \\Sigma = CC^{\\top}\n",
    "$$\n",
    "\n",
    "and $C$ is a lower triangular matrix. \n",
    "- Assume that the observed abundances $\\left(Y_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p}$ are independent conditionally on the $Z=\\left(Z_{i}\\right)_{i}$\n",
    "\n",
    "\n",
    "$$\n",
    "\\left(Y_{i j} \\mid Z_{i j}\\right) \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "$$\n",
    "\n",
    "Where $O = (o_{ij})_{1\\leq i\\leq n, 1\\leq j\\leq p}$ are known offsets. \n",
    "\n",
    "The unknown parameter is $\\theta = (C,\\beta)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "seasonal-plaintiff",
   "metadata": {},
   "source": [
    "$Z$ being a latent variable, we want to use the EM algorithm to derive the maximum likelihood estimator. However, it requires to compute \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{\\theta}\\left[p_{\\theta}\\left(Z_{i} \\mid Y\\right)\\right]=\\mathbb{E}_{\\theta}\\left[p_{\\theta}\\left(Z_{i} \\mid Y_{i}\\right)\\right] \\propto \\int_{\\mathbb{R}^{p}} p_{\\theta}\\left(Z_{i}\\right) \\prod_{j} p_{\\theta}\\left(Y_{i j} \\mid Z_{i j}\\right) \\mathrm{d} Z_{i}\n",
    "$$ which is intractable in practice. \n",
    "\n",
    "We thus choose the variationnal approximation. We set\n",
    "\n",
    "$$ \n",
    "q^{\\star} = \\underset{q \\in \\mathcal{Q_{gauss}}}{\\operatorname{argmax}} J_{\\theta,q}(Y) \n",
    "$$\n",
    "<br>\n",
    "$$\n",
    "\\begin{align} J_{\\theta, q}(Y)& =\\log p_{\\theta}(Y)-K L\\left[q(Z) \\| p_{\\theta}(Z \\mid Y)\\right]                                    \\\\ \n",
    "                              & = \\mathbb{E}_{q}\\left[\\log p_{\\theta}(Y, Z)\\right] \\underbrace{-\\mathbb{E}_{q}[\\log q(Z)]}_{\\text {entropy } \\mathcal{H}(q)}    \\end{align}\n",
    "$$\n",
    "\n",
    "where \n",
    "\n",
    "$$\n",
    "\\mathcal{Q}_{\\text {Gauss }}=\\{\n",
    "q=\\left(q_{1}, \\ldots q_{n}\\right), q_{i} \\sim \\mathcal{N}\\left(M_{i}, \\operatorname{diag} (S_{i}\\odot S_i ), M_i \\in \\mathbb{S} ^p, S_i \\in \\mathbb{R} ^p\\right)\\}\n",
    "$$\n",
    "\n",
    "\n",
    "The Variational EM (VEM) consists in alternate between two steps : \n",
    "- VE step: update $q$\n",
    "$$\n",
    "q^{h+1}=\\underset{q \\in \\mathcal{Q_{gauss}}}{\\arg \\max } J_{\\theta^{h}, q}(Y)=\\underset{q \\in \\mathcal{Q_{gauss}}}{\\arg \\min } K L\\left[q(Z) \\| p_{\\theta^{h}}(Z \\mid Y)\\right]\n",
    "$$\n",
    "- M step: update $\\theta$\n",
    "$$\n",
    "\\theta^{h+1}=\\underset{\\theta}{\\arg \\max } J_{\\theta, q^{h+1}}(Y)=\\underset{\\theta}{\\arg \\max } \\mathbb{E}_{q^{h+1}}\\left[\\log p_{\\theta}(Y, Z)\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "single-skiing",
   "metadata": {},
   "source": [
    "Let's compute the ELBO $J_{\\theta, q}(Y)$\n",
    "\n",
    "\n",
    "$$\n",
    "J_{\\sigma, q}(Y)=\\underbrace{\\mathbb{E}_{q}\\left[\\log p_{\\theta}(Y \\mid Z)\\right]}_{(1)}+\\underbrace{E_{q}\\left[\\log p_{\\theta}(Z)\\right]}_{(2)}+\\underbrace{H(q)}_{(3)}\n",
    "$$\n",
    "$$\n",
    "\\begin{align}\n",
    "(1)& =\\sum_{i} \\mathbb{E}_{q}\\left[\\log p_{\\theta}\\left(Y_{i} \\mid Z\\right)\\right] \\\\\n",
    "&=\\sum_{i} \\mathbb{E}_{q}\\left[\\log p_{\\theta}\\left(Y_{i} \\mid Z_{i}\\right)\\right] \\\\\n",
    "&=\\sum_{i, j} \\mathbb{E}_{q}\\left[\\log p_{\\theta}\\left(Y_{i j} \\mid Z_{i j}\\right)\\right] \\\\\n",
    "& =\\sum_{i, j} \\mathbb{E}_{q}\\left[Y_{i j}\\left(o_{i j}+Z_{i j}\\right)-\\exp \\left(o_{i j}+Z_{i j}\\right)\\right] + cst\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "We need to compute some moments of $Z$ under $q$. \n",
    "\n",
    "$$\n",
    "\\mathbb{E}_{q}\\left[Z_{i j}\\right]=M_{i j} \\quad \\quad E_{q}\\left[\\operatorname{exp}\\left(Z_{i j}\\right)\\right]=\\frac{1}{2} \\operatorname{exp}\\left(M_{i j}+\\frac{(S_{ij})^2}{2}\\right)\n",
    "$$\n",
    "\n",
    "So that \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(1) &=\\sum_{i, j} Y_{i j}\\left(o_{i j}+M_{i j}\\right)-\\frac{1}{2} \\exp \\left(o_{i j}+M_{i j}+\\frac{(S_{i_{jj}})^2}{2}\\right) + cst \\\\\n",
    "&=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S \\odot S}{2}\\right)\\right)\\mathbb{1}_p + cst \n",
    "\\end{aligned}\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-nelson",
   "metadata": {},
   "source": [
    "Where we have denoted $M = (M_{ij})_{1\\leq i\\leq n, 1\\leq j\\leq p}$ and $ S = (S_{ij})_{1\\leq i\\leq n, 1\\leq j\\leq p}$. The exponential is applied pointwise on the last equation. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(3)=H(q) &=\\sum_{i} H\\left(q_{i}\\right) \\\\\n",
    "&=\\sum_{i} \\log \\left(\\sqrt{(2 \\pi e)^{p}\\left|S_{i}\\right|}\\right) \\\\\n",
    "&=\\frac 12\\sum_{i} \\log \\left|(S_{i} \\odot S_i)^2 \\right|+cst \\\\\n",
    "&=\\sum_{i j} \\log S_{i j}+cst \\\\\n",
    "& =  \\mathbb{1}_n ^{\\top}(\\log S )\\mathbb{1}_p+cst\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Where the log is applied pointwise at the last equation. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(2)=\\mathbb{E}_{q}\\left[\\log p_{\\theta}(Z)\\right] &=\\sum_{1} E_{q}\\left[\\log p_{\\theta}\\left(Z_{i}\\right)\\right] \\\\\n",
    "&=-\\frac{n}{2} \\log |\\Sigma|+\\sum_{i} \\mathbb{E}_{q}\\left[-\\frac{1}{2}\\left(Z_{i}-X_{i} \\beta\\right)^{\\top} \\Sigma^{-1}\\left(Z_{i} - X_{i} \\beta\\right)\\right] +cst\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text { Let } V \\sim \\mathcal{N} \\left(\\mu, \\Lambda), \\mu \\in \\mathbb{R}^{p}, \\Lambda \\in \\mathcal S _p ^{++}\\right.  \\\\\n",
    "\\text { Let's compute } \\; \\mathbb{E}\\left[V^{\\top} \\Sigma^{-1} V\\right]\n",
    "$\n",
    "\n",
    "We denote $\\Sigma ^{-1 / 2}$ the square root Matrix of $\\Sigma^{-1}$. It exists since $\\Sigma ^{-1} \\in \\mathcal{S}_p^{++}$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[V^{\\top} \\Sigma^{-1} V\\right] &=\\mathbb{E}\\left[V^{\\top}\\Sigma ^{-1 / 2} \\Sigma^{-1 / 2} V\\right]\\\\\n",
    "&=\\mathbb{E}\\left[\\left(\\Sigma^{-1 / 2} V\\right)^{\\top}\\left(\\Sigma^{-1 / 2} V\\right)\\right] \\\\\n",
    "&=\\mathbb{E} \\|\\Sigma^{-1 / 2} V \\|_{2}^{2}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "$\n",
    "\\text {Let } \\tilde{V}=\\Sigma^{-1 / 2} V, \\quad \\tilde{V} \\sim \\mathcal{N}\\left(\\Sigma^{-1 / 2} \\mu,  \\Sigma^{-1 / 2} \\Lambda \\Sigma^{-\\frac{1}{2}}\\right)\n",
    "$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\mathbb{E}\\left[V^{\\top} \\Sigma^{-1} V\\right] &=\\mathbb{E}\\|\\widetilde{V}\\|_{2}^{2} \\\\\n",
    "&=\\sum \\mathbb E \\widetilde{V}_{j}^{2} \\\\\n",
    "&=\\sum \\operatorname{var}\\left(\\widetilde{V}_{j}\\right)^{2}+\\mathbb{E}\\left[\\widetilde{V}_{j}\\right]^{2}\\\\\n",
    "&=\\sum_{j}\\left(\\Sigma^{-1 / 2} \\Lambda \\Sigma^{-1 / 2}\\right)_{j j}+\\left(\\Sigma^{-1 / 2} \\mu\\right)^{2}_j\\\\\n",
    "&= \\operatorname{tr}\\left(\\Sigma^{-1 / 2} \\Lambda \\Sigma^{-1 / 2}\\right)+\\sum_{j}\\left(\\left(\\Sigma_{j,.}^{-1 / 2}\\right)^{\\top} \\mu\\right)^{2}\\\\\n",
    "&=\\operatorname{tr}\\left(\\Sigma^{-1} \\Lambda\\right)+\\sum_{j}\\left(\\left(\\Sigma_{j,.}^{-1/2}\\right)^{\\top} \\mu\\right)^2\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Since under $q$, $Z_{i}-X_{i} \\beta \\sim \\mathcal N (M_i - X_i \\beta, S_i \\odot S_i ) $\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(2) &=- \\frac 12 \\sum_{i} \\operatorname{tr}\\left(\\Sigma^{-1} (S_{i} \\odot S_i) \\right) - \\frac 12 \\sum_{i, j}\\left(\\left(\\Sigma_{j,.}^{-1 / 2} \\right) ^{\\top}\\left(M_{i}-X_{i} \\beta\\right)\\right)^2  - \\frac n2 \\log |\\Sigma|+ cst  \\\\\n",
    "&= - \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\sum_{i} S_{i}\\odot S_i \\right)\\right) - \\frac 12 \\sum_{i, j}\\left(\\Sigma^{-1 / 2}(M-X \\beta)\\right)_{j, i}^{\\top}\\left(\\Sigma\n",
    "^{-1 / 2}(M-X \\beta)\\right)_{i, j}  - \\frac n2 \\log |\\Sigma|+ cst  \\\\\n",
    "&=- \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\sum_{i} S_{i} \\odot S_i  \\right)\\right)- \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1 / 2}(M-X \\beta)^{\\top}(M-X \\beta) \\Sigma^{-1 / 2}\\right)  - \\frac n2 \\log |\\Sigma|+ cst  \\\\\n",
    "&=- \\frac 12 \\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right)  - \\frac n2 \\log |\\Sigma|+ cst \n",
    "\\end{aligned}\n",
    "$$\n",
    "We then have : \n",
    "\n",
    "\n",
    "$$\\boxed{\\begin{align} J_{\\theta, q}(Y) &=\\mathbb{1}_n^{\\top}\\left(Y \\odot(O+M)-\\frac{1}{2} \\exp \\left(O+M+\\frac{S\\odot S}{2}\\right) + \\frac 12 \\log (S \\odot S) \\right)\\mathbb{1}_p \\\\\n",
    "& \\quad  - \\frac 12\\operatorname{tr}\\left(\\Sigma^{-1}\\left(\\operatorname{diag}(\\mathbb{1}_n^{\\top} (S\\odot S))+(M-X \\beta)^{\\top}(M-X \\beta)\\right)\\right) \\\\\n",
    "& \\quad  - \\frac n2 \\log |\\Sigma|+ cst \n",
    "\\end{align}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ongoing-lodging",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
