{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import seaborn as sns \n",
    "import torch \n",
    "from pandas import read_csv\n",
    "import time\n",
    "#torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\mathbf{x}_{i}\\beta +W_{i}\\mathbf{C}^{\\top}, \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta} P_{\\theta}(Y)$$\n",
    "\n",
    "But we need to integrate out $W$ in order to compute the quantity inside the max : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i}\\right) &=\\int P_{\\theta}\\left(Y_{i}, W\\right) d W \\\\\n",
    "&=\\int P_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This integral being untractable, we are going to approximate it with Monte Carlo methods : \n",
    "\n",
    "$$\n",
    "\\int p_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W \\approx \\frac{1}{K} \\sum_{k = 1 }^Kp_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)\n",
    "$$\n",
    "$$W_{i,k} \\sim \\mathcal N (0, I_q)$$\n",
    "\n",
    "The larger the $K$ the better the approximation.  \n",
    "\n",
    "Let's compute $p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)$. \n",
    "\n",
    "\n",
    "First, \n",
    "\n",
    "$$\n",
    "P\\left(W_{i,k}\\right)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2}\\left\\|W_{i,k}\\right\\|_{2}^{2}\\right)\n",
    "$$ \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical purposes, we may want to use a logarithmic scale and apply the exponential function after. Indeed, $Y_{ij}$ can go up to a thousand, and computing this factorial would give infinite values. \n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=\\sum_{j=1}^{p} - \\ln \\left(Y_{i j} !\\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+Z_{i j}\\right)\n",
    "$$\n",
    "\n",
    "If we consider the whole likelihood : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ln p_{\\theta}\\left(Y \\mid W_{k}\\right) &=\\sum_{i=1}^{n} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i k}\\right) \\\\\n",
    "&=\\sum_{i=1}^{n} \\sum_{j} - \\ln \\left(Y_{ij} ! \\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(0_{i j}+Z_{i j}\\right) \\\\\n",
    "&=1_{n}^{T}\\left[-\\ln (Y !)-\\exp (0+Z)+Y \\odot (0+Z)\\right] 1_{p} \\\\\n",
    "Z=& X \\beta+W_{k} C^{\\top}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We are going tu use the Stirling Formula in order to compute the log of the factorial, to avoid computing directly the factorial.  \n",
    "\n",
    "We now need to compute the gradients. Since \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i}| W_{i,k}\\right) \\nabla_{\\theta} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\n",
    "$$\n",
    "\n",
    "We get : \n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-x_{i}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ x_{i}^{\\top}Y_i\\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-W_{i,k}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ W_{i,k}^{\\top}Y_i\\right]\n",
    "$$\n",
    "This is if we take only one sample $Y_i$. If we take the whole dataset (or a mini-batch), we get (writed in matrix form) :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying to optimize the likelihood, we have found that the likelihood is very small (the order of the log likelihood is about $1e^{-3}$ for some samples which makes the exponential 0 numerically. We can't optimize the log likelihood directly due to the integral : \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p_{\\theta}(Y) &=\\log \\left(\\int p_{\\theta}(Y, W)dW\\right) \\\\\n",
    "& \\approx \\log \\left(\\frac{1}{K}\\sum^K p_{\\theta}\\left(Y, W_{k}\\right)\\right)\\\\\n",
    "& \\neq \\frac{1}{K} \\sum^{K} \\log \\left(p_{\\theta}\\left(Y, W_{k}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Indeed, we must compute the likelihood which results in numerical zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$C :  (p,q)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be smaller (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*toeplitz(0.95**np.arange(block_size))\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.98**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    w,v = SLA.eigh(Sigma)\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:]))\n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes anything since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        torch.manual_seed(0)\n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        \n",
    "        #self.C = torch.clone(true_C)\n",
    "        \n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        \n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        \n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        self.running_times = list()\n",
    "        self.MSE_Sigma_list = list()\n",
    "        self.MSE_beta_list = list()\n",
    "        \n",
    "        self.t0 = time.time()\n",
    "        \n",
    "    def single_likelihood_test(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        '''\n",
    "        computes the likelihood of the whole dataset. \n",
    "        '''\n",
    "        likelihood = 1\n",
    "        N_samples = int(1/acc)\n",
    "        for Y_b, covariates_b, O_b in self.get_batch(self.batch_size): \n",
    "            W = torch.randn(N_samples, self.batch_size, self.q)\n",
    "            batch_like = self.batch_likelihood(Y_b,covariates_b,O_b,W)\n",
    "            #print('added : ', batch_like)\n",
    "            likelihood *=  batch_like\n",
    "        return likelihood\n",
    "            \n",
    "    def fit(self, N_iter, acc,lr_beta,lr_C,C_optim = False): \n",
    "        '''\n",
    "        fit the data. DOes not work yet. You can choose to optimize beta or C\n",
    "        '''\n",
    "        optim_beta = torch.optim.Rprop([self.beta], lr = lr_beta)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = lr_C)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            compteur_beta = 0\n",
    "            compteur_C = 0 \n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q)\n",
    "                ma = torch.max(Y_b).item()\n",
    "                #if C_optim : \n",
    "                optim_C.zero_grad()\n",
    "\n",
    "                grad_C = self.grad_batch_C(Y_b, covariates_b, O_b,W)\n",
    "                loss_C = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                loss_C.backward()\n",
    "\n",
    "                diff= torch.norm(grad_C+self.C.grad)/(torch.norm(grad_C)+torch.norm(self.C.grad))\n",
    "                if loss_C <0  :\n",
    "                    print('MSE Sigma ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    optim_C.step()\n",
    "                    self.running_times.append(time.time()-self.t0)\n",
    "                    self.MSE_Sigma_list.append((torch.mean((self.get_Sigma()-true_Sigma)**2)).item())\n",
    "                    '''\n",
    "                    if ma > 500 : \n",
    "                        print('-------------------------------GOOD_ONE : ', Y_b )\n",
    "                        tmp = self.batch_likelihood(Y_b,covariates_b, O_b,W, verbose = True)\n",
    "                        print('loss : ', tmp)\n",
    "                    '''\n",
    "                else: \n",
    "                        compteur_C+=1\n",
    "                        #print('BAD_ONE  : ', Y_b)\n",
    "                        #self.batch_likelihood(Y_b,covariates_b, O_b,W, verbose = True )\n",
    "                #else : \n",
    "                optim_beta.zero_grad()\n",
    "                grad_beta = self.grad_batch_beta(Y_b,covariates_b, O_b,W)\n",
    "                loss_beta = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                loss_beta.backward()\n",
    "                #self.beta.grad = -grad_beta/torch.mean(torch.abs(grad_beta))\n",
    "                if loss_beta <0 : \n",
    "                    #print('loss : ',loss)\n",
    "                    #print('MSE beta_before : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    self.MSE_beta_list.append((torch.mean((self.beta-true_beta)**2)).item())\n",
    "                    optim_beta.step()\n",
    "                    print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    self.running_times.append(time.time()-self.t0)\n",
    "                    '''\n",
    "                    if ma > 500 : \n",
    "                        print('-------------------------------GOOD_ONE : ', Y_b )\n",
    "                        tmp = self.batch_likelihood(Y_b,covariates_b, O_b,W, verbose = True)\n",
    "                        print('loss : ', tmp)\n",
    "                    '''\n",
    "                else : \n",
    "                    compteur_beta += 1\n",
    "                    #print('BAD_ONE  : ', Y_b)\n",
    "                \n",
    "                    #print('beta : ',self.beta )\n",
    "                print('loss_beta : ', loss_beta)\n",
    "                print('loss_Sigma; ', loss_C)\n",
    "            print('----------------------------------------------------------------------compteur_beta : ', compteur_beta,' out_of :', self.n//self.batch_size)\n",
    "            print('----------------------------------------------------------------------compteur_C : ', compteur_C,' out_of :', self.n//self.batch_size)\n",
    "\n",
    "    \n",
    "    def batch_likelihood(self, Y_b,covariates_b, O_b, W, mean = True, verbose = False): \n",
    "        norm_W = torch.sum(torch.norm(W,dim = 2)**2, axis = 1) \n",
    "        log_fact = torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b),axis = (1,2))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b),axis = (1,2))\n",
    "        result = (-1/2*norm_W -log_fact +exp_term+data_term) \n",
    "        if verbose :\n",
    "            result = (-1/2*norm_W -log_fact +exp_term+data_term) \n",
    "            #print('Z_b +O_b : ', torch.sum(covariates_b@self.beta + W@(self.C.T)+O_b ,axis = (1,2)))\n",
    "\n",
    "            print('norm_W : ', -1/2*norm_W )\n",
    "\n",
    "            print('log_factoriel', -log_fact)\n",
    "\n",
    "            print('exp_term ', exp_term)\n",
    "\n",
    "            print('data ',data_term)\n",
    "            print('somme ( = norm_W +log_fact + exp_term + data) : ', result )\n",
    "            print('result = exp(somme)  ', torch.exp(result))\n",
    "        nb_nonzero =  torch.sum(torch.exp(result)>0)\n",
    "        if mean : \n",
    "            return torch.sum(torch.exp(-1/2*norm_W -log_fact +exp_term+data_term))/nb_nonzero\n",
    "            #return torch.mean(torch.exp(-1/2*norm_W -log_fact +exp_term+data_term))\n",
    "        else : \n",
    "            return torch.exp(-1/2*norm_W -log_fact +exp_term+data_term)\n",
    "    def grad_batch_beta(self, Y_b,covariates_b, O_b, W):\n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return torch.mean(likelihood.reshape(-1,1,1)*(covariates_b.unsqueeze(2).T@(-torch.exp(Z_b)+Y_b)), axis = 0) \n",
    "        \n",
    "    def grad_batch_C(self, Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return torch.mean(likelihood.reshape(-1,1,1)*((-torch.exp(Z_b)+Y_b).permute(0,2,1)@W), axis = 0) \n",
    "        \n",
    "    def check_batch(self, Y_b,covariates_b, O_b, W): \n",
    "        optim = torch.optim.Rprop([self.beta, self.C], lr = 0.01)\n",
    "        optim.zero_grad()\n",
    "        loss = self.batch_likelihood(Y_b, covariates_b, O_b, W)\n",
    "        loss.backward()\n",
    "        print('loss : ', loss )\n",
    "        grad_C = self.grad_batch_C(Y_b, covariates_b, O_b, W)\n",
    "        true_grad = self.C.grad\n",
    "        diff = torch.norm(grad_C-true_grad)\n",
    "        print('diff : ', diff)\n",
    "        print('my_grad : ', grad_C)\n",
    "        print('true : ', true_grad)\n",
    "        \n",
    "    def fit_all(self,lr_beta,lr_C, N_iter,acc): \n",
    "        optim_beta = torch.optim.Rprop([self.beta], lr = lr_beta)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = lr_C)\n",
    "        for i in range(N_iter):\n",
    "            optim_beta.zero_grad()\n",
    "            optim_C.zero_grad()\n",
    "            loss = -self.compute_likelihood(acc)\n",
    "            loss.backward()\n",
    "            #print('grad : ', self.beta.grad)\n",
    "            optim_beta.step()\n",
    "            optim_C.step()\n",
    "            \n",
    "            print('---------------------MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "            print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "            print('likelihood--------------------------------------',self.compute_likelihood(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 200;  p = 20\n",
    "q = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(0)\n",
    "true_Sigma = torch.from_numpy(build_block_Sigma(p,q))/2\n",
    "true_C = torch.from_numpy(C_from_Sigma(true_Sigma, q))\n",
    "true_beta =torch.randn((d, p))/10\n",
    "\n",
    "covariates = torch.randn((n,d))/100\n",
    "O =  0.0+torch.zeros((n,p))\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled, Z_sampled  = sample_model.sample(true_Sigma,true_beta, O, covariates)\n",
    "Y_sampled = torch.from_numpy(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAAD4CAYAAADSIzzWAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAeY0lEQVR4nO3dfZRdVZnn8e+PSpAXeTVGYhIFnchSaQ0YM6AtIKADaRYMjvaEaVt8mU7LgA1MO4oyAzquXkvxrXXhkEULoi3iCwLSCELGFtFZgkBMIJgoEVEqROIb7zZQVc/8cXYWl6Kq7r7nnHvPyc3vk3VW3Zezz9m5dWvffffZz34UEZiZ2eDt0HQFzMy2V26Azcwa4gbYzKwhboDNzBriBtjMrCGzBnmyh//bMT1PuRj/47/1fJ65l9/Vc5myPIukGr96Np2xJzap6jGe/N3dWW+x2XNeVPlcZbgHbGbWEDfAZja8Jsbzti4k7STpx5LWSrpT0oen2EeSPitpo6TbJR3U7biVGmBJR0v6WTrhmVWOZWZWu/GxvK27x4EjIuKVwGLgaEkHT9rnGGBR2lYA53c7aOkGWNII8Ll00pcBJ0p6WdnjmZnVLWIia+t+nIiIeCTdnZ22yePLxwNfSvveBOwpad5Mx63SA14KbIyIuyPiCeCrqQJmZu0wMZG1SVoh6daObcXkQ0kakbQG2AKsioibJ+0yH7i34/5oemxaVRrgrJN1/se+8NN7Jz9tZtY/MZG1RcQFEbGkY7vgGYeKGI+IxcACYKmkAybtMtVMihlnYVRpgLNO1vkfe8fLFlY4nZlZj2q6CNcpIh4AbgCOnvTUKNDZyC0A7pvpWFUa4J5PZmY2UJk94G4kPVfSnun2zsBRwIZJu10FvC3NhjgYeDAiNs903CqBGLcAiyTtB2wClgP/pcLxzMxqFXkzHHLMA76YJh/sAHw9Iq6W9G6AiFgJXAMsAzYCjwHv6HbQ0g1wRIxJOhW4DhgBLoqIO8sez8ysdhPde7c5IuJ24MApHl/ZcTuAU3o5bqVQ5Ii4hqLVz1ImrHhkr516LjNIUu8RjA5ffkrZ+E+/gpYlY3ihSQNdC8LMbKB6vMA2aG6AzWx4tbwHXDUU+SJJWyStq6tCZma1qS8UuS+qLsZzMc+cC2dm1g6ZkXBNqXoR7kZJ+9ZUFzOzWkW0ewy478tRdoYiX7zRcRpmNkA1BWL0S98vwqWY6gsAHjjx9Z49ZGaD0+DwQg7PgjCz4dXyWRBugM1seI0/2XQNZlR1GtqlwI+A/SWNSnpXPdUyM6vBkM+COLGX/QeVrfg/PG9xqXJjJa6YfnfLHT2XKRO+DA5h7lTmFfSrtx3yEISZWUNafhGuSk64hZK+J2l9yhJ6Wp0VMzOrbIiHIMaAv4+I1ZJ2A26TtCoiflpT3czMKomWX4Srsh7wZmBzuv2wpPUUOeHcAJtZO2wPY8ApHPlAYHKWUDOz5rR8DLhyAyzp2cA3gdMj4qEpnl8BrAAYGdmTHUZ2rXpKM7M8w9wDljSbovG9JCIun2qfzlDkHZ+1wDOBzGxwhrUHrGIy64XA+oj4VH1VMjOryRD3gF8L/DVwh6Q16bEPpjxxZmbNG2tusfUcVWZB/JDyORX7qkxEG8AsjdRck3o5AWg1TgC6HRriHrCZWbsN6xiwmVnrDWsPWNJOwI3As9JxLouIc+qqmJlZZUPcA34cOCIiHknT0X4o6dqIuKmmupmZVdPyHnDpxXii8Ei6Ozttvl5hZu0xNpa3dZGz+JikwyU9KGlN2s7udtyqgRgjwG3AvwM+FxHPCEV2JJyZNaa+WUC5i4/9ICKOzT1opYwYETEeEYuBBcBSSQdMsc8FEbEkIpa48TWzgappOcqI2BwRq9Pth4Gti49VUkta+oh4ALgBOLqO45mZ1SKzAZa0QtKtHduK6Q7ZZfGxQyStlXStpJd3q16VWRDPBZ6MiAck7QwcBXys7PHMzGqXeRGuc82amXRZfGw18MI0MWEZcCWwaKbjVekBzwO+J+l24BZgVURcXeF4Zmb1Gh/P2zJ0W3wsIh7aOjEhLckwW9KcmY5ZJRT5dopueC9lej5PmfDbMokyyzpnn8N6LvMnlbswcO7mG3su4wSg1TkB6DaspnnAOYuPSdoHuD8iQtJSig7u72c6riPhzGx41ReIMeXiY8ALACJiJfBm4GRJY8CfgOXRpSfjBtjMhldNgRg5i49FxHnAeb0ct46MGCPArcCmXua/mZn1W0y0ezCojh7waRRz4nav4VhmZvVp+VoQleYBS1oA/AXw+XqqY2ZWoxpnQfRD1R7wPwLvA3abbofOUOQdRvZghx0cDWdmAzKsPWBJxwJbIuK2mfZ7WiiyG18zG6SaQpH7pWpOuONSxMdOwO6SvhwRb62namZmFbV8PnuV5Sg/EBELImJfYDnwr258zaxVhrgHbGbWbtvBNDQi4gaK1dBqN6jw5bLKhBXvHK1MJv00zsBcjcOXW6LBGQ453AM2s6EVLZ8F4QbYzIbXMA9BSLoHeBgYB8YiYkkdlTIzq0XLk3LW0QN+fUT8robjmJnVa5h7wGZmrTbW7otwVXPCBXC9pNumy6HUmWtpYuLRiqczM+tBTORtDanaA35tRNwnaS6wStKGiHha2obOXEuzd5zf7u8DZjZcWj4EUTUt/X3p5xbgCmBpHZUyM6tDTExkbU2pshjPrpJ223obeCOwrq6KmZlVNhF5W0OqDEE8D7giRUzNAr4SEd+ppVZmZnVo+RBElazIdwOvrLEutSkbElsm/LZMpuKyHt3U+7km/nBfqXPt9mfLey7j8OVqygao+xWcgUORzcyasT3khDMza6eWN8BVc8LtKekySRskrZd0SF0VMzOrbMjXA/4M8J2IeLOkHYFdaqiTmVk9Wt4DLt0AS9odOBR4O0BEPAE8UU+1zMxq0PIGuMoQxIuA3wJfkPQTSZ9P84GfxqHIZtaUGJ/I2ppSpQGeBRwEnB8RBwKPAmdO3slZkc2sMS0PxKjSAI8CoxFxc7p/GUWDbGbWCjERWVs3khZK+l6abHCnpNOm2EeSPitpo6TbJXVtD6tkRf4NcK+k/dNDRwI/LXs8M7Pa1dcDHgP+PiJeChwMnCLpZZP2OQZYlLYVwPndDlp1FsR7gEvSDIi7gXdUPJ6ZWX1qGt6NiM3A5nT7YUnrgfk8vdN5PPClKMI7b0rTdOelslOq1ABHxBpgaNIQtT0Dc5mw4h32fn4falKfsq+fQ5if4gzM04uxvBY4rWfeuab5BWkp3an23Rc4ELh50lPzgXs77o+mx/rTAJuZtVpmD7hz3fKZSHo28E3g9Ih4aPLTUx16puO5ATazoVXnWhCSZlM0vpdExOVT7DIKLOy4vwCY8WtrlfWA95e0pmN7SNLpZY9nZla7icytCxVjZRcC6yPiU9PsdhXwtjQb4mDgwZnGf6HacpQ/Axanyo0AmyiyYpiZtUKNPeDXAn8N3CFpTXrsg8ALACJiJXANsAzYCDxGxqSEuoYgjgR+ERG/qul4ZmbV1TcL4od0ud6ZZj+c0stx62qAlwOXTvVE59XFHUb2wNFwZjYoMdZ0DWZWNS09aQ7wccA3pnreochm1pSWZ6WvpQd8DLA6Iu6v4VhmZvVpsHHNUUcDfCLTDD+YmTWpyd5tjkoNsKRdgDcAf1tPdczM6jPUDXBEPAY8p6a6bJMGGb5cJlNxWX/80JE9l4k/TA4M6m7vz97ScxlwBuaqtpcMzDE+uKUCynAknJkNraHuAZuZtVlMtLsHXDUr8hlpceJ1ki6VtFNdFTMzq6rt09CqrAUxH/g7YElEHACMUARkmJm1QoSytqZUHYKYBews6UmKlPS9L1hrZtYnbR8DrpKSaBPwCeDXFAsOPxgR10/ez1mRzawpE+PK2ppSZQhiL4oUHPsBzwd2lfTWyfs5FNnMmhITytqaUuUi3FHALyPitxHxJHA58Jp6qmVmVl3bG+AqY8C/Bg5O0XB/oliS8tZaamVmVoO2x95UWZD9ZkmXAaspUjb/hIycSmZmg9L2ecBVQ5HPAc6pqS7bjbIhsYPMwFwmrFh7796HmtTHGZir29YyMDc5xSyHI+HMbGiNt3wtiKqRcKelKLg7nZDTzNpmaAMxJB0A/A2wFHgC+I6kb0fEXXVVzsysiraPAVfpAb8UuCkiHouIMeD7wAn1VMvMrLqIvK0pVRrgdcChkp6TpqItAxbWUy0zs+qGdh5wRKyX9DFgFfAIsJZiOtrTOCuymTVlfKJy3uG+qlS7iLgwIg6KiEOBPwDPGP91KLKZNaXtQxBVc8LNjYgtkl4AvAk4pJ5qmZlVNzHk84C/Kek5wJPAKRHxxxrqZGZWi6EOxIiI19VVETOzurU9iHGgkXBlXot2f34N1iAzMJfNVtyr/7XPYaXKPa7eX4tzN99Y6lzOwFxNk3/DdQ5BSLoIOBbYkrIATX7+cOBbwC/TQ5dHxP+e6ZgORTazoVXzLIiLgfOAL82wzw8i4tjcA3atnaSLJG2RtK7jsb0lrZJ0V/q5V+4JzcwGJTK3rGNF3Egx26s2OR8PFwNHT3rsTOC7EbEI+G66b2bWKhOhrK0zdVraVpQ85SGS1kq6VtLLu+3cdQgiIm6UtO+kh48HDk+3vwjcALy/t3qamfVX7iyIiLiA6uuZrwZeGBGPSFoGXAksmqlA2QGS50XEZoD0c27J45iZ9c1E5laHiHgoIh5Jt68BZkuaM1OZvsfpOSuymTUlUNZWB0n7KE2ZkbSUon39/Uxlys6CuF/SvIjYLGkesGW6HTu79rN2nO+5OWY2MGP1TkO7lGLodY6kUYpsQLMBImIl8GbgZEljFHkyl0eX+YhlG+CrgJOAj6af3yp5HDOzvqmrdwsQESd2ef48imlq2XKmoV0K/AjYX9KopHdRNLxvkHQX8IZ038ysVQY5BlxGziyI6Vr9I2uuy9TnL1HG0XNPaXsC0DIRbQDPanmMv6Pn2qHOHnA/OBLOzIZWk73bHG6AzWxojbe8B1w2FPktKRPyhKQl/a2imVk5E8rbmlI2FHkdxQLs5ZaXMjMbgAmUtTWlVChyRKyHwV2oMTMro+2XNfs+BtyZlFNOymlmA7TdX4RzJJyZNWWi5d/SPQvCzIbWeNMV6MINsJkNrSZnOOQoFYos6YS0GMUhwLclXdfvipqZ9WoYZkFMF4p8Rc11qU3ZgeaWf1gO1KASgJZNlFnGBweYAPSTJf5fZWcVOYR5em1/ZTwEYWZDq+1DEG6AzWxotX0aWtlQ5I9L2iDpdklXSNqzr7U0MythXHlbU8qGIq8CDoiIVwA/Bz5Qc73MzCpr+3rAXRvgiLgR+MOkx66PiLF09yZgQR/qZmZWyTbfAGd4J3DtdE86KaeZNSWUtzWl0kU4SWcBY8Al0+3jUGQza0rbL8KVboAlnQQcCxzZLfOnmVkThjIUWdLRwPuBwyLisXqrZGZWj7bPAy6bFfk8YDdglaQ1klb2uZ5mZj1r+0W4sqHIF/ahLo1zBuZqBhW+XJYzMD9lexk1HNoxYDOztmv7x4wbYDMbWsMwBjxVKPJHUhjyGknXS3p+f6tpZta78cytKWVDkT8eEa+IiMXA1cDZNdfLzKyyCSJryzFVZ3TS85L0WUkbUwf1oG7HLBuK/FDH3V1p/1CLmW2Hap4FcTHP7Ix2OgZYlLYVwPndDlglEOMfgLcBDwKvn2E/Z0U2s0bU2TOMiBsl7TvDLscDX0qBaTdJ2lPSvIjYPF2B0mtBRMRZEbGQIgz51Bn2uyAilkTEEje+ZjZIuT3gzjVr0raixOnmA/d23B9Nj02rjlkQXwG+DZxTw7HMzGozljn3u3PNmgqmmnMxYwVK9YAlLeq4exywocxxzMz6KTK3mowCCzvuLwDum6lA1x5wCkU+HJiTMiGfAyyTtD9F7/1XwLtLVtjMrG8GHAl3FXCqpK8C/x54cKbxX3AocmUOX66mbEhsmfDbMpmKyzp07st7LjMe5ZqL//fb9T2X2V4yMOdOMcsxTWd0NkBErASuAZYBG4HHgHd0O6Yj4cxsaNU8C2Kqzmjn8wGc0ssx3QCb2dBq+2I8pUKRO557r6SQNKc/1TMzK2+cyNqaUjYUGUkLgTcAv665TmZmtWj7esClQpGTTwPvw2HIZtZSkfmvKWVTEh0HbIqItd2upjoU2cya0vYx4J4bYEm7AGcBb8zZ31mRzawpdU5D64cykXAvBvYD1kq6hyLaY7WkfeqsmJlZVQOOhOtZzz3giLgDmLv1fmqEl0TE72qsl5lZZWPbeg94mqzIZmatt81fhMuI/ti3ttpsJ8r+uh3C/JS2Z2AuE1Y8otKrww7MtpaBeeguwpmZbSua7N3mcANsZkOr7T3gslmRPyRpU8qKvEbSsv5W08ysd+MRWVtTSociA5+OiMVpu6beapmZVVdnVuR+yLkI1y0RnZlZK7V9DLjKZddTJd2ehij2mm6nzmR3ExOPVjidmVlvtvnFeKZxPkVE3GJgM/DJ6XZ0VmQza8o2PwQxlYi4f+ttSf8EXF1bjczMatL2IYiyq6HN60g2dwLwjMXazcya1uQMhxxlsyIfLmkxRVDXPcDf9q+KZmbltH01NGdF3oY4A3M1g8zAXCZTcVllMjCXfS3KNGhNDgO0PRDDkXBmNrSGcgzYzGxb0PYhiNJZkSW9R9LPJN0p6dz+VdHMrJyIyNqaktMDvhg4D/jS1gckvR44HnhFRDwuae40Zc3MGtNkyvkcZUORTwY+GhGPp3229KFuZmaVbPNDENN4CfA6STdL+r6kV0+3o0ORzawpdQ5BSDo6DbtulHTmFM8fLunBjlUiz+52zLIX4WYBewEHA68Gvi7pRTHF/8RZkc2sKXX1gCWNAJ8D3gCMArdIuioifjpp1x9ExLG5xy3bAx4FLo/Cjymm280peSwzs76oMSfcUmBjRNwdEU8AX6W4DlZJ2Qb4SuAIAEkvAXYEnBXZzFold0H2zqHStK2YdKj5wL0d90fTY5MdImmtpGsldY2QKRuKfBFwUZqa9gRw0lTDD2ZmTcodgugcKp3GVOGQkw++GnhhRDySsgRdCSya6bxVsiK/tVtZa54zMFfX9gzMg6zfDiXeUE2GA9c4C2IUWNhxfwFwX+cOEfFQx+1rJP0fSXMiYtrRgfbnwTYzK6nGWRC3AIsk7SdpR2A5cFXnDpL2Ufpkk7SUon39/UwHzRmCuAg4FtgSEQekx74G7J922RN4ICIW5/wvzMwGpa4ecESMSToVuA4YAS6KiDslvTs9vxJ4M3CypDHgT8DybkOzpSLhIuI/b70t6ZPAg739d8zM+q/OxXhS8uFrJj22suP2eRRtZbZKSTlTd/svSTMizMzaZDzavSBl1dXQXgfcHxF31VEZM7M6tX1yVtUG+ETg0pl2SPPpVgBoZA+cmNPMBqXta0GUboAlzQLeBLxqpv0cimxmTRnmBdmPAjZExGhdlTEzq9NEy4cgchZkvxT4EbC/pFFJ70pPLafL8IOZWZNqXAuiL0pHwkXE22uvjZlZjYZ9FoQNKWdgrmaQ4cFlLjSVCSmGcnUse646tH0Iwg2wmQ2ttl+EK5WUU9JiSTelVd9vTXHPZmatMhGRtTUlZzGei4GjJz12LvDhtP7D2em+mVmrDMNFuKlCkQPYPd3eg0nLspmZtcF4jDddhRmVHQM+HbhO0icoetGvmW5HR8KZWVPaHopcdj3gk4EzImIhcAZw4XQ7RsQFEbEkIpa48TWzQZogsramlG2ATwIuT7e/QZGwzsysVepMS98PZRvg+4DD0u0jAK+GZmat0/ZZEGWTcv4N8Jm0IM+/kcZ4zczapO3zgKsk5ZxxFTTb/jh6rpqyX4XLNDJlA3TLRLUNMkHpZA5FNjNrSNtnQbgBNrOh1fa1IMqGIr9S0o8k3SHpXyTtPtMxzMyaMAyzIC7mmaHInwfOjIg/A64A/kfN9TIzq2ybnwccETcCf5j08P7Ajen2KuA/1VwvM7PKhqEHPJV1wHHp9luAhdPtKGlFWjHt1omJR0uezsysd+MxkbU1pWwD/E7gFEm3AbsBT0y3o0ORzawp23wgxlQiYgPwRgBJLwH+os5KmZnVoe3T0Er1gCXNTT93AP4nsLLOSpmZ1WGbXw94mlDkZ0s6Je1yOfCFvtXQzKyktveAq4Qif6bmupiZ1artgRjZ0zT6vQEr2lpmWM/V9vr5tfBrMexb4xXo+EXc2tYyw3quttfPr4Vfi2Hfyk5DMzOzitwAm5k1pE0N8AUtLjOs52p7/QZ5rrbXb5Dnanv9hobSOIyZmQ1Ym3rAZmbbFTfAZmYNabwBlnS0pJ9J2ijpzMwyz1gkPqPMQknfk7Re0p2STssos5OkH0tam8p8uIfzjUj6iaSreyhzT1rkfo2kW3sot6ekyyRtSP+/Q7rsv386x9btIUmnZ5znjPQ6rJN0qaSdMut3Wipz53TnmWbh/70lrZJ0V/q5V2a5t6RzTUhaklnm4+n1u13SFZL2zCjzkbT/GknXS3p+zrk6nnuvpJA0J+NcH5K0qeN3tiz3XJLek/7G7pR0bsa5vtZxnnskrckos1jSTVvfu5KW5tRP23tyhybnwAEjwC+AFwE7AmuBl2WUOxQ4CFjXw7nmAQel27sBP+92Loqckc9Ot2cDNwMHZ57vvwNfAa7uoY73AHNKvI5fBP5rur0jsGePv4PfAC/sst984JfAzun+14G3Zxz/AIrlS3ehiLz8v8CinN8pcC7Fwv8AZwIfyyz3Uoo1q28AlmSWeSMwK93+2ORzTVNm947bfweszH2vUizheh3wq8m/82nO9SHgvb3+XQCvT6/5s9L9uTn163j+k8DZGee5Hjgm3V4G3JBZv1uAw9LtdwIf6fX9vy1vTfeAlwIbI+LuiHgC+CpwfLdCMfUi8d3KbI6I1en2w8B6ikZlpjIREY+ku7PT1vWqpaQFFCvEfb6XOpaRegyHAhcCRMQTEfFAD4c4EvhFRPwqY99ZwM6SZlE0qPdllHkpcFNEPBYRY8D3gRMm7zTN7/R4ig8X0s//mFMuItZHxM+mq9A0Za5P9QO4CViQUeahjru7MsV7Y4b36qeB9/VYZkbTlDsZ+GhEPJ722ZJ7LkkC/hK4NKNMAFt7r3swxXtjmnLbdXKHphvg+cC9HfdH6dIo1kHSvsCBFD3abvuOpK9gW4BVEdG1DPCPFH9cva70HMD1km6TtCKzzIuA3wJfSEMen5fUy8LLy5n0BzZlxSI2AZ8Afg1sBh6MiOszjr8OOFTScyTtQtE7mnYB/0meFxGb0/k3A3Mzy1X1TuDanB0l/YOke4G/As7OLHMcsCki1vZYr1PTkMdFUw3HTOMlwOsk3Szp+5Je3cP5XgfcHxF3Zex7OvDx9Fp8AvhA5jmykzsMo6YbYE3xWF/nxUl6NvBN4PRJPZgpRcR4RCym6BEtlXRAl+MfC2yJiNtKVO+1EXEQcAzFgveHZpSZRfG17vyIOBB4lOLreleSdqR4838jY9+9KHqk+wHPB3aV9NZu5SJiPcVX+lXAdyiGmcZmLNQgSWdR1O+SnP0j4qyIWJj2PzXj+LsAZ5HZWHc4H3gxsJjiA/CTmeVmAXsBB1Pkbvx66tnmOJGMD+fkZOCM9FqcQfpGliE7ucMwaroBHuXpn3gLyPtaW4qk2RSN7yURcXkvZdPX+ht4ZoLSyV4LHCfpHoohlSMkfTnzHPeln1sokp0+40LGFEaB0Y6e+WUUDXKOY4DVEXF/xr5HAb+MiN9GxJMUy5C+JuckEXFhRBwUEYdSfAXN6VEB3C9pHkD6uaXL/pVIOgk4FvirSIOSPfgKeV+fX0zxIbY2vUcWAKsl7TNToYi4P3UGJoB/Iu+9AcX74/I0nPZjim9lc7qUIQ0zvQn4WuZ5TqJ4T0DxgZ5Vv4jYEBFvjIhXUTT2v8g831BougG+BVgkab/UG1sOXNWPE6VP/QuB9RHxqcwyz916NVzSzhSN0IaZykTEByJiQUTsS/H/+deI6NpTlLSrpN223qa4KNR1lkdE/Aa4V9L+6aEjgZ92K5f00sP5NXCwpF3Sa3kkxTh6V3pqAf8XUPxR557zKoo/bNLPb2WW65mko4H3A8dFxGOZZRZ13D2OLu8NgIi4IyLmRsS+6T0ySnFx+DddzjWv4+4JZLw3kiuBI9IxXkJxkfZ3GeWOAjZExGjmee4DDku3jyDzQ1bbe3KHpq8CUowJ/pzik++szDKXUnwNe5LiDfyujDJ/TjG8cTuwJm3LupR5BfCTVGYdk64GZ5zzcDJnQVCM5a5N2525r0Uquxi4NdXzSmCvjDK7AL8H9ujhPB+maGTWAf9MurKeUe4HFB8Ka4Ejc3+nwHOA71L8MX8X2Duz3Anp9uPA/cB1GWU2UlyP2PreWJlR5pvptbgd+Bdgfq/vVaaY+TLNuf4ZuCOd6ypgXuZrsSPw5VTP1cAROfUDLgbe3cPv6s+B29Lv+GbgVZnlTqP4+/858FFSdO72sjkU2cysIU0PQZiZbbfcAJuZNcQNsJlZQ9wAm5k1xA2wmVlD3ACbmTXEDbCZWUP+P8mGIB0uzWgnAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(true_Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.0119)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(covariates@true_beta))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 475,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(133)\n"
     ]
    }
   ],
   "source": [
    "print(torch.max(Y_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Sigma  0.22556577480625475\n",
      "MSE beta :  1.0721475247676153\n",
      "loss_beta :  tensor(-7.3490e-112, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.6706e-133, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.2176472926777304\n",
      "MSE beta :  1.05530545386776\n",
      "loss_beta :  tensor(-9.5494e-87, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.4707e-103, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.22742917792548295\n",
      "MSE beta :  1.0536554191613248\n",
      "loss_beta :  tensor(-1.7666e-99, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.6496e-121, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.23423742441713014\n",
      "MSE beta :  1.0621543608715702\n",
      "loss_beta :  tensor(-6.2440e-130, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.5171e-141, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.2465627495632003\n",
      "MSE beta :  1.0886866349673539\n",
      "loss_beta :  tensor(-9.0129e-87, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.1617e-93, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.236068779947637\n",
      "MSE beta :  1.10131328279826\n",
      "loss_beta :  tensor(-6.3823e-124, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2978e-135, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.23327043367504074\n",
      "MSE beta :  1.1126177687485914\n",
      "loss_beta :  tensor(-2.6040e-84, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.4813e-91, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.22271697111903332\n",
      "MSE beta :  1.1075038735945617\n",
      "loss_beta :  tensor(-1.8561e-88, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.0396e-96, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.2021861026968052\n",
      "MSE beta :  1.1217286850653498\n",
      "loss_beta :  tensor(-1.0522e-92, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.4775e-99, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.18215024424798237\n",
      "MSE beta :  1.137050039841263\n",
      "loss_beta :  tensor(-1.3905e-98, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.1854e-103, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.18050896614505973\n",
      "MSE beta :  1.1438431054498646\n",
      "loss_beta :  tensor(-7.6950e-99, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.0937e-108, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.18925116478346404\n",
      "MSE beta :  1.149473439462628\n",
      "loss_beta :  tensor(-5.2516e-174, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-6.9739e-180, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1781152934568351\n",
      "MSE beta :  1.1416474468630688\n",
      "loss_beta :  tensor(-7.4780e-73, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-5.3589e-77, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.17515941334080323\n",
      "MSE beta :  1.1485303312847162\n",
      "loss_beta :  tensor(-9.3291e-101, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.6681e-105, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.17191021377979987\n",
      "MSE beta :  1.1515985801862783\n",
      "loss_beta :  tensor(-4.0694e-100, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.5842e-104, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.17219711204665042\n",
      "MSE beta :  1.1450467299588734\n",
      "loss_beta :  tensor(-9.5130e-107, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.3503e-109, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.16065615595026678\n",
      "MSE beta :  1.1454703713796683\n",
      "loss_beta :  tensor(-4.0411e-92, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.1615e-95, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.14532147705515108\n",
      "MSE beta :  1.1419154708927048\n",
      "loss_beta :  tensor(-7.0140e-102, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-8.7380e-105, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.13691741182293185\n",
      "MSE beta :  1.1417811143524912\n",
      "loss_beta :  tensor(-4.2474e-98, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-9.8139e-100, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1330044785290549\n",
      "MSE beta :  1.1422883483252955\n",
      "loss_beta :  tensor(-5.8321e-134, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-8.5457e-138, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12773936579918907\n",
      "MSE beta :  1.1423901537150303\n",
      "loss_beta :  tensor(-1.4504e-125, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.0795e-132, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12218882533302017\n",
      "MSE beta :  1.142432411952432\n",
      "loss_beta :  tensor(-1.6089e-92, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.7880e-95, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12331370872828966\n",
      "MSE beta :  1.1401863856403887\n",
      "loss_beta :  tensor(-2.5855e-102, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2048e-104, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12846246358657903\n",
      "MSE beta :  1.1360673080626094\n",
      "loss_beta :  tensor(-2.6307e-76, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.2030e-77, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1340362053179024\n",
      "MSE beta :  1.1354007815536435\n",
      "loss_beta :  tensor(-6.5683e-62, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2786e-62, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1343406290574656\n",
      "MSE beta :  1.1344602079679291\n",
      "loss_beta :  tensor(-3.4019e-128, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.0515e-129, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.13106474118769157\n",
      "MSE beta :  1.1342789247051344\n",
      "loss_beta :  tensor(-3.3123e-87, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2703e-88, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.13074328992457096\n",
      "MSE beta :  1.1358808089206391\n",
      "loss_beta :  tensor(-4.6806e-93, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.9763e-93, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.13134049827423738\n",
      "MSE beta :  1.1378203128247015\n",
      "loss_beta :  tensor(-2.6318e-90, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-8.3371e-91, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.13204305826057375\n",
      "MSE beta :  1.13845777222194\n",
      "loss_beta :  tensor(-2.3395e-88, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-7.3339e-89, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.13086379433956208\n",
      "MSE beta :  1.1394955335242933\n",
      "loss_beta :  tensor(-3.4031e-150, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-7.9225e-151, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.13035700324951927\n",
      "MSE beta :  1.1413507426975706\n",
      "loss_beta :  tensor(-1.4368e-89, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.7432e-90, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12972315714210694\n",
      "MSE beta :  1.1440970970635949\n",
      "loss_beta :  tensor(-6.5653e-122, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2552e-122, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1285749204870757\n",
      "MSE beta :  1.1481433381895667\n",
      "loss_beta :  tensor(-2.6438e-116, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-7.4093e-117, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1278487177866108\n",
      "MSE beta :  1.154856628625675\n",
      "loss_beta :  tensor(-8.2996e-110, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.3437e-110, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12665195345693486\n",
      "MSE beta :  1.1545180316341006\n",
      "loss_beta :  tensor(-6.3277e-105, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.6247e-105, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1258836590713858\n",
      "MSE beta :  1.1587679535232085\n",
      "loss_beta :  tensor(-9.5165e-92, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.1006e-92, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12446289181728332\n",
      "MSE beta :  1.1640205773859662\n",
      "loss_beta :  tensor(-1.3720e-110, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-9.9207e-111, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12307978161059654\n",
      "MSE beta :  1.1636078559008889\n",
      "loss_beta :  tensor(-2.0153e-98, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.1588e-98, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12252161096246642\n",
      "MSE beta :  1.1662451013489317\n",
      "loss_beta :  tensor(-1.5760e-120, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-9.3159e-121, grad_fn=<NegBackward>)\n",
      "----------------------------------------------------------------------compteur_beta :  0  out_of : 40\n",
      "----------------------------------------------------------------------compteur_C :  0  out_of : 40\n",
      "MSE Sigma  0.12153633187160424\n",
      "MSE beta :  1.166084683898397\n",
      "loss_beta :  tensor(-7.3842e-102, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.3138e-102, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.12041016008871282\n",
      "MSE beta :  1.1674586474726403\n",
      "loss_beta :  tensor(-3.7818e-82, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.1475e-82, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11919882699756179\n",
      "MSE beta :  1.1674319782774742\n",
      "loss_beta :  tensor(-1.4556e-122, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.7203e-124, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11813113858073684\n",
      "MSE beta :  1.1682400544815175\n",
      "loss_beta :  tensor(-5.8528e-104, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.5400e-104, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11692081061767094\n",
      "MSE beta :  1.1683503414211405\n",
      "loss_beta :  tensor(-2.7623e-88, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.7910e-88, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11676597100707156\n",
      "MSE beta :  1.1680275152759902\n",
      "loss_beta :  tensor(-5.5796e-105, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.4769e-105, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1170010236631404\n",
      "MSE beta :  1.168193516437799\n",
      "loss_beta :  tensor(-5.6265e-85, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.6390e-85, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11691891374734287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE beta :  1.1681341667570408\n",
      "loss_beta :  tensor(-1.3886e-90, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.0579e-90, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11706429573324438\n",
      "MSE beta :  1.1681872090623522\n",
      "loss_beta :  tensor(-3.5070e-79, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.9125e-79, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1173388581688226\n",
      "MSE beta :  1.1682009096652384\n",
      "loss_beta :  tensor(-3.0826e-101, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.6230e-101, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11750689764218239\n",
      "MSE beta :  1.168322567720899\n",
      "loss_beta :  tensor(-3.8718e-118, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.1250e-118, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11770093200256286\n",
      "MSE beta :  1.1685237245428648\n",
      "loss_beta :  tensor(-4.6395e-150, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.6075e-150, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11810135765784142\n",
      "MSE beta :  1.168474255794401\n",
      "loss_beta :  tensor(-6.3599e-75, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-6.0130e-75, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11800943596127439\n",
      "MSE beta :  1.1685651995537956\n",
      "loss_beta :  tensor(-6.8313e-94, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-6.4175e-94, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11772403787596573\n",
      "MSE beta :  1.1685958813815538\n",
      "loss_beta :  tensor(-1.8037e-114, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.2520e-114, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11730176186269808\n",
      "MSE beta :  1.1685452902296114\n",
      "loss_beta :  tensor(-4.2910e-102, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.6298e-102, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11691552714615133\n",
      "MSE beta :  1.1685110579082232\n",
      "loss_beta :  tensor(-3.4339e-89, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.2459e-89, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11690230815114613\n",
      "MSE beta :  1.168445340887332\n",
      "loss_beta :  tensor(-2.2454e-112, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.8778e-112, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11669318397958311\n",
      "MSE beta :  1.1683744614711131\n",
      "loss_beta :  tensor(-1.0454e-97, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-9.6696e-98, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11644154368155478\n",
      "MSE beta :  1.1683028068983536\n",
      "loss_beta :  tensor(-1.3909e-129, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.3296e-129, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11636646029324182\n",
      "MSE beta :  1.168305611484089\n",
      "loss_beta :  tensor(-6.0943e-141, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-5.5597e-141, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1163932347746145\n",
      "MSE beta :  1.1682876331728314\n",
      "loss_beta :  tensor(-3.4700e-95, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.3446e-95, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11635230431832844\n",
      "MSE beta :  1.1682315022959533\n",
      "loss_beta :  tensor(-5.1969e-101, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.9952e-101, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11624677131829798\n",
      "MSE beta :  1.1681715583363614\n",
      "loss_beta :  tensor(-7.5461e-78, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-7.3457e-78, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11612378275607288\n",
      "MSE beta :  1.1681688456090487\n",
      "loss_beta :  tensor(-5.2531e-61, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-5.2035e-61, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11606046051283472\n",
      "MSE beta :  1.168202349121918\n",
      "loss_beta :  tensor(-3.7203e-133, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.5757e-133, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11601364468337758\n",
      "MSE beta :  1.1682449362757525\n",
      "loss_beta :  tensor(-2.5142e-92, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2774e-92, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11595584318094915\n",
      "MSE beta :  1.168291228661213\n",
      "loss_beta :  tensor(-1.1267e-92, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.0824e-92, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11583992530895071\n",
      "MSE beta :  1.1683488316915047\n",
      "loss_beta :  tensor(-3.8032e-85, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.7147e-85, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11568544248962484\n",
      "MSE beta :  1.168358832106771\n",
      "loss_beta :  tensor(-1.6465e-87, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.6317e-87, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11565478936898764\n",
      "MSE beta :  1.1683391991824112\n",
      "loss_beta :  tensor(-2.5104e-138, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2529e-138, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11568549393496606\n",
      "MSE beta :  1.1683111155359844\n",
      "loss_beta :  tensor(-1.9787e-90, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.9022e-90, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.1157260633178276\n",
      "MSE beta :  1.1682830260100456\n",
      "loss_beta :  tensor(-1.8889e-119, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.8622e-119, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11577217722638956\n",
      "MSE beta :  1.168245224430848\n",
      "loss_beta :  tensor(-2.3994e-110, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.3081e-110, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11582867460122326\n",
      "MSE beta :  1.16820023424309\n",
      "loss_beta :  tensor(-2.4092e-105, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-2.2519e-105, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11590619646999707\n",
      "MSE beta :  1.1681951639185102\n",
      "loss_beta :  tensor(-1.9915e-105, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-1.8778e-105, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11599968856225543\n",
      "MSE beta :  1.1682267053508084\n",
      "loss_beta :  tensor(-4.3399e-91, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.1439e-91, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11611409517177895\n",
      "MSE beta :  1.1682304759568898\n",
      "loss_beta :  tensor(-3.0969e-106, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.0870e-106, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11611048120008477\n",
      "MSE beta :  1.1682495651707536\n",
      "loss_beta :  tensor(-4.6012e-103, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-4.3744e-103, grad_fn=<NegBackward>)\n",
      "MSE Sigma  0.11596603148375689\n",
      "MSE beta :  1.1682734856774957\n",
      "loss_beta :  tensor(-3.3339e-112, grad_fn=<NegBackward>)\n",
      "loss_Sigma;  tensor(-3.2053e-112, grad_fn=<NegBackward>)\n",
      "----------------------------------------------------------------------compteur_beta :  0  out_of : 40\n",
      "----------------------------------------------------------------------compteur_C :  0  out_of : 40\n",
      "CPU times: user 30.1 s, sys: 612 ms, total: 30.7 s\n",
      "Wall time: 5.15 s\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(int(time.time()))\n",
    "batch_size = 5\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "%time model.fit(2, 0.0001,0.1,0.1,C_optim = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 454,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 454,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(model.get_Sigma().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(true_Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.5919,  1.4653,  1.3975,  ...,  0.0523,  0.1513,  0.0845],\n",
       "        [ 1.4653,  1.3724,  1.3188,  ...,  0.0073,  0.1212,  0.0513],\n",
       "        [ 1.3975,  1.3188,  1.2856,  ..., -0.0389,  0.0880,  0.0095],\n",
       "        ...,\n",
       "        [ 0.0523,  0.0073, -0.0389,  ...,  0.8697,  0.6853,  0.7058],\n",
       "        [ 0.1513,  0.1212,  0.0880,  ...,  0.6853,  0.5577,  0.5608],\n",
       "        [ 0.0845,  0.0513,  0.0095,  ...,  0.7058,  0.5608,  0.5871]],\n",
       "       grad_fn=<MmBackward>)"
      ]
     },
     "execution_count": 414,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.get_Sigma()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.1059, 2.0006, 1.9006,  ..., 0.0045, 0.0043, 0.0040],\n",
       "        [2.1034, 2.0031, 1.9030,  ..., 0.0047, 0.0045, 0.0043],\n",
       "        [2.1011, 2.0006, 1.9055,  ..., 0.0050, 0.0047, 0.0045],\n",
       "        ...,\n",
       "        [0.0045, 0.0047, 0.0050,  ..., 1.6170, 1.5361, 1.4593],\n",
       "        [0.0043, 0.0045, 0.0047,  ..., 1.6145, 1.5386, 1.4617],\n",
       "        [0.0040, 0.0043, 0.0045,  ..., 1.6121, 1.5361, 1.4642]])"
      ]
     },
     "execution_count": 413,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_Sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0224, -0.0648,  0.2322, -0.0565,  0.2007,  0.1496, -0.1741,  0.0774,\n",
       "          0.1026,  0.1720, -0.1018,  0.0733,  0.0255, -0.0008,  0.1648,  0.0512,\n",
       "          0.0007,  0.0959, -0.0435,  0.2044,  0.1561,  0.0928, -0.0796,  0.0966,\n",
       "         -0.1068, -0.0393,  0.0171, -0.1143, -0.0653,  0.1510, -0.0907, -0.0745,\n",
       "          0.0812,  0.1543,  0.3191, -0.0329,  0.0117, -0.2476,  0.0617,  0.0626,\n",
       "         -0.1248, -0.1566, -0.0016, -0.0591, -0.0326, -0.1341,  0.1301,  0.0491,\n",
       "          0.1655,  0.2150],\n",
       "        [ 0.2058,  0.0979,  0.1655,  0.4085, -0.0024,  0.3002,  0.0918,  0.0158,\n",
       "          0.0314, -0.0790,  0.1559,  0.1129,  0.0203, -0.0432, -0.1270,  0.2124,\n",
       "          0.1088, -0.1736,  0.0283,  0.1422,  0.0921,  0.0490,  0.0322,  0.0548,\n",
       "         -0.0036, -0.0447,  0.0938,  0.2520, -0.3129,  0.0265,  0.0545,  0.0692,\n",
       "          0.1205,  0.1318,  0.0331, -0.0601, -0.0641,  0.1256,  0.0591,  0.0618,\n",
       "         -0.0393, -0.0963, -0.0185, -0.2947, -0.0857, -0.1736,  0.1096,  0.0468,\n",
       "          0.0967,  0.0740],\n",
       "        [-0.1643,  0.1895,  0.1075,  0.1499, -0.0268,  0.0794, -0.0447,  0.0468,\n",
       "         -0.0296, -0.0016,  0.1057, -0.0204,  0.0334, -0.0755, -0.1655, -0.0079,\n",
       "          0.1765, -0.2600,  0.0881,  0.0845,  0.0349, -0.0358, -0.0006, -0.0129,\n",
       "          0.0344, -0.0648, -0.0467, -0.1843, -0.0159, -0.1786, -0.2958, -0.1069,\n",
       "         -0.1917, -0.3084,  0.0514,  0.0630, -0.0935, -0.3950,  0.2895, -0.0846,\n",
       "          0.3093,  0.0627,  0.0125,  0.0026,  0.1690, -0.1522,  0.1193,  0.2393,\n",
       "         -0.0550,  0.1898],\n",
       "        [-0.3973, -0.1918,  0.0360, -0.2774, -0.0924, -0.1921, -0.2480, -0.1086,\n",
       "         -0.0637,  0.2139,  0.1420,  0.1178, -0.0180,  0.0885,  0.1512,  0.0404,\n",
       "         -0.0730, -0.2199, -0.0004, -0.0226,  0.1495, -0.0045,  0.0742, -0.1759,\n",
       "          0.0990, -0.1614,  0.1403, -0.0882, -0.1584,  0.1739, -0.1682,  0.0459,\n",
       "          0.3221,  0.0408,  0.0272,  0.1582,  0.1589,  0.3181,  0.2252,  0.1125,\n",
       "         -0.1259,  0.1971, -0.0785,  0.0463,  0.1431,  0.1177,  0.1765,  0.0016,\n",
       "         -0.2095, -0.0334]], requires_grad=True)"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0237, -0.1394,  0.2144, -0.0596,  0.1708,  0.1546, -0.0580,  0.0305,\n",
       "          0.0673,  0.1377, -0.1620,  0.0359, -0.0199,  0.0257,  0.0807,  0.0286,\n",
       "         -0.1005,  0.0185, -0.0578,  0.1322,  0.2489,  0.0642, -0.0408,  0.0875,\n",
       "         -0.0384,  0.0763, -0.0292, -0.1030, -0.0884,  0.1484, -0.0787, -0.0588,\n",
       "          0.0731,  0.1313,  0.1439, -0.0207, -0.0474, -0.1664,  0.0339,  0.0170,\n",
       "         -0.0660, -0.1478,  0.0863, -0.0490, -0.0255, -0.1836,  0.0216,  0.0421,\n",
       "          0.0359, -0.0672],\n",
       "        [-0.0819,  0.0110, -0.0940,  0.1822, -0.1549,  0.1019, -0.0537, -0.0547,\n",
       "         -0.1119, -0.1093,  0.1268, -0.0076, -0.0549, -0.1772, -0.2342,  0.1171,\n",
       "          0.0027, -0.1536,  0.0128,  0.0200, -0.0291,  0.0901, -0.0007,  0.0192,\n",
       "         -0.0225, -0.0580,  0.0505,  0.0117, -0.2717,  0.1360, -0.0084,  0.0495,\n",
       "          0.0846,  0.0283, -0.0207, -0.0486,  0.0099,  0.1378,  0.0999, -0.0008,\n",
       "         -0.0540,  0.0271,  0.0178, -0.2330, -0.1096,  0.0083,  0.0981,  0.0635,\n",
       "         -0.1138, -0.0390],\n",
       "        [-0.0388,  0.0866,  0.0474,  0.0715, -0.0533, -0.0555, -0.0303, -0.0077,\n",
       "          0.0257,  0.0709,  0.0749,  0.0348,  0.0360, -0.0257, -0.0455, -0.0503,\n",
       "          0.1042, -0.1651,  0.0837,  0.0876, -0.0041,  0.0004, -0.0077,  0.0608,\n",
       "          0.0337, -0.0522, -0.0537, -0.1562,  0.0660, -0.2129, -0.2954, -0.0716,\n",
       "         -0.1000, -0.1174,  0.0222,  0.1371,  0.0081, -0.0133, -0.0939, -0.0604,\n",
       "          0.0655,  0.0977,  0.0883,  0.0057,  0.0392, -0.1034,  0.1687,  0.1110,\n",
       "          0.0475,  0.1615],\n",
       "        [-0.1202, -0.0800, -0.0150, -0.0907, -0.0463, -0.0790, -0.1099, -0.0750,\n",
       "         -0.0365,  0.2741,  0.0837,  0.0115,  0.0138,  0.0866,  0.1802,  0.0143,\n",
       "         -0.0364, -0.1764,  0.0348,  0.0553,  0.0758,  0.0069,  0.1152, -0.0248,\n",
       "          0.0338, -0.0028,  0.0204, -0.0590, -0.1130,  0.0377, -0.1398,  0.0485,\n",
       "          0.2168,  0.1098, -0.0270,  0.1455,  0.1064, -0.0597,  0.1480,  0.0435,\n",
       "         -0.1238,  0.1766, -0.0817, -0.0188,  0.0055, -0.0778,  0.0682, -0.0497,\n",
       "         -0.0975,  0.1112]])"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Y_b :  tensor([[  0,   0,  11,   0,   0],\n",
      "        [  2,   0,   1,   0,   0],\n",
      "        [  1,   1,   2,   0,   0],\n",
      "        [  2,   0,   2,   3,   1],\n",
      "        [  8,   0,   2, 136,  73]])\n",
      "loss :  tensor(5.9508e-281, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-2.8157e-280, -1.1190e-280, -1.9145e-280],\n",
      "        [-6.2642e-281, -1.7611e-281,  1.7572e-280],\n",
      "        [-1.7002e-280,  5.4231e-280,  8.3324e-280],\n",
      "        [-7.8557e-279,  3.0020e-280, -4.6934e-279],\n",
      "        [-3.8433e-279,  8.9527e-281, -2.2005e-279]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-2.8157e-280, -1.1190e-280, -1.9145e-280],\n",
      "        [-6.2642e-281, -1.7611e-281,  1.7572e-280],\n",
      "        [-1.7002e-280,  5.4231e-280,  8.3324e-280],\n",
      "        [-7.8557e-279,  3.0020e-280, -4.6934e-279],\n",
      "        [-3.8433e-279,  8.9527e-281, -2.2005e-279]])\n",
      "Y_b :  tensor([[33,  1,  1,  5,  4],\n",
      "        [ 5, 14,  0,  1,  1],\n",
      "        [ 9,  5,  1,  0,  0],\n",
      "        [ 0,  1,  3,  3,  6],\n",
      "        [ 0,  1,  0,  0,  1]])\n",
      "loss :  tensor(4.5986e-67, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(2.4406e-81, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.9501e-66, -4.6154e-66,  5.6257e-67],\n",
      "        [-4.8946e-66, -2.5913e-66,  7.1527e-66],\n",
      "        [-5.2409e-67,  3.7952e-67,  1.2541e-66],\n",
      "        [ 5.3754e-67, -3.4412e-67, -3.2448e-67],\n",
      "        [ 2.2441e-66,  1.0497e-66, -1.0988e-66]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.9501e-66, -4.6154e-66,  5.6257e-67],\n",
      "        [-4.8946e-66, -2.5913e-66,  7.1527e-66],\n",
      "        [-5.2409e-67,  3.7952e-67,  1.2541e-66],\n",
      "        [ 5.3754e-67, -3.4412e-67, -3.2448e-67],\n",
      "        [ 2.2441e-66,  1.0497e-66, -1.0988e-66]])\n",
      "Y_b :  tensor([[ 12,   7,   6, 127, 185],\n",
      "        [  1,   1,   9,   1,   0],\n",
      "        [ 19,   2,   2,   3,   1],\n",
      "        [  1,   3,   0,  51,  27],\n",
      "        [  2,   0,   0,   9,  13]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Y_b :  tensor([[ 3,  1, 15,  8,  9],\n",
      "        [ 2,  3,  3,  0,  1],\n",
      "        [ 0,  5, 13,  0,  2],\n",
      "        [ 1,  3, 15,  4,  5],\n",
      "        [ 0,  6,  5,  6,  4]])\n",
      "loss :  tensor(2.8397e-66, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.3996e-80, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.9424e-65, -1.7330e-65, -1.5518e-65],\n",
      "        [ 1.2647e-65, -2.2781e-65,  1.7920e-65],\n",
      "        [ 1.2330e-65, -5.1014e-65,  2.1395e-65],\n",
      "        [-4.7657e-66, -4.1657e-65, -5.5849e-66],\n",
      "        [ 2.0402e-66, -3.5405e-65, -2.0495e-66]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.9424e-65, -1.7330e-65, -1.5518e-65],\n",
      "        [ 1.2647e-65, -2.2781e-65,  1.7920e-65],\n",
      "        [ 1.2330e-65, -5.1014e-65,  2.1395e-65],\n",
      "        [-4.7657e-66, -4.1657e-65, -5.5849e-66],\n",
      "        [ 2.0402e-66, -3.5405e-65, -2.0495e-66]])\n",
      "Y_b :  tensor([[ 3,  2,  1,  0,  2],\n",
      "        [ 0,  1,  2,  1,  0],\n",
      "        [ 0,  0,  0, 20, 12],\n",
      "        [ 0,  0,  0,  0,  0],\n",
      "        [ 4,  3,  0,  0,  0]])\n",
      "loss :  tensor(6.6744e-54, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(4.1357e-68, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-3.5710e-53, -4.5134e-54, -5.6630e-54],\n",
      "        [-1.1329e-53,  2.9015e-53, -2.0418e-53],\n",
      "        [-3.3460e-53,  1.7211e-53,  1.2865e-53],\n",
      "        [ 2.2109e-52, -6.9756e-53, -5.1116e-53],\n",
      "        [ 1.7433e-52, -2.4932e-53, -4.5245e-53]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-3.5710e-53, -4.5134e-54, -5.6630e-54],\n",
      "        [-1.1329e-53,  2.9015e-53, -2.0418e-53],\n",
      "        [-3.3460e-53,  1.7211e-53,  1.2865e-53],\n",
      "        [ 2.2109e-52, -6.9756e-53, -5.1116e-53],\n",
      "        [ 1.7433e-52, -2.4932e-53, -4.5245e-53]])\n",
      "Y_b :  tensor([[5, 6, 0, 0, 0],\n",
      "        [0, 2, 1, 3, 7],\n",
      "        [2, 0, 0, 2, 0],\n",
      "        [1, 0, 0, 0, 2],\n",
      "        [0, 0, 0, 1, 1]])\n",
      "loss :  tensor(1.0097e-37, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(4.9026e-52, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 1.3160e-36, -1.3251e-36,  9.2782e-37],\n",
      "        [ 6.4583e-37, -4.2967e-37,  8.1393e-37],\n",
      "        [ 2.1228e-36,  1.1191e-36,  1.7312e-36],\n",
      "        [ 1.7948e-37,  9.0034e-38,  2.4238e-38],\n",
      "        [ 1.0484e-36, -2.1118e-37,  2.6815e-37]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 1.3160e-36, -1.3251e-36,  9.2782e-37],\n",
      "        [ 6.4583e-37, -4.2967e-37,  8.1393e-37],\n",
      "        [ 2.1228e-36,  1.1191e-36,  1.7312e-36],\n",
      "        [ 1.7948e-37,  9.0034e-38,  2.4238e-38],\n",
      "        [ 1.0484e-36, -2.1118e-37,  2.6815e-37]])\n",
      "Y_b :  tensor([[ 26,   5,   2,   1,   0],\n",
      "        [  0,   0,   4,   0,   1],\n",
      "        [  3,   2,   0, 114,  77],\n",
      "        [  0,   8,  28,   1,   0],\n",
      "        [  0,   2,   0,  31,  16]])\n",
      "loss :  tensor(1.7094e-317, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-5.0639e-316,  2.3007e-316,  1.0722e-316],\n",
      "        [-1.7524e-316,  8.5702e-317,  9.9564e-317],\n",
      "        [-6.5835e-316,  2.3516e-316,  4.4085e-316],\n",
      "        [-4.1839e-315, -1.5746e-315,  1.4499e-315],\n",
      "        [-1.7664e-315, -8.8200e-316,  6.3625e-316]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-5.0639e-316,  2.3007e-316,  1.0722e-316],\n",
      "        [-1.7524e-316,  8.5702e-317,  9.9564e-317],\n",
      "        [-6.5835e-316,  2.3516e-316,  4.4085e-316],\n",
      "        [-4.1839e-315, -1.5746e-315,  1.4499e-315],\n",
      "        [-1.7664e-315, -8.8200e-316,  6.3625e-316]])\n",
      "Y_b :  tensor([[  2,   6,   1,   3,   0],\n",
      "        [ 27,   1,   9,  29,  36],\n",
      "        [  8,   2,   1, 182, 101],\n",
      "        [  0,   1,   0,  13,   2],\n",
      "        [  1,   5,   1,   5,   7]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Y_b :  tensor([[ 1,  5, 12,  1,  0],\n",
      "        [34, 24, 18,  0,  0],\n",
      "        [17,  2,  3,  2,  0],\n",
      "        [57,  1,  0,  0,  0],\n",
      "        [ 3,  0,  3,  0,  0]])\n",
      "loss :  tensor(1.3723e-105, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.9138e-119, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 9.8209e-105,  9.1465e-104, -5.8706e-105],\n",
      "        [-2.8302e-104,  1.0277e-104,  1.1084e-106],\n",
      "        [-3.6848e-104, -1.5406e-104,  1.3027e-104],\n",
      "        [-3.7725e-107, -1.9046e-105, -1.5254e-105],\n",
      "        [ 1.2487e-104,  5.3244e-105, -4.2288e-105]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 9.8209e-105,  9.1465e-104, -5.8706e-105],\n",
      "        [-2.8302e-104,  1.0277e-104,  1.1084e-106],\n",
      "        [-3.6848e-104, -1.5406e-104,  1.3027e-104],\n",
      "        [-3.7725e-107, -1.9046e-105, -1.5254e-105],\n",
      "        [ 1.2487e-104,  5.3244e-105, -4.2288e-105]])\n",
      "Y_b :  tensor([[  1,   3,   0,   0,   0],\n",
      "        [ 27,   1,   0,   0,   0],\n",
      "        [ 11,   2,   2,  49,  50],\n",
      "        [104,   1,   8,   0,   1],\n",
      "        [  0,   0,   0,   0,   0]])\n",
      "loss :  tensor(1.0044e-261, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.1064e-260,  1.1935e-259,  1.5609e-259],\n",
      "        [ 5.1629e-261, -8.2247e-262,  1.0679e-261],\n",
      "        [ 1.6110e-259,  9.0014e-260,  1.3224e-259],\n",
      "        [ 8.4294e-261,  3.1154e-260, -3.5155e-260],\n",
      "        [ 6.9346e-260,  4.6288e-260, -1.2220e-260]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.1064e-260,  1.1935e-259,  1.5609e-259],\n",
      "        [ 5.1629e-261, -8.2247e-262,  1.0679e-261],\n",
      "        [ 1.6110e-259,  9.0014e-260,  1.3224e-259],\n",
      "        [ 8.4294e-261,  3.1154e-260, -3.5155e-260],\n",
      "        [ 6.9346e-260,  4.6288e-260, -1.2220e-260]])\n",
      "Y_b :  tensor([[ 0,  5, 60,  0,  0],\n",
      "        [ 0,  0,  0, 10, 24],\n",
      "        [ 0,  0,  2,  0,  1],\n",
      "        [ 1,  1,  8,  3,  0],\n",
      "        [14,  0,  0,  2,  5]])\n",
      "loss :  tensor(8.5170e-105, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.0295e-118, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-8.1218e-104,  2.3657e-105, -6.3230e-104],\n",
      "        [ 1.5289e-104, -3.5325e-104, -6.0804e-105],\n",
      "        [-4.8970e-103, -7.4795e-104,  3.4449e-103],\n",
      "        [ 1.3961e-103, -4.1513e-104, -1.0394e-103],\n",
      "        [ 3.5568e-103, -1.5075e-103, -3.0926e-103]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-8.1218e-104,  2.3657e-105, -6.3230e-104],\n",
      "        [ 1.5289e-104, -3.5325e-104, -6.0804e-105],\n",
      "        [-4.8970e-103, -7.4795e-104,  3.4449e-103],\n",
      "        [ 1.3961e-103, -4.1513e-104, -1.0394e-103],\n",
      "        [ 3.5568e-103, -1.5075e-103, -3.0926e-103]])\n",
      "Y_b :  tensor([[101,   1,   4,   2,   2],\n",
      "        [  0,   0,  14,   0,   1],\n",
      "        [  1,   0,   3,   0,   0],\n",
      "        [ 36,   0,   1,   1,   0],\n",
      "        [  1,   1,   0,   0,   0]])\n",
      "loss :  tensor(3.3659e-174, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 9.3236e-172,  1.6245e-172,  5.1538e-173],\n",
      "        [-5.0028e-175,  4.2235e-175, -9.3781e-174],\n",
      "        [-7.0308e-174, -5.2202e-173,  3.1771e-173],\n",
      "        [ 1.3930e-173,  9.6209e-174, -5.7890e-174],\n",
      "        [ 2.9539e-173, -1.1904e-173, -1.4197e-173]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 9.3236e-172,  1.6245e-172,  5.1538e-173],\n",
      "        [-5.0028e-175,  4.2235e-175, -9.3781e-174],\n",
      "        [-7.0308e-174, -5.2202e-173,  3.1771e-173],\n",
      "        [ 1.3930e-173,  9.6209e-174, -5.7890e-174],\n",
      "        [ 2.9539e-173, -1.1904e-173, -1.4197e-173]])\n",
      "Y_b :  tensor([[ 3,  8,  1,  0,  0],\n",
      "        [ 3, 10, 11,  4,  0],\n",
      "        [10,  4,  2,  3,  6],\n",
      "        [23,  1,  0,  1,  1],\n",
      "        [ 4,  3,  3,  0,  0]])\n",
      "loss :  tensor(4.8057e-67, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(6.1037e-81, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-8.9634e-66, -3.8156e-67,  1.7211e-65],\n",
      "        [-1.8086e-66, -1.9527e-66, -4.2559e-68],\n",
      "        [ 1.3225e-65, -1.2683e-65,  1.5881e-65],\n",
      "        [ 2.8984e-66,  1.2261e-66, -7.7720e-67],\n",
      "        [ 6.6185e-66,  3.6461e-66, -2.4597e-66]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-8.9634e-66, -3.8156e-67,  1.7211e-65],\n",
      "        [-1.8086e-66, -1.9527e-66, -4.2559e-68],\n",
      "        [ 1.3225e-65, -1.2683e-65,  1.5881e-65],\n",
      "        [ 2.8984e-66,  1.2261e-66, -7.7720e-67],\n",
      "        [ 6.6185e-66,  3.6461e-66, -2.4597e-66]])\n",
      "Y_b :  tensor([[ 1,  8,  0,  1,  3],\n",
      "        [ 3,  0,  2,  0,  1],\n",
      "        [ 0,  0,  0,  2,  1],\n",
      "        [17,  1,  0, 47, 50],\n",
      "        [ 1,  3, 17,  0,  0]])\n",
      "loss :  tensor(8.8300e-127, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.3235e-140, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-3.4878e-126, -1.1549e-125, -1.2277e-125],\n",
      "        [-9.1351e-127,  6.8003e-126,  4.8573e-126],\n",
      "        [ 5.7745e-126,  5.5218e-126, -4.5392e-126],\n",
      "        [-9.0048e-126,  4.1959e-125, -1.2757e-125],\n",
      "        [-8.5068e-126,  4.4420e-125, -1.3815e-125]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-3.4878e-126, -1.1549e-125, -1.2277e-125],\n",
      "        [-9.1351e-127,  6.8003e-126,  4.8573e-126],\n",
      "        [ 5.7745e-126,  5.5218e-126, -4.5392e-126],\n",
      "        [-9.0048e-126,  4.1959e-125, -1.2757e-125],\n",
      "        [-8.5068e-126,  4.4420e-125, -1.3815e-125]])\n",
      "Y_b :  tensor([[  5,   5,   4,   2,   3],\n",
      "        [ 76,  30,   0,   5,   5],\n",
      "        [  0,   2,   0,   0,   0],\n",
      "        [  1,   0,   0,   1,   0],\n",
      "        [265,   0,   0,   0,   1]])\n",
      "loss :  tensor(8.7213e-290, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 1.7910e-288,  4.2795e-287,  2.7002e-287],\n",
      "        [ 4.7854e-288,  8.1152e-289,  6.9405e-288],\n",
      "        [ 2.5591e-289, -5.7400e-289,  8.2196e-289],\n",
      "        [ 5.8815e-289, -1.7773e-289,  9.9973e-289],\n",
      "        [ 1.3247e-288, -5.4728e-289,  1.4326e-288]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 1.7910e-288,  4.2795e-287,  2.7002e-287],\n",
      "        [ 4.7854e-288,  8.1152e-289,  6.9405e-288],\n",
      "        [ 2.5591e-289, -5.7400e-289,  8.2196e-289],\n",
      "        [ 5.8815e-289, -1.7773e-289,  9.9973e-289],\n",
      "        [ 1.3247e-288, -5.4728e-289,  1.4326e-288]])\n",
      "Y_b :  tensor([[ 2,  1,  0,  1,  9],\n",
      "        [ 2,  1, 33, 31,  8],\n",
      "        [ 0,  0,  2,  0,  1],\n",
      "        [11,  1,  0,  0,  0],\n",
      "        [ 0,  4,  2,  0,  0]])\n",
      "loss :  tensor(1.6930e-102, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.8303e-116, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.4380e-101, -7.6454e-102, -1.3608e-101],\n",
      "        [ 1.3178e-102, -4.3585e-102,  5.9778e-103],\n",
      "        [ 5.5153e-101,  4.0320e-101,  1.6246e-101],\n",
      "        [ 4.1352e-101,  3.6653e-101,  2.5371e-102],\n",
      "        [ 1.6390e-101, -4.5322e-102,  2.6111e-102]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.4380e-101, -7.6454e-102, -1.3608e-101],\n",
      "        [ 1.3178e-102, -4.3585e-102,  5.9778e-103],\n",
      "        [ 5.5153e-101,  4.0320e-101,  1.6246e-101],\n",
      "        [ 4.1352e-101,  3.6653e-101,  2.5371e-102],\n",
      "        [ 1.6390e-101, -4.5322e-102,  2.6111e-102]])\n",
      "Y_b :  tensor([[ 3,  7, 15, 81, 59],\n",
      "        [ 0,  0,  0,  5,  8],\n",
      "        [ 0,  1,  0,  7,  1],\n",
      "        [ 2,  3,  1,  0,  0],\n",
      "        [ 6,  9,  8, 24, 21]])\n",
      "loss :  tensor(1.6139e-256, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-2.4165e-256, -1.5908e-255, -2.6106e-257],\n",
      "        [-4.4708e-256, -1.0966e-255,  3.7437e-256],\n",
      "        [ 5.2412e-255,  9.2340e-255,  1.4213e-254],\n",
      "        [-7.9575e-255, -1.3967e-254, -1.4354e-254],\n",
      "        [-4.3285e-255, -1.0412e-254, -7.2631e-255]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-2.4165e-256, -1.5908e-255, -2.6106e-257],\n",
      "        [-4.4708e-256, -1.0966e-255,  3.7437e-256],\n",
      "        [ 5.2412e-255,  9.2340e-255,  1.4213e-254],\n",
      "        [-7.9575e-255, -1.3967e-254, -1.4354e-254],\n",
      "        [-4.3285e-255, -1.0412e-254, -7.2631e-255]])\n",
      "Y_b :  tensor([[ 1,  2,  0,  1,  1],\n",
      "        [ 0,  0,  2,  4,  4],\n",
      "        [ 0,  0,  0,  0,  0],\n",
      "        [ 0,  0,  0,  1,  0],\n",
      "        [ 0,  0, 17,  2,  2]])\n",
      "loss :  tensor(8.3889e-36, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.5026e-49, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 1.1429e-34, -3.0548e-34, -9.1080e-36],\n",
      "        [ 4.1120e-35, -5.6405e-35,  3.5091e-35],\n",
      "        [ 5.6639e-35,  5.9050e-36,  1.9666e-34],\n",
      "        [ 3.8088e-36,  9.3574e-37, -1.4285e-35],\n",
      "        [ 6.1687e-35, -3.5450e-35,  1.0893e-35]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 1.1429e-34, -3.0548e-34, -9.1080e-36],\n",
      "        [ 4.1120e-35, -5.6405e-35,  3.5091e-35],\n",
      "        [ 5.6639e-35,  5.9050e-36,  1.9666e-34],\n",
      "        [ 3.8088e-36,  9.3574e-37, -1.4285e-35],\n",
      "        [ 6.1687e-35, -3.5450e-35,  1.0893e-35]])\n",
      "Y_b :  tensor([[ 0,  0,  0,  3,  2],\n",
      "        [25,  0,  1,  0,  0],\n",
      "        [ 3,  7,  0,  0,  2],\n",
      "        [ 3,  0,  1,  0,  0],\n",
      "        [ 3,  5,  3,  1,  3]])\n",
      "loss :  tensor(2.4566e-39, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.8095e-53, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 5.2791e-38, -3.6894e-38, -1.5638e-38],\n",
      "        [-1.3841e-38, -8.8562e-39,  3.4381e-38],\n",
      "        [ 2.6497e-38,  4.9024e-39,  5.0742e-38],\n",
      "        [-2.5672e-39, -6.3911e-39, -1.2086e-38],\n",
      "        [ 7.0699e-39, -1.6452e-38, -7.6230e-39]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 5.2791e-38, -3.6894e-38, -1.5638e-38],\n",
      "        [-1.3841e-38, -8.8562e-39,  3.4381e-38],\n",
      "        [ 2.6497e-38,  4.9024e-39,  5.0742e-38],\n",
      "        [-2.5672e-39, -6.3911e-39, -1.2086e-38],\n",
      "        [ 7.0699e-39, -1.6452e-38, -7.6230e-39]])\n",
      "Y_b :  tensor([[  0,   0,   1,   5,  23],\n",
      "        [  4,   2,   0, 119, 168],\n",
      "        [  0,   1,   2,   3,   1],\n",
      "        [  4,   0,   6,   2,   0],\n",
      "        [  0,   0,   0,   2,   6]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Y_b :  tensor([[  1,   4,   4,   1,   2],\n",
      "        [137,   6,  77,   0,   1],\n",
      "        [  1,   4,   5,   0,   0],\n",
      "        [ 20,   6,  19,   1,   8],\n",
      "        [  1,   2,   1,   4,   0]])\n",
      "loss :  tensor(8.1013e-218, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 4.6961e-216,  6.5042e-216, -2.0103e-215],\n",
      "        [-3.4470e-217,  2.0340e-217, -5.1340e-217],\n",
      "        [-5.3407e-216, -1.1284e-215,  2.5227e-215],\n",
      "        [-8.8315e-218,  8.6246e-218,  2.2884e-217],\n",
      "        [ 3.6820e-217, -7.4024e-217,  7.3306e-217]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 4.6961e-216,  6.5042e-216, -2.0103e-215],\n",
      "        [-3.4470e-217,  2.0340e-217, -5.1340e-217],\n",
      "        [-5.3407e-216, -1.1284e-215,  2.5227e-215],\n",
      "        [-8.8315e-218,  8.6246e-218,  2.2884e-217],\n",
      "        [ 3.6820e-217, -7.4024e-217,  7.3306e-217]])\n",
      "Y_b :  tensor([[ 0,  1,  0,  2,  8],\n",
      "        [ 3,  8,  0,  0,  0],\n",
      "        [25,  4,  1,  2,  2],\n",
      "        [ 2,  0,  4,  2,  0],\n",
      "        [15,  0,  0,  0,  1]])\n",
      "loss :  tensor(8.1874e-45, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(3.7555e-59, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 7.9803e-44, -3.9944e-43, -3.1535e-44],\n",
      "        [-4.3951e-44,  4.3498e-44,  8.3814e-44],\n",
      "        [-3.5165e-44, -4.1487e-44,  1.0851e-43],\n",
      "        [ 1.8095e-45, -4.1306e-45, -7.0135e-45],\n",
      "        [ 1.4148e-44,  4.9897e-44, -2.0679e-44]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 7.9803e-44, -3.9944e-43, -3.1535e-44],\n",
      "        [-4.3951e-44,  4.3498e-44,  8.3814e-44],\n",
      "        [-3.5165e-44, -4.1487e-44,  1.0851e-43],\n",
      "        [ 1.8095e-45, -4.1306e-45, -7.0135e-45],\n",
      "        [ 1.4148e-44,  4.9897e-44, -2.0679e-44]])\n",
      "Y_b :  tensor([[ 0,  0,  0,  1,  0],\n",
      "        [ 0,  3,  1,  1,  0],\n",
      "        [29,  5, 15,  3,  2],\n",
      "        [11,  1, 19, 29, 32],\n",
      "        [34,  3,  1,  1,  2]])\n",
      "loss :  tensor(5.0997e-187, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 3.1241e-185, -1.7296e-185, -1.9051e-185],\n",
      "        [ 4.5181e-186, -1.7597e-186, -2.0525e-186],\n",
      "        [-5.3475e-185,  9.7741e-186,  5.4568e-185],\n",
      "        [-1.3308e-186, -1.2070e-185,  1.4915e-185],\n",
      "        [-5.3023e-187, -1.2661e-185,  1.5817e-185]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 3.1241e-185, -1.7296e-185, -1.9051e-185],\n",
      "        [ 4.5181e-186, -1.7597e-186, -2.0525e-186],\n",
      "        [-5.3475e-185,  9.7741e-186,  5.4568e-185],\n",
      "        [-1.3308e-186, -1.2070e-185,  1.4915e-185],\n",
      "        [-5.3023e-187, -1.2661e-185,  1.5817e-185]])\n",
      "Y_b :  tensor([[  0,   2,   4,   0,   0],\n",
      "        [  0,   1,   7, 170, 162],\n",
      "        [ 12,   8,   3,   2,   2],\n",
      "        [ 12,   2,   2,   0,   0],\n",
      "        [  0,   4,   1,   8,   7]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Y_b :  tensor([[ 0,  0,  0,  0,  0],\n",
      "        [ 6,  1,  2,  2,  0],\n",
      "        [ 3,  1,  0,  0,  5],\n",
      "        [20,  6, 13,  0,  3],\n",
      "        [42,  0,  1,  4,  3]])\n",
      "loss :  tensor(5.4698e-69, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.3492e-81, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-2.7642e-67, -5.6405e-67, -3.4096e-67],\n",
      "        [-3.0469e-68,  1.0653e-68, -5.4209e-69],\n",
      "        [-6.7057e-68,  4.4619e-68,  9.0311e-68],\n",
      "        [ 5.7927e-69,  3.7463e-68,  9.1592e-69],\n",
      "        [ 7.0485e-68,  4.3498e-68,  8.5285e-69]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-2.7642e-67, -5.6405e-67, -3.4096e-67],\n",
      "        [-3.0469e-68,  1.0653e-68, -5.4209e-69],\n",
      "        [-6.7057e-68,  4.4619e-68,  9.0311e-68],\n",
      "        [ 5.7927e-69,  3.7463e-68,  9.1592e-69],\n",
      "        [ 7.0485e-68,  4.3498e-68,  8.5285e-69]])\n",
      "Y_b :  tensor([[ 2,  3,  0, 30, 16],\n",
      "        [ 3,  0,  0, 11, 29],\n",
      "        [ 0,  5,  3,  1,  1],\n",
      "        [ 6,  3,  3,  1,  1],\n",
      "        [ 2,  1,  5,  4,  2]])\n",
      "loss :  tensor(9.4264e-91, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.5873e-104, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.3111e-89, -4.1459e-90, -1.1586e-90],\n",
      "        [ 2.4757e-90,  2.6896e-90, -2.8785e-90],\n",
      "        [-2.5160e-90, -6.3094e-90,  6.3881e-89],\n",
      "        [-3.9863e-89,  9.2841e-94, -1.1427e-89],\n",
      "        [-2.9137e-89, -1.0697e-89, -2.1487e-89]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.3111e-89, -4.1459e-90, -1.1586e-90],\n",
      "        [ 2.4757e-90,  2.6896e-90, -2.8785e-90],\n",
      "        [-2.5160e-90, -6.3094e-90,  6.3881e-89],\n",
      "        [-3.9863e-89,  9.2841e-94, -1.1427e-89],\n",
      "        [-2.9137e-89, -1.0697e-89, -2.1487e-89]])\n",
      "Y_b :  tensor([[ 23,   1,   2,   0,   0],\n",
      "        [  0,   1,   4,   0,   0],\n",
      "        [  9,   2,   0,   0,   0],\n",
      "        [  1,   0,   7,   0,   0],\n",
      "        [107,   4,   3,   7,   8]])\n",
      "loss :  tensor(1.3294e-191, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.0035e-189,  2.7448e-190,  1.1974e-189],\n",
      "        [ 4.2351e-191,  2.6675e-191,  6.1345e-191],\n",
      "        [-1.7691e-191,  3.4007e-191,  8.4172e-191],\n",
      "        [-8.7663e-192, -3.5919e-192,  5.6059e-191],\n",
      "        [ 1.6495e-190,  1.5490e-191,  1.2139e-190]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.0035e-189,  2.7448e-190,  1.1974e-189],\n",
      "        [ 4.2351e-191,  2.6675e-191,  6.1345e-191],\n",
      "        [-1.7691e-191,  3.4007e-191,  8.4172e-191],\n",
      "        [-8.7663e-192, -3.5919e-192,  5.6059e-191],\n",
      "        [ 1.6495e-190,  1.5490e-191,  1.2139e-190]])\n",
      "Y_b :  tensor([[ 2,  8,  0,  0,  1],\n",
      "        [20,  1, 10, 11,  9],\n",
      "        [ 0, 12,  0,  0,  2],\n",
      "        [ 0,  2,  3,  1,  0],\n",
      "        [ 0,  6,  3,  0,  0]])\n",
      "loss :  tensor(9.4896e-61, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.2427e-74, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.5153e-59,  6.6157e-60,  7.4604e-60],\n",
      "        [ 1.7011e-59, -1.2765e-59, -1.0102e-59],\n",
      "        [-2.0649e-59,  4.1668e-59,  2.8699e-59],\n",
      "        [-1.0720e-59,  6.9098e-60,  6.0810e-60],\n",
      "        [ 3.6905e-60, -9.9981e-61,  2.3227e-60]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.5153e-59,  6.6157e-60,  7.4604e-60],\n",
      "        [ 1.7011e-59, -1.2765e-59, -1.0102e-59],\n",
      "        [-2.0649e-59,  4.1668e-59,  2.8699e-59],\n",
      "        [-1.0720e-59,  6.9098e-60,  6.0810e-60],\n",
      "        [ 3.6905e-60, -9.9981e-61,  2.3227e-60]])\n",
      "Y_b :  tensor([[ 7,  0, 35, 11, 25],\n",
      "        [ 4,  0,  1,  9, 10],\n",
      "        [ 0,  3,  0,  7, 24],\n",
      "        [ 0,  2, 18,  0,  0],\n",
      "        [ 1,  0,  0, 27, 14]])\n",
      "loss :  tensor(1.0733e-115, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.0380e-129, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 8.5654e-115, -2.3963e-114,  1.3825e-114],\n",
      "        [ 1.7922e-115, -4.6632e-116,  1.8777e-115],\n",
      "        [-9.9284e-115,  4.0091e-114, -2.8543e-114],\n",
      "        [-4.3312e-114, -2.4777e-116, -3.7940e-115],\n",
      "        [-2.7098e-115,  2.3625e-114, -2.6079e-114]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 8.5654e-115, -2.3963e-114,  1.3825e-114],\n",
      "        [ 1.7922e-115, -4.6632e-116,  1.8777e-115],\n",
      "        [-9.9284e-115,  4.0091e-114, -2.8543e-114],\n",
      "        [-4.3312e-114, -2.4777e-116, -3.7940e-115],\n",
      "        [-2.7098e-115,  2.3625e-114, -2.6079e-114]])\n",
      "Y_b :  tensor([[ 11,   2,   1, 126, 116],\n",
      "        [  6,   1,  14,   2,   1],\n",
      "        [ 10,   6,   3,   0,   0],\n",
      "        [ 38,  40,   4,  18,  30],\n",
      "        [  1,   1,   2,   2,   3]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Y_b :  tensor([[  0,   0,   8,   0,   0],\n",
      "        [  5,   2,   2,   3,   1],\n",
      "        [  2,   1,   1,  71, 369],\n",
      "        [  1,   1,   3,   1,   3],\n",
      "        [  2,   1,   1,   1,   0]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Y_b :  tensor([[ 1,  0,  3,  0,  0],\n",
      "        [ 0,  3,  6,  0,  0],\n",
      "        [51,  1,  0,  0,  0],\n",
      "        [ 4,  9,  1,  2,  5],\n",
      "        [ 1,  2,  0,  0,  2]])\n",
      "loss :  tensor(1.4129e-100, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(2.6687e-114, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[  1.1754e-98,  -2.2423e-99,  -4.9147e-99],\n",
      "        [ -1.5019e-99,   1.7146e-99,  8.4598e-100],\n",
      "        [  1.8046e-99,  2.6183e-100,   4.9470e-99],\n",
      "        [-2.2892e-100, -3.4693e-100,  1.7353e-100],\n",
      "        [  6.1910e-99,  -4.6682e-99,  5.4526e-100]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[  1.1754e-98,  -2.2423e-99,  -4.9147e-99],\n",
      "        [ -1.5019e-99,   1.7146e-99,  8.4598e-100],\n",
      "        [  1.8046e-99,  2.6183e-100,   4.9470e-99],\n",
      "        [-2.2892e-100, -3.4693e-100,  1.7353e-100],\n",
      "        [  6.1910e-99,  -4.6682e-99,  5.4526e-100]])\n",
      "Y_b :  tensor([[ 2,  1, 15,  1,  3],\n",
      "        [ 8,  0,  0,  1,  5],\n",
      "        [ 0,  0,  2,  1,  1],\n",
      "        [ 1,  2,  0,  1,  2],\n",
      "        [ 0,  0, 23,  0,  1]])\n",
      "loss :  tensor(1.8650e-64, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(7.0602e-79, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.6216e-63, -3.2925e-63,  1.2502e-63],\n",
      "        [ 2.2431e-64, -2.2317e-64, -3.1468e-64],\n",
      "        [ 1.1351e-63,  7.3848e-63,  1.4540e-63],\n",
      "        [ 4.3854e-64, -4.0267e-64, -6.9562e-64],\n",
      "        [ 1.2830e-63,  1.2239e-63, -1.0673e-63]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.6216e-63, -3.2925e-63,  1.2502e-63],\n",
      "        [ 2.2431e-64, -2.2317e-64, -3.1468e-64],\n",
      "        [ 1.1351e-63,  7.3848e-63,  1.4540e-63],\n",
      "        [ 4.3854e-64, -4.0267e-64, -6.9562e-64],\n",
      "        [ 1.2830e-63,  1.2239e-63, -1.0673e-63]])\n",
      "Y_b :  tensor([[42,  3,  5,  0,  0],\n",
      "        [12,  4,  2,  2,  0],\n",
      "        [ 1,  3,  0,  7,  4],\n",
      "        [ 0,  6, 19,  0,  0],\n",
      "        [40,  3,  1,  2,  1]])\n",
      "loss :  tensor(1.2287e-86, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(8.3002e-101, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 4.1164e-85, -1.8782e-86,  3.8720e-85],\n",
      "        [-5.2434e-86,  1.5323e-85,  4.7038e-86],\n",
      "        [-1.7613e-85,  3.7861e-85, -7.6492e-86],\n",
      "        [ 3.4137e-86, -5.7319e-86,  1.6738e-85],\n",
      "        [ 1.1585e-85, -1.7387e-85,  1.0887e-85]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 4.1164e-85, -1.8782e-86,  3.8720e-85],\n",
      "        [-5.2434e-86,  1.5323e-85,  4.7038e-86],\n",
      "        [-1.7613e-85,  3.7861e-85, -7.6492e-86],\n",
      "        [ 3.4137e-86, -5.7319e-86,  1.6738e-85],\n",
      "        [ 1.1585e-85, -1.7387e-85,  1.0887e-85]])\n",
      "Y_b :  tensor([[50, 73,  2,  4,  2],\n",
      "        [38,  0,  0,  8,  1],\n",
      "        [ 1,  1,  8, 15, 30],\n",
      "        [ 0,  4, 11,  1,  3],\n",
      "        [ 2,  1,  1,  0,  0]])\n",
      "loss :  tensor(2.6973e-238, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 1.1442e-236, -4.9117e-237,  8.7234e-237],\n",
      "        [ 1.4919e-237, -4.7490e-237,  2.8270e-236],\n",
      "        [ 4.4392e-238, -2.0282e-237,  1.3659e-236],\n",
      "        [-2.8016e-237,  5.8840e-238, -5.7441e-237],\n",
      "        [-6.7180e-237,  1.4522e-237, -8.6470e-237]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 1.1442e-236, -4.9117e-237,  8.7234e-237],\n",
      "        [ 1.4919e-237, -4.7490e-237,  2.8270e-236],\n",
      "        [ 4.4392e-238, -2.0282e-237,  1.3659e-236],\n",
      "        [-2.8016e-237,  5.8840e-238, -5.7441e-237],\n",
      "        [-6.7180e-237,  1.4522e-237, -8.6470e-237]])\n",
      "Y_b :  tensor([[ 0, 16,  0,  0,  0],\n",
      "        [ 1,  1,  8,  4,  8],\n",
      "        [ 0,  1,  1,  8,  4],\n",
      "        [ 0,  2,  1,  4,  1],\n",
      "        [ 4,  0,  2,  0,  0]])\n",
      "loss :  tensor(3.9701e-64, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(5.5346e-78, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-1.4371e-63, -5.6080e-63,  4.4983e-63],\n",
      "        [ 2.9156e-63,  4.4100e-63, -8.6561e-63],\n",
      "        [ 1.4843e-63,  9.5690e-64,  2.3628e-62],\n",
      "        [-2.7339e-63, -5.5260e-63,  6.3246e-63],\n",
      "        [-4.3149e-64, -4.5477e-63, -7.0842e-64]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-1.4371e-63, -5.6080e-63,  4.4983e-63],\n",
      "        [ 2.9156e-63,  4.4100e-63, -8.6561e-63],\n",
      "        [ 1.4843e-63,  9.5690e-64,  2.3628e-62],\n",
      "        [-2.7339e-63, -5.5260e-63,  6.3246e-63],\n",
      "        [-4.3149e-64, -4.5477e-63, -7.0842e-64]])\n",
      "Y_b :  tensor([[ 1,  1, 10,  0,  1],\n",
      "        [ 3,  0,  0,  9,  5],\n",
      "        [ 1,  2,  4,  0,  0],\n",
      "        [ 0,  4,  2, 40, 38],\n",
      "        [ 6,  8,  0,  8, 12]])\n",
      "loss :  tensor(2.5123e-171, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 1.0833e-170, -4.1020e-170, -4.5427e-170],\n",
      "        [ 4.7099e-171, -6.6926e-171,  2.3282e-170],\n",
      "        [-1.2425e-170,  3.3429e-170,  5.4042e-170],\n",
      "        [ 8.7774e-170, -1.1079e-169,  4.0969e-170],\n",
      "        [ 8.2500e-170, -1.0510e-169,  5.7013e-170]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 1.0833e-170, -4.1020e-170, -4.5427e-170],\n",
      "        [ 4.7099e-171, -6.6926e-171,  2.3282e-170],\n",
      "        [-1.2425e-170,  3.3429e-170,  5.4042e-170],\n",
      "        [ 8.7774e-170, -1.1079e-169,  4.0969e-170],\n",
      "        [ 8.2500e-170, -1.0510e-169,  5.7013e-170]])\n",
      "Y_b :  tensor([[ 5,  4,  4,  0,  0],\n",
      "        [ 1,  1,  2,  0,  1],\n",
      "        [ 0,  1,  4,  0,  0],\n",
      "        [11,  0,  0, 65, 31],\n",
      "        [ 0,  4, 18,  1,  1]])\n",
      "loss :  tensor(1.6760e-134, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(3.2560e-148, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[ 3.3395e-134, -7.1030e-134,  3.8923e-134],\n",
      "        [-1.0772e-134, -1.0874e-133,  6.9450e-135],\n",
      "        [-4.3516e-133, -3.2847e-134,  9.7308e-133],\n",
      "        [-1.0865e-132,  1.5291e-132, -5.0078e-133],\n",
      "        [-3.0061e-133,  5.9256e-133, -2.0483e-133]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[ 3.3395e-134, -7.1030e-134,  3.8923e-134],\n",
      "        [-1.0772e-134, -1.0874e-133,  6.9450e-135],\n",
      "        [-4.3516e-133, -3.2847e-134,  9.7308e-133],\n",
      "        [-1.0865e-132,  1.5291e-132, -5.0078e-133],\n",
      "        [-3.0061e-133,  5.9256e-133, -2.0483e-133]])\n",
      "Y_b :  tensor([[ 2,  3,  3, 20,  5],\n",
      "        [ 0,  0,  1,  5,  5],\n",
      "        [ 1,  9,  2,  2,  2],\n",
      "        [22,  9,  0,  1,  0],\n",
      "        [ 0,  9, 13,  2,  3]])\n",
      "loss :  tensor(3.0089e-59, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(2.6009e-73, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-3.6231e-59,  3.4468e-58, -2.6468e-58],\n",
      "        [ 5.6825e-58, -2.0152e-58, -3.0173e-58],\n",
      "        [ 3.2022e-58, -4.1205e-58,  1.8294e-58],\n",
      "        [-1.8932e-58, -6.0318e-58,  4.8633e-58],\n",
      "        [ 1.7476e-58, -2.4931e-58,  9.5061e-59]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-3.6231e-59,  3.4468e-58, -2.6468e-58],\n",
      "        [ 5.6825e-58, -2.0152e-58, -3.0173e-58],\n",
      "        [ 3.2022e-58, -4.1205e-58,  1.8294e-58],\n",
      "        [-1.8932e-58, -6.0318e-58,  4.8633e-58],\n",
      "        [ 1.7476e-58, -2.4931e-58,  9.5061e-59]])\n",
      "Y_b :  tensor([[ 5, 23, 15,  0,  0],\n",
      "        [ 0,  4,  3,  5,  2],\n",
      "        [ 3,  0,  0,  0,  0],\n",
      "        [ 1,  1,  0,  0,  1],\n",
      "        [ 0,  0,  9, 15, 34]])\n",
      "loss :  tensor(1.4764e-80, grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(1.3192e-94, grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[-7.2442e-80, -6.4027e-80, -5.9481e-80],\n",
      "        [-2.7780e-79, -2.3959e-79, -3.2005e-79],\n",
      "        [ 9.3284e-80,  1.8244e-80,  1.2592e-79],\n",
      "        [-2.2819e-80, -1.0899e-79,  3.6390e-81],\n",
      "        [ 3.8598e-80, -2.0111e-79, -2.4440e-80]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[-7.2442e-80, -6.4027e-80, -5.9481e-80],\n",
      "        [-2.7780e-79, -2.3959e-79, -3.2005e-79],\n",
      "        [ 9.3284e-80,  1.8244e-80,  1.2592e-79],\n",
      "        [-2.2819e-80, -1.0899e-79,  3.6390e-81],\n",
      "        [ 3.8598e-80, -2.0111e-79, -2.4440e-80]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = 5\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "\n",
    "torch.manual_seed(int(time.time()))\n",
    "for Y_b, covariates_b, O_b in model.get_batch(model.batch_size): \n",
    "    print('Y_b : ', Y_b)\n",
    "    W = torch.randn(5, model.batch_size,q)\n",
    "    model.check_batch(Y_b, covariates_b, O_b,W)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n",
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n",
      "---------------------MSE Sigma:  5.654197074345726\n",
      "MSE beta :  1.6826883721366408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood-------------------------------------- tensor(0., grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "model.fit_all(0.1,0.1,N_iter = 50, acc = 0.01)\n",
    "#model.compute_likelihood(0.0001)\n",
    "#model.fit(10,0.001, lr_beta = 0.1,lr_C = 0.1, C_optim = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''   def compute_single_log_like(self, i, acc):\n",
    "        N_iter = int(1/acc)\n",
    "        E = 0 \n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            E -= 1/2*SLA.norm(W)**2\n",
    "            E -= np.sum(np.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            E+= np.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            #print('E : ', E)\n",
    "        E/= N_iter\n",
    "        return E\n",
    "    \n",
    "    def batch_log_like(self,acc): \n",
    "        batch_E = 0\n",
    "        for i in range(10): \n",
    "            batch_E += self.compute_single_log_like(i,acc) \n",
    "        return batch_E\n",
    "    \n",
    "    def single_grad_beta_log_like(self,i, acc): \n",
    "        N_iter = int(1/acc)\n",
    "        grad = 0\n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(np.exp(self.O[i,:]+ self.covariates[i,:]@self.beta+self.C@W)).reshape(1,-1)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(self.Y[i,:].reshape(1,-1))\n",
    "        return grad/N_iter\n",
    "'''\n",
    "'''\n",
    "fonctions pour tester les gradients avec des W que l'on simule qu'une seule fois \n",
    "    def batch_grad_beta(self, acc): \n",
    "        batch_grad = 0\n",
    "        for i in range(10): \n",
    "            batch_grad += self.single_grad_beta_log_like(i,acc) \n",
    "        return batch_grad\n",
    "        \n",
    "        def single_likelihood(self,i,W): \n",
    "            ## W should be an array of size q \n",
    "            \n",
    "\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "\n",
    "        norm_W = TLA.norm(W)**2\n",
    "        log_fact = -torch.sum(log_stirling(Y_i))\n",
    "        Z_i = x_i@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_i +Z_i))\n",
    "        data_term = torch.sum(Y_i*(O_i+Z_i))\n",
    "        return torch.exp(log_fact + exp_term + data_term-1/2*norm_W)\n",
    "            \n",
    "    def single_grad_beta(self,i,W): \n",
    "        likeli = self.single_likelihood(i,W)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:].reshape(1,-1)\n",
    "        O_i = self.O[i,:]\n",
    "        exp = torch.exp(O_i + x_i@self.beta + W@(self.C.T))\n",
    "        return likeli*(x_i.T@(-exp+Y_i))\n",
    "    \n",
    "    def single_fit(self,i,W): \n",
    "        loss = self.single_likelihood(i,W)\n",
    "        loss.backward()\n",
    "        print('error : ', torch.norm(self.beta.grad-self.single_grad_beta(i,W)))\n",
    "    def batch_likelihood_test(self,Y_b,covariates_b, O_b, W): \n",
    "        norm_W = torch.sum(torch.norm(W, dim = 1)**2)\n",
    "        log_fact = torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b))\n",
    "        return torch.exp(-log_fact-norm_W/2 +exp_term + data_term)\n",
    "    \n",
    "    def batch_grad_beta_test(self,Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood_test(Y_b,covariates_b, O_b, W)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return likelihood*(covariates_b.T@(-torch.exp(Z_b)+Y_b))\n",
    "        \n",
    "   '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA_bis(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        #self.C = torch.clone(true_C)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def single_likelihood_test(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        W = torch.randn(N_samples, self.n, self.q)\n",
    "        likelihood +=  self.batch_likelihood(self.Y,self.covariates, self.O,W)\n",
    "        return likelihood/self.n\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def batch_grad_beta(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0\n",
    "        log_like = torch.sum(self.batch_likelihood(Y_b,covariates_b, O_b,W, somme = False), axis = 1)\n",
    "        first_term = -torch.exp(O_b +covariates_b@self.beta + W@(self.C.T))\n",
    "        second_term = Y_b\n",
    "        grad =  torch.sum(torch.multiply(log_like.reshape(-1,1,1),((covariates_b.T)@(first_term + second_term))), axis = 0)\n",
    "        # the for loop here does the same, just a sanity check\n",
    "        '''\n",
    "        grad = 0\n",
    "        for k in range(N_samples): \n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k])#/N_samples\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*(covariates_b.T)@(exp_term + Y_b)\n",
    "        '''\n",
    "        return grad/W.shape[0]\n",
    "    \n",
    "    \n",
    "    def batch_grad_C(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0 \n",
    "        for k in range(W.shape[0]):\n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k], somme = True)\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*((exp_term + 0*Y_b).T@W[k])\n",
    "        return grad/W.shape[0]\n",
    "            \n",
    "    def fit(self, N_iter, acc): \n",
    "        optim_beta = torch.optim.Rprop([self.beta], lr = 0.3)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = 0.3)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q) \n",
    "                if False : \n",
    "                    optim_C.zero_grad()\n",
    "                    #print('MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    grad_C = self.batch_grad_C(Y_b, covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    #self.C.grad =  -grad_C/torch.norm(grad_C)\n",
    "                    print('loss : ', loss.item())\n",
    "                    optim_C.step()\n",
    "                else : \n",
    "                    optim_beta.zero_grad()\n",
    "                    print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    grad_beta = self.batch_grad_beta(Y_b,covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    self.beta.grad = -grad_beta/torch.mean(torch.abs(grad_beta))\n",
    "                    optim_beta.step()\n",
    "            print('----------------------------------------------------------------------new_epoch')\n",
    "            \n",
    "            \n",
    "    def batch_likelihood(self,Y_batch,covariates_batch, O_batch, W, somme = True ): \n",
    "        '''\n",
    "        computes the approximation of the likelihood of a batch. \n",
    "        \n",
    "        args : \n",
    "                'Y_batch' : tensor of size(batch_size, p)\n",
    "                'covariates_batch' : tensor of size(batch_size, d)\n",
    "                'O_batch' : tensor of size(batch_size, p)\n",
    "                'acc' : float. the accuracy you want. The lower the accuracy, the lower the algorithm. \n",
    "                        we will sampThe size of tensor a (1000) must match the size of tensor b (20) at non-singleton dimension 2les 1/acc times. \n",
    "        returns : \n",
    "                the approximation of the likelihood. \n",
    "        ''' \n",
    "    \n",
    "        last_dim = len(W.shape)-1\n",
    "        if last_dim >1 : \n",
    "            N_samples = W.shape[0] # number of samples of W \n",
    "        else : N_samples = 1\n",
    "        #N_samples = W.shape[0]\n",
    "        Z = covariates_batch@self.beta + W@(self.C.T)\n",
    "        norm_W = TLA.norm(W, dim = last_dim)**2\n",
    "        log_fact =  torch.sum(log_stirling(Y_batch), axis = 1) # the factorial term \n",
    "        poiss_like =  - torch.sum(torch.exp(O_batch+Z), axis = last_dim) # first term of the poisson likelihood\n",
    "                                                                         #the normalising term with the exponential \n",
    "        poiss_like += torch.sum((O_batch+Z)*Y_batch, axis = last_dim)    # second term of the poisson likelihood\n",
    "        \n",
    "            \n",
    "        if somme : \n",
    "            # If we want the true likelihood\n",
    "            # We first take the exponential of the sum of the logs and then divide by the Number of samples we took.  \n",
    "            return torch.sum(torch.exp(-log_fact -1/2*norm_W+poiss_like))/N_samples \n",
    "        #for some purposes, we may want only the exponential and not the sum\n",
    "        else : return torch.exp(-log_fact -1/2*norm_W+poiss_like)/N_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE beta :  1.9102846328682492\n",
      "MSE beta :  1.8096150648046705\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  1.8552132061091728\n",
      "MSE beta :  1.9129760878867497\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.0701245395137264\n",
      "MSE beta :  2.3871577710512315\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.6920821053624366\n",
      "MSE beta :  3.339558541508027\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  4.266740013933548\n",
      "MSE beta :  5.829849607680941\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  8.225635687097924\n",
      "MSE beta :  9.993858093434689\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  13.342151645474365\n",
      "MSE beta :  14.396301048193923\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  17.317854779004286\n",
      "MSE beta :  19.84766625799734\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  23.838020380241492\n",
      "MSE beta :  24.570160465086182\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  27.37239572319345\n",
      "MSE beta :  28.576635872398175\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  29.592638528944445\n",
      "MSE beta :  30.05920858997783\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.101195695338298\n",
      "MSE beta :  31.85275115268177\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  32.91296705718656\n",
      "MSE beta :  34.02862881189437\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  34.21566166371409\n",
      "MSE beta :  33.66280883232819\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  33.16962321568695\n",
      "MSE beta :  32.483863490851355\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.87436034427889\n",
      "MSE beta :  31.822704369688143\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.48744476301318\n",
      "MSE beta :  31.168103517498643\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.860666924171813\n",
      "MSE beta :  30.878538073475863\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.748499069595027\n",
      "MSE beta :  30.507270106335973\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.25545053219896\n",
      "MSE beta :  30.294877734221195\n",
      "----------------------------------------------------------------------new_epoch\n"
     ]
    }
   ],
   "source": [
    "model_bis = MC_PLNPCA_bis(q,n//2) \n",
    "model_bis.init_data(Y_sampled, O,covariates )\n",
    "model_bis.fit(20,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def proba_mean(mean_):\n",
    "    mean = torch.Tensor([mean_])\n",
    "    k = torch.Tensor([int(mean)])\n",
    "    log_fact = -log_stirling(torch.Tensor([k]))\n",
    "    exp_term = - mean\n",
    "    data_term = k*torch.log(mean)\n",
    "    somme = log_fact + exp_term + data_term \n",
    "    #print('somme ', somme)\n",
    "    #print('result : ', torch.exp(somme))\n",
    "    return torch.exp(somme)\n",
    "\n",
    "for i in range(60): \n",
    "    pass\n",
    "    #print('proab', proba_mean(np.exp(i)))\n",
    "\n",
    "length = 100\n",
    "prob = [proba_mean(np.exp(i)).numpy() for i in range(length)]\n",
    "\n",
    "fig= plt.figure()\n",
    "plt.plot(np.arange(length),prob, label = 'P(poiss(n)=n)')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('probability')\n",
    "plt.xlabel('log(n)')\n",
    "plt.title('Probability that a Poisson(exp(n))= n ')\n",
    "plt.legend()\n",
    "#plt.savefig('poisson distribution')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.36787944117144233, 0.2706705664732254, 0.22404180765538773, 0.19536681481316456, 0.17546736976785068, 0.16062314104798003, 0.1490027796743379, 0.13958653195059692, 0.1317556400095227, 0.1251100357211333, 0.1193780602280255, 0.11436791550944654, 0.10993981424841086, 0.10598914793051553, 0.10243586666453418, 0.09921753162215582, 0.09628462779844535, 0.09359731648870141, 0.09112313246841229, 0.08883531739208522, 0.08671159160336754, 0.0847332342752624, 0.08288438439146861, 0.0811515025272517, 0.07952295146806547, 0.07798866585178797, 0.07653988933052164, 0.07516896352687369, 0.07386915713933492, 0.07263452647159147, 0.07145980077773666, 0.07034028736850317, 0.06927179257562581, 0.06825055553466172, 0.06727319239963175, 0.06633664910130228, 0.06543816114459435, 0.06457521923824946, 0.06374553978250255, 0.06294703942359217, 0.062177813028978145, 0.061436114552760206, 0.06072034035351376, 0.06002901460152964, 0.05936077647306704, 0.058714368878625516, 0.058088628512689325, 0.05748247704566866, 0.05689491330625193, 0.05632500632519082, 0.05577188913053974, 0.05523475320026098, 0.054712843491442474, 0.05420545397660961, 0.05371192362710683, 0.053231632791576916, 0.052763999924414016, 0.05230847862491079, 0.05186455495282056, 0.051431744990345855, 0.0510095926242581, 0.05059766752503755, 0.05019556330267834, 0.04980289582119196, 0.049419301655917874, 0.04904443667955918, 0.04867797476443825, 0.04831960658984965, 0.04796903854459724]\n"
     ]
    }
   ],
   "source": [
    "def poiss_density(mean, k ): \n",
    "    return 1/factorial(k)*np.exp(-mean)*mean**k\n",
    "\n",
    "#print([poiss_density(i,i) for i in range(70)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
