{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bastien/anaconda3/lib/python3.8/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /opt/conda/conda-bld/pytorch_1607369981906/work/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import seaborn as sns \n",
    "import torch \n",
    "from pandas import read_csv\n",
    "import time\n",
    "#torch.set_default_dtype(torch.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\mathbf{x}_{i}\\beta +W_{i}\\mathbf{C}^{\\top}, \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta} P_{\\theta}(Y)$$\n",
    "\n",
    "But we need to integrate out $W$ in order to compute the quantity inside the max : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i}\\right) &=\\int P_{\\theta}\\left(Y_{i}, W\\right) d W \\\\\n",
    "&=\\int P_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This integral being untractable, we are going to approximate it with Monte Carlo methods : \n",
    "\n",
    "$$\n",
    "\\int p_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W \\approx \\frac{1}{K} \\sum_{k = 1 }^Kp_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)\n",
    "$$\n",
    "$$W_{i,k} \\sim \\mathcal N (0, I_q)$$\n",
    "\n",
    "The larger the $K$ the better the approximation.  \n",
    "\n",
    "Let's compute $p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)$. \n",
    "\n",
    "\n",
    "First, \n",
    "\n",
    "$$\n",
    "P\\left(W_{i,k}\\right)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2}\\left\\|W_{i,k}\\right\\|_{2}^{2}\\right)\n",
    "$$ \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical purposes, we may want to use a logarithmic scale and apply the exponential function after. Indeed, $Y_{ij}$ can go up to a thousand, and computing this factorial would give infinite values. \n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=\\sum_{j=1}^{p} - \\ln \\left(Y_{i j} !\\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+Z_{i j}\\right)\n",
    "$$\n",
    "\n",
    "If we consider the whole likelihood : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ln p_{\\theta}\\left(Y \\mid W_{k}\\right) &=\\sum_{i=1}^{n} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i k}\\right) \\\\\n",
    "&=\\sum_{i=1}^{n} \\sum_{j} - \\ln \\left(Y_{ij} ! \\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(0_{i j}+Z_{i j}\\right) \\\\\n",
    "&=1_{n}^{T}\\left[-\\ln (Y !)-\\exp (0+Z)+Y \\odot (0+Z)\\right] 1_{p} \\\\\n",
    "Z=& X \\beta+W_{k} C^{\\top}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We are going tu use the Stirling Formula in order to compute the log of the factorial, to avoid computing directly the factorial.  \n",
    "\n",
    "We now need to compute the gradients. Since \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i}| W_{i,k}\\right) \\nabla_{\\theta} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\n",
    "$$\n",
    "\n",
    "We get : \n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-x_{i}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ x_{i}^{\\top}Y_i\\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-W_{i,k}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ W_{i,k}^{\\top}Y_i\\right]\n",
    "$$\n",
    "This is if we take only one sample $Y_i$. If we take the whole dataset (or a mini-batch), we get (writed in matrix form) :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After trying to optimize the likelihood, we have found that the likelihood is very small (the order of the log likelihood is about $1e^{-3}$ for some samples which makes the exponential 0 numerically. We can't optimize the log likelihood directly due to the integral : \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p_{\\theta}(Y) &=\\log \\left(\\int p_{\\theta}(Y, W)dW\\right) \\\\\n",
    "& \\approx \\log \\left(\\frac{1}{K}\\sum^K p_{\\theta}\\left(Y, W_{k}\\right)\\right)\\\\\n",
    "& \\neq \\frac{1}{K} \\sum^{K} \\log \\left(p_{\\theta}\\left(Y, W_{k}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Indeed, we must compute the likelihood which results in numerical zero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$C :  (p,q)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be smaller (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*toeplitz(0.95**np.arange(block_size))\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.98**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    w,v = SLA.eigh(Sigma)\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:]))\n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(dict_models,name_doss, percentage_kept = 1, save = False):\n",
    "    '''\n",
    "    function to compare models. It will plot the MSE of Sigma and beta according to the true parameters and \n",
    "    the likelihood in the y axis with the runtime in the x-axis.\n",
    "    \n",
    "    args : \n",
    "        'dict_models' dict with key the name of the model and values the model.\n",
    "        'name_doss' : str. the name of the file you want to save the graphic. \n",
    "        'percentage_kept' float . should be positive and lower than 1\n",
    "        'save' : bool. If True, the graphic will be saved. If false, won't be saved. \n",
    "        \n",
    "    returns : \n",
    "            None but displays the figure. you can save the figure if you uncomment the last line. \n",
    "    '''\n",
    "    \n",
    "    fig,ax = plt.subplots(7,1,figsize = (15,15))\n",
    "    \n",
    "    for name,model in dict_models.items() : \n",
    "        \n",
    "        d = model.beta.shape[0]\n",
    "\n",
    "        abscisse = model.running_times\n",
    "        plt.subplots_adjust(hspace = 0.4)\n",
    "        ax[0].plot(abscisse, model.MSE_Sigma_list, label = name)\n",
    "        ax[0].legend()\n",
    "        ax[0].set_title('MSE Sigma')\n",
    "        #ax[0].set_xlabel('Seconds')\n",
    "        ax[0].set_ylabel('MSE')\n",
    "        ax[0].set_yscale('log')\n",
    "        #ax[0].set_yticks()\n",
    "        \n",
    "        ax[1].plot(abscisse, model.MSE_beta_list, label = name)\n",
    "        ax[1].legend()\n",
    "        ax[1].set_title('MSE beta') \n",
    "        #ax[1].set_xlabel('Seconds')\n",
    "        ax[1].set_ylabel('MSE')\n",
    "        ax[1].set_yscale('log')\n",
    "        length = len(abscisse)\n",
    "        \n",
    "        ax[2].plot(abscisse[-int(length*percentage_kept):], model.likelihood_list[-int(length*percentage_kept):], label = name )\n",
    "        ax[2].legend()\n",
    "        ax[2].set_yscale('log')\n",
    "        ax[2].set_title('likelihood')\n",
    "        ax[2].set_ylabel('likelihood')\n",
    "        #ax[2].set_xlabel('Seconds')\n",
    "\n",
    "        '''\n",
    "        ax[3].plot(abscisse, model.norm_grad_C_list, label = name)\n",
    "        ax[3].set_title('norm Grad C ')\n",
    "        #ax[3].set_xlabel('Seconds')\n",
    "        ax[3].set_ylabel('L1 norm')\n",
    "        ax[3].set_yscale('log')\n",
    "        ax[3].legend()\n",
    "        \n",
    "        ax[4].plot(abscisse, model.norm_grad_beta_list, label = name)\n",
    "        ax[4].legend()\n",
    "        ax[4].set_title('norm grad beta')\n",
    "        #ax[4].set_xlabel('Seconds')\n",
    "        ax[4].set_ylabel('L1 norm') \n",
    "        ax[4].set_yscale('log')\n",
    "    \n",
    "        ax[5].plot(abscisse, model.norm_grad_log_beta_list, label = name)\n",
    "        ax[5].legend()\n",
    "        ax[5].set_title('Gradient norm wrt beta of the loglikelihood given W')\n",
    "        #ax[5].set_xlabel('Seconds')\n",
    "        ax[5].set_ylabel('L1 norm') \n",
    "        ax[5].set_yscale('log')\n",
    "        \n",
    "        ax[6].plot(abscisse, model.norm_grad_log_C_list, label = name)\n",
    "        ax[6].legend()\n",
    "        ax[6].set_title('Gradient norm wrt C of the loglikelihood given W')\n",
    "        ax[6].set_xlabel('Seconds')\n",
    "        ax[6].set_ylabel('L1 norm') \n",
    "        #ax[6].set_yscale('log')\n",
    "        '''\n",
    "        \n",
    "        \n",
    "    if save : \n",
    "        plt.savefig(name_doss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes anything since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        torch.manual_seed(0)\n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        \n",
    "        #self.C = torch.clone(true_C)\n",
    "        \n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        \n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        \n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        self.running_times = list()\n",
    "        self.norm_grad_C_list = list()\n",
    "        self.norm_grad_beta_list = list()\n",
    "        self.norm_grad_log_C_list = list()\n",
    "        self.norm_grad_log_beta_list = list()\n",
    "        self.MSE_Sigma_list = list()\n",
    "        self.MSE_beta_list = list()\n",
    "        self.likelihood_list = list()\n",
    "        self.t0 = time.time()\n",
    "        \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        #np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "\n",
    "            \n",
    "    def fit(self, N_iter, acc,lr,class_optimizer = torch.optim.Rprop, my_grad = False): \n",
    "        '''\n",
    "        fit the data. DOes not work yet. You can choose to optimize beta or C\n",
    "        '''\n",
    "        optim = class_optimizer([self.beta,self.C], lr = lr)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            compteur = 0\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q)\n",
    "                #model.check_batch(Y_b, covariates_b, O_b,W)\n",
    "                #ma = torch.max(Y_b).item()\n",
    "                optim.zero_grad()\n",
    "\n",
    "                loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                #diff= torch.norm(grad_C+self.C.grad)/(torch.norm(grad_C)+torch.norm(self.C.grad))\n",
    "                if loss <0 :\n",
    "                    grad_C = self.grad_batch_C(Y_b, covariates_b, O_b,W)\n",
    "                    grad_beta = self.grad_batch_beta(Y_b,covariates_b, O_b,W)\n",
    "                    self.norm_grad_C_list.append(torch.mean(torch.abs(grad_C)).item())\n",
    "                    self.norm_grad_beta_list.append(torch.mean(torch.abs(grad_beta)).item())\n",
    "                    #self.check_grad(grad_C,self.C.grad,grad_beta,self.beta.grad)\n",
    "                    if my_grad : \n",
    "                        self.C.grad = -grad_C#/torch.mean(torch.abs(grad_C))\n",
    "                        self.beta.grad = -grad_beta#/torch.mean(torch.abs(grad_beta))\n",
    "                    else : \n",
    "                        loss.backward()\n",
    "                    random_number = torch.rand(1)\n",
    "                    if random_number >0.9 : \n",
    "                        print('MSE Sigma ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                        print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    optim.step()\n",
    "                    self.running_times.append(time.time()-self.t0)\n",
    "                    self.likelihood_list.append(-loss.item())\n",
    "                    self.MSE_Sigma_list.append((torch.mean((self.get_Sigma()-true_Sigma)**2)).item())\n",
    "                    self.MSE_beta_list.append((torch.mean((self.beta-true_beta)**2)).item())\n",
    "                    '''\n",
    "                    if ma > 500 : \n",
    "                        print('-------------------------------GOOD_ONE : ', Y_b )\n",
    "                        tmp = self.batch_likelihood(Y_b,covariates_b, O_b,W, verbose = True)\n",
    "                        print('loss : ', tmp)\n",
    "                    '''\n",
    "                else : \n",
    "                    compteur+=1\n",
    "                    #self.batch_likelihood(Y_b,covariates_b, O_b,W,verbose = True)\n",
    "\n",
    "            print('log_likelihood', self.compute_mean_log_likelihood(acc))\n",
    "            print('----------------------------------------------------------------------: ', compteur,' batches were full zeros out_of :', self.n//self.batch_size, 'batches')\n",
    "           \n",
    "\n",
    "    \n",
    "    def batch_likelihood(self, Y_b,covariates_b, O_b, W, mean = True, verbose = False): \n",
    "        norm_W = -1/2*torch.sum(torch.norm(W,dim = 2)**2, axis = 1) \n",
    "        log_fact = -torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b),axis = (1,2))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b),axis = (1,2))\n",
    "        sum_of_logs = (norm_W +log_fact +exp_term+data_term) \n",
    "        if verbose :\n",
    "            print('norm_W : ', norm_W.numpy(),'\\n')\n",
    "            print('log_factoriel', log_fact.numpy(),'\\n')\n",
    "            print('exp_term ', exp_term.detach().numpy())\n",
    "            print('data ',data_term.detach().numpy(),'\\n')\n",
    "            print('somme_of_logs ( = norm_W +log_fact + exp_term + data) : ', sum_of_logs.detach().numpy(),'\\n' )\n",
    "            print('somme -max : ',(sum_of_logs-torch.max(sum_of_logs)).detach().numpy(),'\\n' )\n",
    "            print('result = exp(somme)  ', torch.exp(sum_of_logs).detach().numpy(),'\\n')\n",
    "        nb_nonzero =  torch.sum(torch.exp(sum_of_logs)>0)\n",
    "        if mean : \n",
    "            return torch.sum(torch.exp(sum_of_logs-0*torch.max(sum_of_logs)))/nb_nonzero\n",
    "            #return torch.mean(torch.exp(-1/2*norm_W -log_fact +exp_term+data_term))\n",
    "        else : \n",
    "            return torch.exp(sum_of_logs)\n",
    "    def grad_batch_beta(self, Y_b,covariates_b, O_b, W):\n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        nb_non_zero = torch.sum(likelihood >0)\n",
    "        grad_log = (covariates_b.unsqueeze(2).T@(-torch.exp(Z_b)+Y_b))\n",
    "        if nb_non_zero > 0 : \n",
    "            self.norm_grad_log_beta_list.append(torch.mean(torch.abs(torch.sum(grad_log, axis = 0)/nb_non_zero)).item())\n",
    "        \n",
    "        return torch.sum(likelihood.reshape(-1,1,1)*grad_log, axis = 0)/nb_non_zero\n",
    "    \n",
    "    def batch_grad_log_Beta(self, Y_b,covariates_b, O_b, W): \n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return  (covariates_b.unsqueeze(2).T@(-torch.exp(Z_b)+Y_b))\n",
    "        \n",
    "    def grad_batch_C(self, Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        grad_log = ((-torch.exp(Z_b)+Y_b).permute(0,2,1)@W)\n",
    "        nb_non_zero = torch.sum(likelihood >0)\n",
    "        #print('non_zero : ', nb_non_zero)\n",
    "        if nb_non_zero > 0 : \n",
    "            self.norm_grad_log_C_list.append(torch.mean(torch.abs(torch.sum(grad_log, axis = 0)/nb_non_zero)).item())\n",
    "            \n",
    "        return torch.sum(likelihood.reshape(-1,1,1)*grad_log, axis = 0)/nb_non_zero  \n",
    "        \n",
    "    def batch_grad_log_C(self, Y_b,covariates_b, O_b, W):\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return ((-torch.exp(Z_b)+Y_b).permute(0,2,1)@W)\n",
    "        \n",
    "    def check_grad(self,my_grad_C, true_grad_C, my_grad_beta, true_grad_beta): \n",
    "        diff_C = torch.norm(my_grad_C-true_grad_C)\n",
    "        diff_beta = torch.norm(my_grad_beta-true_grad_beta)\n",
    "        print('diff_C : ', diff_C)\n",
    "        print('my_grad_C : ', my_grad_C)\n",
    "        print('true__grad_C : ', true_grad_C)\n",
    "        print('diff_beta : ', diff_beta)\n",
    "        print('my_grad_beta : ', my_grad_beta)\n",
    "        print('true_grad_beta : ', true_grad_beta)\n",
    "        \n",
    "            \n",
    "    def get_mean_likelihood(self):\n",
    "        length = len(self.likelihood_list)\n",
    "        return torch.mean(torch.tensor([x  for x in self.likelihood_list[length//2:] if np.isnan(x) == False]))\n",
    "    \n",
    "    \n",
    "    def compute_mean_log_likelihood(self, acc): \n",
    "        '''\n",
    "        computes the likelihood of the whole dataset. \n",
    "        '''\n",
    "        log_likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        compteur = 0 \n",
    "        for Y_b, covariates_b, O_b in self.get_batch(self.batch_size): \n",
    "            W = torch.randn(N_samples, self.batch_size, self.q)\n",
    "            batch_like = self.batch_likelihood(Y_b,covariates_b,O_b,W)\n",
    "            if torch.isnan(batch_like).item() == False : \n",
    "                log_likelihood +=  torch.log(batch_like)\n",
    "                compteur += self.batch_size\n",
    "        print('compteur : ', compteur)\n",
    "        return log_likelihood/compteur \n",
    "    \n",
    "    def fit_IMPS(self, N_iter, acc,lr,class_optimizer = torch.optim.Rprop, my_grad = False): \n",
    "        '''\n",
    "        fit the data. Dees not work yet. You can choose to optimize beta or C\n",
    "        '''\n",
    "        optim = class_optimizer([self.beta,self.C], lr = lr)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            compteur = 0\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q)\n",
    "                print('shape ;', self.each_log_P_WY_givenY(Y_b,covariates_b,O_b,W[0]).shape)\n",
    "                #print('grad_log', self.batch_grad_log_C(Y_b,covariates_b, O_b, W).shape)\n",
    "                optim.zero_grad()\n",
    "                random_number = torch.rand(1)\n",
    "                if random_number >0.9 : \n",
    "                    print('MSE Sigma ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                optim.step()\n",
    "                self.running_times.append(time.time()-self.t0)\n",
    "                #self.likelihood_list.append(-loss.item())\n",
    "                self.MSE_Sigma_list.append((torch.mean((self.get_Sigma()-true_Sigma)**2)).item())\n",
    "                self.MSE_beta_list.append((torch.mean((self.beta-true_beta)**2)).item())\n",
    "\n",
    "            print('log_likelihood', self.compute_mean_log_likelihood(acc))\n",
    "    def log_P_WYgivenY(self,Y_i,covariates_i, O_i, W):\n",
    "        A_i = O_i + W.reshape(1,-1)@(self.C.T) +covariates_i@self.beta\n",
    "        return -1/2*torch.norm(W)**2 + torch.sum(-torch.exp(A_i)+W.reshape(1,-1)@(self.C.T)*Y_i)\n",
    "\n",
    "    \n",
    "    def each_log_P_WY_givenY(self,Y_b,covariates_b,O_b,W): \n",
    "        A_b = O_b + W@(self.C.T) +covariates_b@self.beta\n",
    "        return -1/2*torch.norm(W,dim = 1)**2+torch.sum(-torch.exp(A_b)+W@(self.C.T)*Y_b, axis = 1)\n",
    "    def find_each_mean(self,Y_b,covariates_b, O_b, lr = 0.5, N_iter_max = 100, tol = 0.00001,class_optimizer = torch.optim.Rprop):\n",
    "        W = torch.randn(self.batch_size,self.q)\n",
    "        W.requires_grad_(True)\n",
    "        optim = class_optimizer([W], lr = lr)\n",
    "        delta = tol +1\n",
    "        old_loss = 0\n",
    "        i = 0\n",
    "        while i < N_iter_max:  #and delta > tol :\n",
    "            optim.zero_grad()\n",
    "            loss = -self.each_log_P_WYgivenY(Y_b,covariates_b, O_b, W)\n",
    "            loss.backward()\n",
    "            delta = abs(loss.item()-old_loss)\n",
    "            print('delta : ', delta)\n",
    "            old_loss = loss .item()\n",
    "            if i //20 == 0: \n",
    "                print('grad_norm : ', torch.norm(W.grad))\n",
    "                print('likelihood : ', -loss)\n",
    "            optim.step()\n",
    "            i+=1\n",
    "    def find_mean(self,Y_i,covariates_i, O_i, lr = 0.5, N_iter_max = 100, tol = 0.00001,class_optimizer = torch.optim.Rprop):\n",
    "        W = torch.randn(self.q)\n",
    "        W.requires_grad_(True)\n",
    "        optim = class_optimizer([W], lr = lr)\n",
    "        delta = tol +1\n",
    "        old_loss = 0\n",
    "        i = 0\n",
    "        while i < N_iter_max:  #and delta > tol : \n",
    "            #print('i : ', i )\n",
    "            optim.zero_grad()\n",
    "            loss = -self.log_P_WYgivenY(Y_i,covariates_i, O_i, W)\n",
    "            loss.backward()\n",
    "            delta = abs(loss.item()-old_loss)\n",
    "            print('delta : ', delta)\n",
    "            old_loss = loss .item()\n",
    "            if i //20 == 0: \n",
    "                print('grad_norm : ', torch.norm(W.grad))\n",
    "                print('likelihood : ', -loss)\n",
    "            optim.step()\n",
    "            i+=1\n",
    "        print('mean : ', W)\n",
    "        \n",
    "    def log_P_WYgivenY_dimj(self,Y_i,covariates_i, O_i, W,j):\n",
    "        A_ij = O_i[j] + W.reshape(1,-1)@(self.C.T[:,j]) +covariates_i.reshape(1,-1)@(self.beta[:,j])\n",
    "        return -1/2*torch.norm(W)**2 + torch.sum(-torch.exp(A_ij)+W.reshape(1,-1)@(self.C.T[:,j])*(Y_i[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 100;  p = 1\n",
    "q = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(int(time.time()))\n",
    "true_Sigma = torch.from_numpy(build_block_Sigma(p,q))/2\n",
    "true_C = torch.from_numpy(C_from_Sigma(true_Sigma, q))\n",
    "true_beta =torch.randn((d, p))/1\n",
    "\n",
    "covariates = torch.randn((n,d))\n",
    "O =  0.5+torch.zeros((n,p))\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled, Z_sampled  = sample_model.sample(true_Sigma,true_beta, O, covariates)\n",
    "Y_sampled = torch.from_numpy(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delta :  23.068997288363953\n",
      "grad_norm :  tensor(16.9888)\n",
      "likelihood :  tensor(-23.0690, grad_fn=<NegBackward>)\n",
      "delta :  5.039898808299206\n",
      "grad_norm :  tensor(16.6039)\n",
      "likelihood :  tensor(-18.0291, grad_fn=<NegBackward>)\n",
      "delta :  5.884531552739833\n",
      "grad_norm :  tensor(16.0718)\n",
      "likelihood :  tensor(-12.1446, grad_fn=<NegBackward>)\n",
      "delta :  6.775501155675438\n",
      "grad_norm :  tensor(15.2527)\n",
      "likelihood :  tensor(-5.3691, grad_fn=<NegBackward>)\n",
      "delta :  7.550684931078246\n",
      "grad_norm :  tensor(13.7450)\n",
      "likelihood :  tensor(2.1816, grad_fn=<NegBackward>)\n",
      "delta :  7.5876636786101646\n",
      "grad_norm :  tensor(10.1764)\n",
      "likelihood :  tensor(9.7693, grad_fn=<NegBackward>)\n",
      "delta :  4.132149790328928\n",
      "grad_norm :  tensor(1.1255)\n",
      "likelihood :  tensor(13.9014, grad_fn=<NegBackward>)\n",
      "delta :  0.0\n",
      "grad_norm :  tensor(1.1255)\n",
      "likelihood :  tensor(13.9014, grad_fn=<NegBackward>)\n",
      "delta :  1.0388242997503951\n",
      "grad_norm :  tensor(6.0318)\n",
      "likelihood :  tensor(12.8626, grad_fn=<NegBackward>)\n",
      "delta :  0.0\n",
      "grad_norm :  tensor(6.0318)\n",
      "likelihood :  tensor(12.8626, grad_fn=<NegBackward>)\n",
      "delta :  0.8510920013678689\n",
      "grad_norm :  tensor(2.9475)\n",
      "likelihood :  tensor(13.7137, grad_fn=<NegBackward>)\n",
      "delta :  0.12795353190782066\n",
      "grad_norm :  tensor(2.0867)\n",
      "likelihood :  tensor(13.8417, grad_fn=<NegBackward>)\n",
      "delta :  0.0\n",
      "grad_norm :  tensor(2.0867)\n",
      "likelihood :  tensor(13.8417, grad_fn=<NegBackward>)\n",
      "delta :  0.07660132269424835\n",
      "grad_norm :  tensor(0.6417)\n",
      "likelihood :  tensor(13.9183, grad_fn=<NegBackward>)\n",
      "delta :  0.0\n",
      "grad_norm :  tensor(0.6417)\n",
      "likelihood :  tensor(13.9183, grad_fn=<NegBackward>)\n",
      "delta :  0.00013573972297820092\n",
      "grad_norm :  tensor(0.6649)\n",
      "likelihood :  tensor(13.9181, grad_fn=<NegBackward>)\n",
      "delta :  0.0\n",
      "grad_norm :  tensor(0.6649)\n",
      "likelihood :  tensor(13.9181, grad_fn=<NegBackward>)\n",
      "delta :  0.009210687523845351\n",
      "grad_norm :  tensor(0.0022)\n",
      "likelihood :  tensor(13.9273, grad_fn=<NegBackward>)\n",
      "delta :  0.0\n",
      "grad_norm :  tensor(0.0022)\n",
      "likelihood :  tensor(13.9273, grad_fn=<NegBackward>)\n",
      "delta :  0.0022712052411009154\n",
      "grad_norm :  tensor(0.3279)\n",
      "likelihood :  tensor(13.9251, grad_fn=<NegBackward>)\n",
      "delta :  0.0\n",
      "delta :  0.001712999582698771\n",
      "delta :  0.0005323393151499545\n",
      "delta :  0.0\n",
      "delta :  5.961850966862414e-05\n",
      "delta :  0.0\n",
      "delta :  8.128390821759979e-05\n",
      "delta :  3.832378300572259e-05\n",
      "delta :  0.0\n",
      "delta :  3.765764160590379e-05\n",
      "delta :  3.6457880945306442e-06\n",
      "delta :  0.0\n",
      "delta :  8.488944533979748e-06\n",
      "delta :  7.410305137511841e-06\n",
      "delta :  0.0\n",
      "delta :  6.103292518488956e-06\n",
      "delta :  9.925733017723815e-07\n",
      "delta :  0.0\n",
      "delta :  3.6739747599767725e-07\n",
      "delta :  0.0\n",
      "delta :  3.2266529004232325e-08\n",
      "delta :  0.0\n",
      "delta :  3.785252111754289e-08\n",
      "delta :  0.0\n",
      "delta :  5.4291238171799705e-09\n",
      "delta :  0.0\n",
      "delta :  6.088757942279699e-09\n",
      "delta :  1.6013537162962166e-09\n",
      "delta :  0.0\n",
      "delta :  2.0153727575689118e-09\n",
      "delta :  7.883471653258312e-10\n",
      "delta :  0.0\n",
      "delta :  8.31462898531754e-10\n",
      "delta :  1.566906604466567e-10\n",
      "delta :  0.0\n",
      "delta :  2.3577229057991644e-10\n",
      "delta :  1.326760923348047e-10\n",
      "delta :  0.0\n",
      "delta :  1.2300915841478854e-10\n",
      "delta :  2.0019541580040823e-12\n",
      "delta :  0.0\n",
      "delta :  2.1405099914773018e-11\n",
      "delta :  2.8178348543406173e-11\n",
      "delta :  0.0\n",
      "delta :  2.460076586885407e-11\n",
      "delta :  1.2754242106893798e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "delta :  0.0\n",
      "delta :  3.398170633772679e-12\n",
      "mean :  tensor([1.5338], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(int(time.time()))\n",
    "batch_size = 5\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "i = 0\n",
    "model.find_mean(Y_sampled[i],covariates[i], O[i],N_iter_max = 200, lr = 0.3, class_optimizer = torch.optim.Rprop)\n",
    "#model.compute_mean_log_likelihood(0.001)\n",
    "#%time model.fit_IMPS(50, 0.1,0.3, class_optimizer = torch.optim.Rprop, my_grad = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "size mismatch, get 1, 1x2,1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-8b76590c8fed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mabscisse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabscisse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_P_WYgivenY_dimj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_sampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcovariates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.4037\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabscisse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4037\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-14-8b76590c8fed>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mabscisse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mabscisse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_P_WYgivenY_dimj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_sampled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcovariates\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mO\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0.4037\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mabscisse\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxvline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.4037\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-feaff68b6c79>\u001b[0m in \u001b[0;36mlog_P_WYgivenY_dimj\u001b[0;34m(self, Y_i, covariates_i, O_i, W, j)\u001b[0m\n\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mlog_P_WYgivenY_dimj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mY_i\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcovariates_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mO_i\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 285\u001b[0;31m         \u001b[0mA_ij\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mO_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0mcovariates_i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mA_ij\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m@\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_i\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: size mismatch, get 1, 1x2,1"
     ]
    }
   ],
   "source": [
    "abscisse = np.linspace(-3,3,200)\n",
    "j = 0\n",
    "plt.plot(abscisse, [torch.exp(model.log_P_WYgivenY_dimj(Y_sampled[i],covariates[i],O[i],torch.Tensor([0.4037,w]),j)) for w in abscisse])\n",
    "plt.axvline(0.4037)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.lines.Line2D at 0x7fc4b27d7130>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "abscisse = np.linspace(-3,3,200)\n",
    "j = 0\n",
    "plt.plot(abscisse, [torch.exp(model.log_P_WYgivenY(Y_sampled[i],covariates[i],O[i],torch.Tensor([w]))) for w in abscisse])\n",
    "plt.axvline(1.5358)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''   def compute_single_log_like(self, i, acc):\n",
    "        N_iter = int(1/acc)\n",
    "        E = 0 \n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            E -= 1/2*SLA.norm(W)**2\n",
    "            E -= np.sum(np.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            E+= np.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            #print('E : ', E)\n",
    "        E/= N_iter\n",
    "        return E\n",
    "    \n",
    "    def batch_log_like(self,acc): \n",
    "        batch_E = 0\n",
    "        for i in range(10): \n",
    "            batch_E += self.compute_single_log_like(i,acc) \n",
    "        return batch_E\n",
    "    \n",
    "    def single_grad_beta_log_like(self,i, acc): \n",
    "        N_iter = int(1/acc)\n",
    "        grad = 0\n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(np.exp(self.O[i,:]+ self.covariates[i,:]@self.beta+self.C@W)).reshape(1,-1)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(self.Y[i,:].reshape(1,-1))\n",
    "        return grad/N_iter\n",
    "'''\n",
    "'''\n",
    "fonctions pour tester les gradients avec des W que l'on simule qu'une seule fois \n",
    "    def batch_grad_beta(self, acc): \n",
    "        batch_grad = 0\n",
    "        for i in range(10): \n",
    "            batch_grad += self.single_grad_beta_log_like(i,acc) \n",
    "        return batch_grad\n",
    "        \n",
    "        def single_likelihood(self,i,W): \n",
    "            ## W should be an array of size q \n",
    "            \n",
    "\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "\n",
    "        norm_W = TLA.norm(W)**2\n",
    "        log_fact = -torch.sum(log_stirling(Y_i))\n",
    "        Z_i = x_i@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_i +Z_i))\n",
    "        data_term = torch.sum(Y_i*(O_i+Z_i))\n",
    "        return torch.exp(log_fact + exp_term + data_term-1/2*norm_W)\n",
    "            \n",
    "    def single_grad_beta(self,i,W): \n",
    "        likeli = self.single_likelihood(i,W)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:].reshape(1,-1)\n",
    "        O_i = self.O[i,:]\n",
    "        exp = torch.exp(O_i + x_i@self.beta + W@(self.C.T))\n",
    "        return likeli*(x_i.T@(-exp+Y_i))\n",
    "    \n",
    "    def single_fit(self,i,W): \n",
    "        loss = self.single_likelihood(i,W)\n",
    "        loss.backward()\n",
    "        print('error : ', torch.norm(self.beta.grad-self.single_grad_beta(i,W)))\n",
    "    def batch_likelihood_test(self,Y_b,covariates_b, O_b, W): \n",
    "        norm_W = torch.sum(torch.norm(W, dim = 1)**2)\n",
    "        log_fact = torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b))\n",
    "        return torch.exp(-log_fact-norm_W/2 +exp_term + data_term)\n",
    "    \n",
    "    def batch_grad_beta_test(self,Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood_test(Y_b,covariates_b, O_b, W)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return likelihood*(covariates_b.T@(-torch.exp(Z_b)+Y_b))\n",
    "        \n",
    "   '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA_bis(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        #self.C = torch.clone(true_C)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def single_likelihood_test(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        W = torch.randn(N_samples, self.n, self.q)\n",
    "        likelihood +=  self.batch_likelihood(self.Y,self.covariates, self.O,W)\n",
    "        return likelihood/self.n\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def batch_grad_beta(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0\n",
    "        log_like = torch.sum(self.batch_likelihood(Y_b,covariates_b, O_b,W, somme = False), axis = 1)\n",
    "        first_term = -torch.exp(O_b +covariates_b@self.beta + W@(self.C.T))\n",
    "        second_term = Y_b\n",
    "        grad =  torch.sum(torch.multiply(log_like.reshape(-1,1,1),((covariates_b.T)@(first_term + second_term))), axis = 0)\n",
    "        # the for loop here does the same, just a sanity check\n",
    "        '''\n",
    "        grad = 0\n",
    "        for k in range(N_samples): \n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k])#/N_samples\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*(covariates_b.T)@(exp_term + Y_b)\n",
    "        '''\n",
    "        return grad/W.shape[0]\n",
    "    \n",
    "    \n",
    "    def batch_grad_C(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0 \n",
    "        for k in range(W.shape[0]):\n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k], somme = True)\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*((exp_term + 0*Y_b).T@W[k])\n",
    "        return grad/W.shape[0]\n",
    "            \n",
    "    def fit(self, N_iter, acc): \n",
    "        optim_beta = torch.optim.Rprop([self.beta], lr = 0.3)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = 0.3)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q) \n",
    "                if False : \n",
    "                    optim_C.zero_grad()\n",
    "                    #print('MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    grad_C = self.batch_grad_C(Y_b, covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    #self.C.grad =  -grad_C/torch.norm(grad_C)\n",
    "                    print('loss : ', loss.item())\n",
    "                    optim_C.step()\n",
    "                else : \n",
    "                    optim_beta.zero_grad()\n",
    "                    print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    grad_beta = self.batch_grad_beta(Y_b,covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    self.beta.grad = -grad_beta/torch.mean(torch.abs(grad_beta))\n",
    "                    optim_beta.step()\n",
    "            print('----------------------------------------------------------------------new_epoch')\n",
    "            \n",
    "            \n",
    "    def batch_likelihood(self,Y_batch,covariates_batch, O_batch, W, somme = True ): \n",
    "        '''\n",
    "        computes the approximation of the likelihood of a batch. \n",
    "        \n",
    "        args : \n",
    "                'Y_batch' : tensor of size(batch_size, p)\n",
    "                'covariates_batch' : tensor of size(batch_size, d)\n",
    "                'O_batch' : tensor of size(batch_size, p)\n",
    "                'acc' : float. the accuracy you want. The lower the accuracy, the lower the algorithm. \n",
    "                        we will sampThe size of tensor a (1000) must match the size of tensor b (20) at non-singleton dimension 2les 1/acc times. \n",
    "        returns : \n",
    "                the approximation of the likelihood. \n",
    "        ''' \n",
    "    \n",
    "        last_dim = len(W.shape)-1\n",
    "        if last_dim >1 : \n",
    "            N_samples = W.shape[0] # number of samples of W \n",
    "        else : N_samples = 1\n",
    "        #N_samples = W.shape[0]\n",
    "        Z = covariates_batch@self.beta + W@(self.C.T)\n",
    "        norm_W = TLA.norm(W, dim = last_dim)**2\n",
    "        log_fact =  torch.sum(log_stirling(Y_batch), axis = 1) # the factorial term \n",
    "        poiss_like =  - torch.sum(torch.exp(O_batch+Z), axis = last_dim) # first term of the poisson likelihood\n",
    "                                                                         #the normalising term with the exponential \n",
    "        poiss_like += torch.sum((O_batch+Z)*Y_batch, axis = last_dim)    # second term of the poisson likelihood\n",
    "        \n",
    "            \n",
    "        if somme : \n",
    "            # If we want the true likelihood\n",
    "            # We first take the exponential of the sum of the logs and then divide by the Number of samples we took.  \n",
    "            return torch.sum(torch.exp(-log_fact -1/2*norm_W+poiss_like))/N_samples \n",
    "        #for some purposes, we may want only the exponential and not the sum\n",
    "        else : return torch.exp(-log_fact -1/2*norm_W+poiss_like)/N_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE beta :  1.9102846328682492\n",
      "MSE beta :  1.8096150648046705\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  1.8552132061091728\n",
      "MSE beta :  1.9129760878867497\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.0701245395137264\n",
      "MSE beta :  2.3871577710512315\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.6920821053624366\n",
      "MSE beta :  3.339558541508027\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  4.266740013933548\n",
      "MSE beta :  5.829849607680941\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  8.225635687097924\n",
      "MSE beta :  9.993858093434689\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  13.342151645474365\n",
      "MSE beta :  14.396301048193923\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  17.317854779004286\n",
      "MSE beta :  19.84766625799734\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  23.838020380241492\n",
      "MSE beta :  24.570160465086182\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  27.37239572319345\n",
      "MSE beta :  28.576635872398175\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  29.592638528944445\n",
      "MSE beta :  30.05920858997783\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.101195695338298\n",
      "MSE beta :  31.85275115268177\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  32.91296705718656\n",
      "MSE beta :  34.02862881189437\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  34.21566166371409\n",
      "MSE beta :  33.66280883232819\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  33.16962321568695\n",
      "MSE beta :  32.483863490851355\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.87436034427889\n",
      "MSE beta :  31.822704369688143\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.48744476301318\n",
      "MSE beta :  31.168103517498643\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.860666924171813\n",
      "MSE beta :  30.878538073475863\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.748499069595027\n",
      "MSE beta :  30.507270106335973\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.25545053219896\n",
      "MSE beta :  30.294877734221195\n",
      "----------------------------------------------------------------------new_epoch\n"
     ]
    }
   ],
   "source": [
    "model_bis = MC_PLNPCA_bis(q,n//2) \n",
    "model_bis.init_data(Y_sampled, O,covariates )\n",
    "model_bis.fit(20,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "def proba_mean(mean_):\n",
    "    mean = torch.Tensor([mean_])\n",
    "    k = torch.Tensor([int(mean)])\n",
    "    log_fact = -log_stirling(torch.Tensor([k]))\n",
    "    exp_term = - mean\n",
    "    data_term = k*torch.log(mean)\n",
    "    somme = log_fact + exp_term + data_term \n",
    "    #print('somme ', somme)\n",
    "    #print('result : ', torch.exp(somme))\n",
    "    return torch.exp(somme)\n",
    "\n",
    "for i in range(60): \n",
    "    pass\n",
    "    #print('proab', proba_mean(np.exp(i)))\n",
    "\n",
    "length = 100\n",
    "prob = [proba_mean(np.exp(i)).numpy() for i in range(length)]\n",
    "\n",
    "fig= plt.figure()\n",
    "plt.plot(np.arange(length),prob, label = 'P(poiss(n)=n)')\n",
    "plt.yscale('log')\n",
    "plt.ylabel('probability')\n",
    "plt.xlabel('log(n)')\n",
    "plt.title('Probability that a Poisson(exp(n))= n ')\n",
    "plt.legend()\n",
    "#plt.savefig('poisson distribution')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 0.36787944117144233, 0.2706705664732254, 0.22404180765538773, 0.19536681481316456, 0.17546736976785068, 0.16062314104798003, 0.1490027796743379, 0.13958653195059692, 0.1317556400095227, 0.1251100357211333, 0.1193780602280255, 0.11436791550944654, 0.10993981424841086, 0.10598914793051553, 0.10243586666453418, 0.09921753162215582, 0.09628462779844535, 0.09359731648870141, 0.09112313246841229, 0.08883531739208522, 0.08671159160336754, 0.0847332342752624, 0.08288438439146861, 0.0811515025272517, 0.07952295146806547, 0.07798866585178797, 0.07653988933052164, 0.07516896352687369, 0.07386915713933492, 0.07263452647159147, 0.07145980077773666, 0.07034028736850317, 0.06927179257562581, 0.06825055553466172, 0.06727319239963175, 0.06633664910130228, 0.06543816114459435, 0.06457521923824946, 0.06374553978250255, 0.06294703942359217, 0.062177813028978145, 0.061436114552760206, 0.06072034035351376, 0.06002901460152964, 0.05936077647306704, 0.058714368878625516, 0.058088628512689325, 0.05748247704566866, 0.05689491330625193, 0.05632500632519082, 0.05577188913053974, 0.05523475320026098, 0.054712843491442474, 0.05420545397660961, 0.05371192362710683, 0.053231632791576916, 0.052763999924414016, 0.05230847862491079, 0.05186455495282056, 0.051431744990345855, 0.0510095926242581, 0.05059766752503755, 0.05019556330267834, 0.04980289582119196, 0.049419301655917874, 0.04904443667955918, 0.04867797476443825, 0.04831960658984965, 0.04796903854459724]\n"
     ]
    }
   ],
   "source": [
    "def poiss_density(mean, k ): \n",
    "    return 1/factorial(k)*np.exp(-mean)*mean**k\n",
    "\n",
    "#print([poiss_density(i,i) for i in range(70)])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
