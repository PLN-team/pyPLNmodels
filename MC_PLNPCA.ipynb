{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import torch \n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\beta \\mathbf{x}_{i}+\\mathbf{C} W_{i}, \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the log likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta} P_{\\theta}(Y)$$\n",
    "\n",
    "But we need to integrate out $W$ in order to compute the quantity inside the max : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i}\\right) &=\\int P_{\\theta}\\left(Y_{i}, W\\right) d W \\\\\n",
    "&=\\int P_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This integral being untractable, we are going to approximate it with Monte Carlo methods : \n",
    "\n",
    "$$\n",
    "\\int p_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W \\approx \\frac{1}{K} \\sum_{k = 1 }^Kp_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)\n",
    "$$\n",
    "$$W_{i,k} \\sim \\mathcal N (0, I_q)$$\n",
    "\n",
    "The larger the $K$ the better the approximation.  \n",
    "\n",
    "Let's compute $p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)$. \n",
    "\n",
    "\n",
    "First, \n",
    "\n",
    "$$\n",
    "P\\left(W_{i,k}\\right)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2}\\left\\|W_{i,k}\\right\\|_{2}^{2}\\right)\n",
    "$$ \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C_{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical purposes, we may want to use a logarithmic scale and apply the exponential function after. Indeed, $Y_{ij}$ can go up to a thousand, and computing this factorial would give infinite values. \n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=\\sum_{j=1}^{p} - \\ln \\left(Y_{i j} !\\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+Z_{i j}\\right)\n",
    "$$\n",
    "\n",
    "We are going tu use the Stirling Formula in order to compute the log of the factorial, to avoid computing directly the factorial.  \n",
    "\n",
    "We now need to compute the gradients. Since \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i}| W_{i,k}\\right) \\nabla_{\\theta} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\n",
    "$$\n",
    "\n",
    "We get : \n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-x_{i}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C\\right)+ x_{i}^{\\top}Y_i\\right]\n",
    "$$\n",
    "This is if we take only one sample $Y_i$. If we take the whole dataset (or a mini-batch), we get (writed in matrix form) :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X^{\\top} \\beta+W_k^{\\top} C\\right)+ X^{\\top}Y\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$C :  (p,q)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be smaller (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*0.95**np.arange(block_size)\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.98**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    w,v = SLA.eigh(Sigma)\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:]))\n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 2000; p = 20\n",
    "q = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(0)\n",
    "true_Sigma = torch.from_numpy(build_block_Sigma(p,8))\n",
    "true_C = torch.from_numpy(C_from_Sigma(true_Sigma, q))\n",
    "true_beta =torch.randn((d, p))\n",
    "\n",
    "covariates = torch.randn((n,d))\n",
    "O =  1+torch.zeros((n,p))\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled, Z_sampled  = sample_model.sample(true_Sigma,true_beta, O, covariates)\n",
    "Y_sampled = torch.from_numpy(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes anything since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        self.beta.requires_grad_(True)\n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        #self.C = torch.clone(true_C)\n",
    "    \n",
    "    def single_likelihood(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "    def batch_likelihood(self,Y_batch,covariates_batch, O_batch, W, somme = True ): \n",
    "        '''\n",
    "        computes the approximation of the likelihood of a batch. \n",
    "        \n",
    "        args : \n",
    "                'Y_batch' : tensor of size(batch_size, p)\n",
    "                'covariates_batch' : tensor of size(batch_size, d)\n",
    "                'O_batch' : tensor of size(batch_size, p)\n",
    "                'acc' : float. the accuracy you want. The lower the accuracy, the lower the algorithm. \n",
    "                        we will sampThe size of tensor a (1000) must match the size of tensor b (20) at non-singleton dimension 2les 1/acc times. \n",
    "        returns : \n",
    "                the approximation of the likelihood. \n",
    "        ''' \n",
    "        last_dim = len(W.shape)-1\n",
    "        #if last_dim >1 : \n",
    "        #    N_samples = W.shape[0] # number of samples of W \n",
    "        #else : N_samples = 1\n",
    "        N_samples = W.shape[0]\n",
    "        E_batch = 0\n",
    "        Z = covariates_batch@self.beta + W@(self.C.T)\n",
    "        norm_W = TLA.norm(W, dim = last_dim)\n",
    "        log_fact =  torch.sum(log_stirling(Y_batch), axis = 1) # the factorial term \n",
    "        poiss_like =  - torch.sum(torch.exp(O_batch+Z), axis = last_dim) # first term of the poisson likelihood\n",
    "                                                                         #the normalising term with the exponential \n",
    "        poiss_like += torch.sum((O_batch+Z)*Y_batch, axis = last_dim)    # second ter of the poisson likelihood\n",
    "        if somme : \n",
    "            # If we want the true likelihood\n",
    "            # We first take the exponential of the sum of the logs and then divide by the Number of samples we took.  \n",
    "            return torch.sum(torch.exp(-log_fact -1/2*norm_W+poiss_like))/N_samples \n",
    "        #for some purposes, we may want only the exponential and not the sum\n",
    "        else : return torch.exp(-log_fact -1/2*norm_W+poiss_like)/N_samples\n",
    "\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        W = torch.randn(N_samples, self.n, self.q)\n",
    "        likelihood +=  self.batch_likelihood(self.Y,self.covariates, self.O,W)\n",
    "        return likelihood\n",
    "    def batch_grad_beta(self,Y_b,covariates_b, O_b, acc): \n",
    "        grad = 0 \n",
    "        N_samples = int(1/acc)\n",
    "        W = torch.randn(N_samples,self.batch_size, self.q) \n",
    "        \n",
    "        log_like = torch.sum(self.batch_likelihood(Y_b,covariates_b, O_b,W, somme = False), axis = 1)\n",
    "        exp_term  = -torch.exp(O_b +covariates_b@self.beta + W@(self.C.T))\n",
    "        grad =  torch.sum(torch.multiply(log_like.reshape(-1,1,1),((covariates_b.T)@(exp_term + Y_b))), axis = 0)\n",
    "        # the for loop here does the same, just a sanity check\n",
    "        '''\n",
    "        for i in range(N_samples): \n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[i])/N_iter\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[i]@(self.C.T))\n",
    "            grad +=  log_like*(covariates_b.T)@(exp_term + Y_b)\n",
    "        '''\n",
    "        return grad/N_samples\n",
    "    \n",
    "    def fit(self, N_iter, acc): \n",
    "        optim = torch.optim.Rprop([self.beta], lr = 0.3)\n",
    "        for i in range(N_iter):\n",
    "            for Y_b, covariates_b, O_b in model.get_batch(self.batch_size):\n",
    "                optim.zero_grad()\n",
    "                grad_beta = self.batch_grad_beta(Y_b,covariates_b, O_b,acc)\n",
    "                self.beta.grad = -grad_beta/torch.norm(grad_beta)\n",
    "                optim.step()\n",
    "                print('MSE : ', torch.mean((self.beta-true_beta)**2))\n",
    "                #print('beta : ', self.beta)\n",
    "            print('likelihood : ', self.compute_likelihood(acc))\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X^{\\top} \\beta+W_k^{\\top} C\\right)+ X^{\\top}Y\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  tensor(1.5953, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(1.1537, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.7693, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.5224, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.4527, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.4154, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.4142, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.3931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.3470, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.2770, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.2291, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1980, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1636, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1450, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1330, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1177, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(6.7170e-14, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.1312, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1366, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1291, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1234, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1179, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1104, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1085, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1155, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1247, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1256, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1241, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1211, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1150, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1087, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1028, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.1015, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(5.4079e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0921, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0925, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0927, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0947, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0964, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0961, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0959, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0952, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0946, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0937, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(2.5201e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0929, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0936, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0935, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0934, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0933, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0932, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(9.9851e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0931, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(6.8065e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(7.6581e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(3.1820e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(6.5566e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(3.4678e-12, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(2.6381e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(1.0977e-12, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(1.3149e-12, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(4.0247e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(1.7614e-12, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(4.7892e-12, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(3.1926e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(1.0622e-12, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(6.5076e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(7.0788e-13, grad_fn=<AddBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "MSE :  tensor(0.0930, grad_fn=<MeanBackward0>)\n",
      "likelihood :  tensor(2.2272e-12, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "model = MC_PLNPCA(q,n//16)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "model.fit(20,0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_oaks = read_csv('oaks_counts.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''   def compute_single_log_like(self, i, acc):\n",
    "        N_iter = int(1/acc)\n",
    "        E = 0 \n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            E -= 1/2*SLA.norm(W)**2\n",
    "            E -= np.sum(np.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            E+= np.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            #print('E : ', E)\n",
    "        E/= N_iter\n",
    "        return E\n",
    "    \n",
    "    def batch_log_like(self,acc): \n",
    "        batch_E = 0\n",
    "        for i in range(10): \n",
    "            batch_E += self.compute_single_log_like(i,acc) \n",
    "        return batch_E\n",
    "    \n",
    "    def single_grad_beta_log_like(self,i, acc): \n",
    "        N_iter = int(1/acc)\n",
    "        grad = 0\n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(np.exp(self.O[i,:]+ self.covariates[i,:]@self.beta+self.C@W)).reshape(1,-1)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(self.Y[i,:].reshape(1,-1))\n",
    "        return grad/N_iter\n",
    "    \n",
    "    def batch_grad_beta(self, acc): \n",
    "        batch_grad = 0\n",
    "        for i in range(10): \n",
    "            batch_grad += self.single_grad_beta_log_like(i,acc) \n",
    "        return batch_grad\n",
    "        \n",
    "   '''\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
