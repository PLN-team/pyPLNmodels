{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import seaborn as sns \n",
    "import torch \n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\mathbf{x}_{i}\\beta +W_{i}\\mathbf{C}^{\\top}, \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the log likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta} P_{\\theta}(Y)$$\n",
    "\n",
    "But we need to integrate out $W$ in order to compute the quantity inside the max : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i}\\right) &=\\int P_{\\theta}\\left(Y_{i}, W\\right) d W \\\\\n",
    "&=\\int P_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This integral being untractable, we are going to approximate it with Monte Carlo methods : \n",
    "\n",
    "$$\n",
    "\\int p_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W \\approx \\frac{1}{K} \\sum_{k = 1 }^Kp_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)\n",
    "$$\n",
    "$$W_{i,k} \\sim \\mathcal N (0, I_q)$$\n",
    "\n",
    "The larger the $K$ the better the approximation.  \n",
    "\n",
    "Let's compute $p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)$. \n",
    "\n",
    "\n",
    "First, \n",
    "\n",
    "$$\n",
    "P\\left(W_{i,k}\\right)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2}\\left\\|W_{i,k}\\right\\|_{2}^{2}\\right)\n",
    "$$ \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical purposes, we may want to use a logarithmic scale and apply the exponential function after. Indeed, $Y_{ij}$ can go up to a thousand, and computing this factorial would give infinite values. \n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=\\sum_{j=1}^{p} - \\ln \\left(Y_{i j} !\\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+Z_{i j}\\right)\n",
    "$$\n",
    "\n",
    "If we consider the whole likelihood : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ln p_{\\theta}\\left(Y \\mid W_{k}\\right) &=\\sum_{i=1}^{n} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i k}\\right) \\\\\n",
    "&=\\sum_{i=1}^{n} \\sum_{j} - \\ln \\left(Y_{ij} ! \\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(0_{i j}+Z_{i j}\\right) \\\\\n",
    "&=1_{n}^{T}\\left[-\\ln (Y !)-\\exp (0+Z)+Y \\odot (0+Z)\\right] 1_{p} \\\\\n",
    "Z=& X \\beta+W_{k} C^{\\top}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We are going tu use the Stirling Formula in order to compute the log of the factorial, to avoid computing directly the factorial.  \n",
    "\n",
    "We now need to compute the gradients. Since \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i}| W_{i,k}\\right) \\nabla_{\\theta} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\n",
    "$$\n",
    "\n",
    "We get : \n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-x_{i}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ x_{i}^{\\top}Y_i\\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-W_{i,k}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ W_{i,k}^{\\top}Y_i\\right]\n",
    "$$\n",
    "This is if we take only one sample $Y_i$. If we take the whole dataset (or a mini-batch), we get (writed in matrix form) :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$C :  (p,q)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be smaller (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*0.95**np.arange(block_size)\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.98**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    w,v = SLA.eigh(Sigma)\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:]))\n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 200;  p = 10\n",
    "q = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(0)\n",
    "true_Sigma = torch.from_numpy(build_block_Sigma(p,3))\n",
    "true_C = torch.from_numpy(C_from_Sigma(true_Sigma, q))\n",
    "true_beta =torch.randn((d, p))\n",
    "\n",
    "covariates = torch.randn((n,d))/10\n",
    "O =  1+torch.zeros((n,p))\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled, Z_sampled  = sample_model.sample(true_Sigma,true_beta, O, covariates)\n",
    "Y_sampled = torch.from_numpy(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[  8,   2,   5,  ...,   8,  12,   0],\n",
      "        [  9,   6,   4,  ...,   5,   7,   3],\n",
      "        [ 12,  14,  16,  ...,   0,   0,   0],\n",
      "        ...,\n",
      "        [  1,   3,   7,  ...,  16,  14,   0],\n",
      "        [  0,   1,   0,  ..., 116, 100,  15],\n",
      "        [  1,   4,   3,  ...,   2,   1,   1]])\n"
     ]
    }
   ],
   "source": [
    "print(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.6770, -1.8332,  0.8517, -0.3622],\n",
      "        [-0.0147, -1.4039,  0.7270, -1.6743],\n",
      "        [-0.7963, -2.1970, -0.3506,  0.2023],\n",
      "        [ 1.9993, -0.2395, -0.4235, -1.3158],\n",
      "        [-0.6169,  1.0364,  1.2339,  0.3564],\n",
      "        [ 3.3514,  0.0191,  0.7540, -2.0378],\n",
      "        [-2.1235,  0.5776,  1.9738,  0.1352],\n",
      "        [-2.0279,  2.5704,  0.3475,  3.2637],\n",
      "        [-3.4273,  1.2245, -0.5945,  3.6548],\n",
      "        [-4.1411, -1.4654,  0.7253,  3.3505],\n",
      "        [-0.1886,  6.7877,  0.8461,  3.8859],\n",
      "        [-1.1607, -0.9435, -0.8499,  1.6444],\n",
      "        [ 1.2559,  1.2483,  0.3944, -0.8037],\n",
      "        [ 1.9975, -0.0336,  0.0459, -0.3742],\n",
      "        [-0.3025,  1.4703, -0.7248,  1.9832],\n",
      "        [-1.0773,  2.1747,  0.4766,  1.7719],\n",
      "        [ 0.2120,  2.6086, -0.5086,  0.7988],\n",
      "        [ 3.7268, -5.8038, -0.3481, -5.7029],\n",
      "        [-2.7119,  1.0946,  1.3409,  2.0723],\n",
      "        [-2.1023, -0.3917, -0.4127,  0.0301],\n",
      "        [-0.4296,  0.3289,  0.7286, -1.0426],\n",
      "        [-3.2960,  6.5339, -2.3002,  5.8347],\n",
      "        [-5.6204,  4.3677,  0.8589,  5.8331],\n",
      "        [-0.5383, -0.3735,  0.7899,  0.1988],\n",
      "        [-0.2584,  4.2540,  0.7209,  2.3532],\n",
      "        [ 1.1048, -1.9260, -0.0520, -1.5077],\n",
      "        [ 0.2729, -1.1649, -1.5107, -1.6883],\n",
      "        [-1.9696,  1.6195, -0.5714,  2.1540],\n",
      "        [ 0.8015,  2.0359,  0.0107,  1.7344],\n",
      "        [-0.1170,  0.0085, -1.6492, -0.2738]])\n"
     ]
    }
   ],
   "source": [
    "print(covariates@true_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(true_Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes anything since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        torch.manual_seed(0)\n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        self.C = torch.clone(true_C)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        self.beta = torch.clone(true_beta)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "    def single_likelihood_test(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        '''\n",
    "        computes the likelihood of the whole dataset. \n",
    "        '''\n",
    "        likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        for Y_b, covariates_b, O_b in self.get_batch(self.batch_size): \n",
    "            W = torch.randn(N_samples, self.batch_size, self.q)\n",
    "            likelihood +=  self.batch_likelihood(Y_b,covariates_b,O_b,W)\n",
    "        return likelihood\n",
    "            \n",
    "    def fit(self, N_iter, acc,lr_beta,lr_C,C_optim = False): \n",
    "        '''\n",
    "        fit the data. DOes not work yet. You can choose to optimize beta or C\n",
    "        '''\n",
    "        optim_beta = torch.optim.SGD([self.beta], lr = lr_beta)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = lr_C)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q) \n",
    "                if C_optim : \n",
    "                    optim_C.zero_grad()\n",
    "                    print('MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    grad_C = self.grad_batch_C(Y_b, covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    diff= torch.norm(grad_C+self.C.grad)/(torch.norm(grad_C)+torch.norm(self.C.grad))\n",
    "                    if diff <0  : \n",
    "                        print('mine', grad_C)\n",
    "                        print('true : ', self.C.grad)\n",
    "                    #self.C.grad =  -grad_C/torch.norm(grad_C)\n",
    "                    print('loss : ', loss.item())\n",
    "                    optim_C.step()\n",
    "                else : \n",
    "                    optim_beta.zero_grad()\n",
    "                    \n",
    "                    grad_beta = self.grad_batch_beta(Y_b,covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    self.beta.grad = -grad_beta/torch.mean(torch.abs(grad_beta))\n",
    "                    if loss <0 : \n",
    "                        print('MSE beta_before : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                        optim_beta.step()\n",
    "                        print('MSE beta_after : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    #print('beta : ',self.beta )\n",
    "            print('----------------------------------------------------------------------new_epoch')\n",
    "            print('likelihood : ', self.compute_likelihood(acc))\n",
    "\n",
    "    \n",
    "    def batch_likelihood(self, Y_b,covariates_b, O_b, W, mean = True): \n",
    "        norm_W = torch.sum(torch.norm(W,dim = 2)**2, axis = 1) \n",
    "        log_fact = torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b),axis = (1,2))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b),axis = (1,2))\n",
    "        \n",
    "        '''\n",
    "        result = (-1/2*norm_W -log_fact +exp_term+data_term) \n",
    "        print('Z_b +O_b : ', torch.sum(covariates_b@self.beta + W@(self.C.T)+O_b ,axis = (1,2)))\n",
    "        print('norm_W div  : ', -1/2*norm_W/result )\n",
    "        print('norm_W : ', -1/2*norm_W )\n",
    "        print('log_fact div : ', -log_fact/result)\n",
    "        print('log_fact', -log_fact)\n",
    "        print('expdiv : ', exp_term/result)\n",
    "        print('exo ', exp_term)\n",
    "        print('datadiv', data_term/result )\n",
    "        print('data',data_term)\n",
    "        print('res : ', result )\n",
    "        print('expres ! ', torch.exp(result))\n",
    "        '''\n",
    "        if mean : \n",
    "            return torch.mean(torch.exp(-1/2*norm_W -log_fact +exp_term+data_term))\n",
    "        else : \n",
    "            return torch.exp(-1/2*norm_W -log_fact +exp_term+data_term)\n",
    "    def grad_batch_beta(self, Y_b,covariates_b, O_b, W):\n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return torch.mean(likelihood.reshape(-1,1,1)*(covariates_b.unsqueeze(2).T@(-torch.exp(Z_b)+Y_b)), axis = 0) \n",
    "        \n",
    "    def grad_batch_C(self, Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return torch.mean(likelihood.reshape(-1,1,1)*((-torch.exp(Z_b)+Y_b).permute(0,2,1)@W), axis = 0) \n",
    "        \n",
    "    def fit_batch(self, Y_b,covariates_b, O_b, W): \n",
    "        optim = torch.optim.Rprop([self.beta, self.C], lr = 0.01)\n",
    "        optim.zero_grad()\n",
    "        loss = self.batch_likelihood(Y_b, covariates_b, O_b, W)\n",
    "        loss.backward()\n",
    "        print('loss : ', loss )\n",
    "        grad_C = self.grad_batch_C(Y_b, covariates_b, O_b, W)\n",
    "        true_grad = self.C.grad\n",
    "        diff = torch.norm(grad_C-true_grad)\n",
    "        print('diff : ', diff)\n",
    "        print('my_grad : ', grad_C)\n",
    "        print('true : ', true_grad)\n",
    "        \n",
    "    def fit_all(self,lr_beta,lr_C, N_iter,acc): \n",
    "        optim_beta = torch.optim.Rprop([self.beta], lr = lr_beta)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = lr_C)\n",
    "        for i in range(N_iter):\n",
    "            optim_beta.zero_grad()\n",
    "            optim_C.zero_grad()\n",
    "            loss = -self.compute_likelihood(acc)\n",
    "            loss.backward()\n",
    "            #print('grad : ', self.beta.grad)\n",
    "            optim_beta.step()\n",
    "            optim_C.step()\n",
    "            \n",
    "            print('---------------------MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "            print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "            print('likelihood--------------------------------------',self.compute_likelihood(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------MSE Sigma:  0.22480206825156315\n",
      "MSE beta :  0.010000000000000004\n",
      "likelihood-------------------------------------- tensor(3.2483e-05, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  0.6235860799384809\n",
      "MSE beta :  0.041680000000000016\n",
      "likelihood-------------------------------------- tensor(8.9407e-05, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  1.6706252907037646\n",
      "MSE beta :  0.09502990000000003\n",
      "likelihood-------------------------------------- tensor(0.0003, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  3.9533002743698087\n",
      "MSE beta :  0.177551932\n",
      "likelihood-------------------------------------- tensor(0.0006, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  8.95800835427072\n",
      "MSE beta :  0.29906296816\n",
      "likelihood-------------------------------------- tensor(0.0013, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  19.84899836040876\n",
      "MSE beta :  0.47901448990719986\n",
      "likelihood-------------------------------------- tensor(0.0029, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  43.103143665171054\n",
      "MSE beta :  0.7323350957977599\n",
      "likelihood-------------------------------------- tensor(0.0050, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  92.04961316234775\n",
      "MSE beta :  1.1402236561857662\n",
      "likelihood-------------------------------------- tensor(0.0065, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  194.10302680443576\n",
      "MSE beta :  1.7662248858642393\n",
      "likelihood-------------------------------------- tensor(0.0105, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  398.1863637103486\n",
      "MSE beta :  2.733157309063013\n",
      "likelihood-------------------------------------- tensor(0.0160, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  793.3437799459731\n",
      "MSE beta :  4.197267106976526\n",
      "likelihood-------------------------------------- tensor(0.0213, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  1443.0717499004325\n",
      "MSE beta :  6.390579358059121\n",
      "likelihood-------------------------------------- tensor(0.0221, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  2647.5139066556194\n",
      "MSE beta :  9.65094197012262\n",
      "likelihood-------------------------------------- tensor(0.0263, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  4372.993323449482\n",
      "MSE beta :  14.456899260201519\n",
      "likelihood-------------------------------------- tensor(0.0268, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  6352.972233635011\n",
      "MSE beta :  21.51885083638191\n",
      "likelihood-------------------------------------- tensor(0.0317, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  8655.197629721293\n",
      "MSE beta :  31.872245946520678\n",
      "likelihood-------------------------------------- tensor(0.0312, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  11074.289377828702\n",
      "MSE beta :  46.0657023935935\n",
      "likelihood-------------------------------------- tensor(0.0326, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  12733.318003832826\n",
      "MSE beta :  67.11218618813261\n",
      "likelihood-------------------------------------- tensor(0.0349, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  13366.174558121851\n",
      "MSE beta :  97.65377863312892\n",
      "likelihood-------------------------------------- tensor(0.0403, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  13122.591692198834\n",
      "MSE beta :  141.2975653259413\n",
      "likelihood-------------------------------------- tensor(0.0432, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  12000.942240985876\n",
      "MSE beta :  204.81954761178295\n",
      "likelihood-------------------------------------- tensor(0.0457, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  10699.292190926106\n",
      "MSE beta :  285.3367412041652\n",
      "likelihood-------------------------------------- tensor(0.0490, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  9334.562355846465\n",
      "MSE beta :  388.41535854155774\n",
      "likelihood-------------------------------------- tensor(0.0509, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  7868.050879101087\n",
      "MSE beta :  514.859653299967\n",
      "likelihood-------------------------------------- tensor(0.0605, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  6505.431145506621\n",
      "MSE beta :  656.5719884640146\n",
      "likelihood-------------------------------------- tensor(0.0696, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  5450.688025949394\n",
      "MSE beta :  863.1753336560869\n",
      "likelihood-------------------------------------- tensor(0.0704, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  4417.19067164678\n",
      "MSE beta :  1155.7599870946312\n",
      "likelihood-------------------------------------- tensor(0.0777, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  3451.133880927704\n",
      "MSE beta :  1570.4087980195834\n",
      "likelihood-------------------------------------- tensor(0.0770, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  2728.8784867453696\n",
      "MSE beta :  2089.1788192845447\n",
      "likelihood-------------------------------------- tensor(0.0771, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  2655.911143771415\n",
      "MSE beta :  2559.052577815063\n",
      "likelihood-------------------------------------- tensor(0.0755, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  2935.844355370253\n",
      "MSE beta :  3109.679673656593\n",
      "likelihood-------------------------------------- tensor(0.0766, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  3385.4259550370475\n",
      "MSE beta :  3880.650156036839\n",
      "likelihood-------------------------------------- tensor(0.0777, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  3792.4975669812497\n",
      "MSE beta :  5007.993695851558\n",
      "likelihood-------------------------------------- tensor(0.0779, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  4214.230356492889\n",
      "MSE beta :  6585.844934593813\n",
      "likelihood-------------------------------------- tensor(0.0770, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  4365.776092261636\n",
      "MSE beta :  8843.468898715311\n",
      "likelihood-------------------------------------- tensor(0.0780, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  4576.799266767055\n",
      "MSE beta :  11536.078754292565\n",
      "likelihood-------------------------------------- tensor(0.0754, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  5024.7546089101115\n",
      "MSE beta :  14714.560999680305\n",
      "likelihood-------------------------------------- tensor(0.0757, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  5325.6990568814435\n",
      "MSE beta :  18269.187540270603\n",
      "likelihood-------------------------------------- tensor(0.0775, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  5771.65263827119\n",
      "MSE beta :  21961.555494536613\n",
      "likelihood-------------------------------------- tensor(0.0775, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  5975.72350663471\n",
      "MSE beta :  26276.17470152972\n",
      "likelihood-------------------------------------- tensor(0.0776, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  6197.510942949611\n",
      "MSE beta :  31064.59512969971\n",
      "likelihood-------------------------------------- tensor(0.0776, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  6673.970749413916\n",
      "MSE beta :  36150.725179047964\n",
      "likelihood-------------------------------------- tensor(0.0757, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  7119.280690361459\n",
      "MSE beta :  36460.380111685925\n",
      "likelihood-------------------------------------- tensor(0.0783, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  7734.026891663198\n",
      "MSE beta :  37458.9956327942\n",
      "likelihood-------------------------------------- tensor(0.0771, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  8453.813852684787\n",
      "MSE beta :  38488.49245261967\n",
      "likelihood-------------------------------------- tensor(0.0782, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  8461.918077990156\n",
      "MSE beta :  38640.193084547005\n",
      "likelihood-------------------------------------- tensor(0.0766, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  8760.719945937988\n",
      "MSE beta :  38657.10216816815\n",
      "likelihood-------------------------------------- tensor(0.0787, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  9182.358356537294\n",
      "MSE beta :  38214.689802456385\n",
      "likelihood-------------------------------------- tensor(0.0760, grad_fn=<AddBackward0>)\n",
      "---------------------MSE Sigma:  9747.16759238285\n",
      "MSE beta :  38158.923987071525\n",
      "likelihood-------------------------------------- tensor(0.0790, grad_fn=<AddBackward0>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------MSE Sigma:  9743.91295210357\n",
      "MSE beta :  38811.480036268666\n",
      "likelihood-------------------------------------- tensor(0.0749, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "model.fit_all(0.1,0.1,N_iter = 50, acc = 0.001)\n",
    "#model.compute_likelihood(0.0001)\n",
    "#model.fit(10,0.001, lr_beta = 0.1,lr_C = 0.1, C_optim = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 53.1172,  54.9169,  57.1006,   5.8670,   9.7988,  55.8536, -19.7243,\n",
       "          54.5041,  55.5516,  73.7415],\n",
       "        [ -1.6841,  -1.8241,  -3.9329,  -1.9162,   2.1200,  -0.3344, -38.8216,\n",
       "          -2.2547,  -4.1625,  -0.9848],\n",
       "        [-24.1910, -23.9034,  -8.3193,   0.3848,   1.0350, -13.9846,   1.3624,\n",
       "         -34.1909, -23.0386,   0.5268],\n",
       "        [  9.6812,   9.9008,  13.0152,   1.7178,   1.3332,   8.4896,  -0.6718,\n",
       "           5.2278,   6.7412,  -0.1647]], requires_grad=True)"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.8136, -0.6513,  1.8937,  0.7632, -0.1027,  0.5357,  1.4522,  0.1101,\n",
       "          0.5326, -0.2131],\n",
       "        [-0.7202, -0.1027,  0.8985, -0.6883, -0.3802,  0.5361, -0.4484,  0.6421,\n",
       "         -1.4641, -1.2042],\n",
       "        [ 0.9864, -1.0280, -0.8013,  1.0045,  1.1537,  0.4313,  0.7548, -1.4239,\n",
       "          0.6443,  0.9832],\n",
       "        [-0.2799,  0.0106, -0.1753,  0.7863,  1.3945,  1.1535, -2.0487,  0.8947,\n",
       "         -0.6329, -0.2511]])"
      ]
     },
     "execution_count": 235,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "true_beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8855e-10, grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compute_likelihood(0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(true_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Z_b +O_b :  tensor([26.4781, 15.2091, 18.0693, 22.1403, 15.8713], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([4.0302e-05, 3.8500e-05, 2.6256e-05, 3.1458e-05, 1.7748e-05],\n",
      "       grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([-10.0561,  -9.8419,  -6.5526,  -7.9348,  -4.4420])\n",
      "log_fact div :  tensor([1.1244, 1.0975, 1.1242, 1.1123, 1.1210], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-280567.7702)\n",
      "expdiv :  tensor([0.0059, 0.0051, 0.0071, 0.0043, 0.0079], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-1469.6753, -1303.5968, -1776.8250, -1087.4623, -1977.7734],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.1304, -0.1027, -0.1314, -0.1167, -0.1289], grad_fn=<DivBackward0>)\n",
      "data tensor([32529.0306, 26245.2563, 32788.5509, 29429.4423, 32266.7483],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-249518.4710, -255635.9526, -249562.5969, -252233.7249, -250283.2373],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "Z_b +O_b :  tensor([26.4781, 15.2091, 18.0693, 22.1403, 15.8713], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([4.0302e-05, 3.8500e-05, 2.6256e-05, 3.1458e-05, 1.7748e-05],\n",
      "       grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([-10.0561,  -9.8419,  -6.5526,  -7.9348,  -4.4420])\n",
      "log_fact div :  tensor([1.1244, 1.0975, 1.1242, 1.1123, 1.1210], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-280567.7702)\n",
      "expdiv :  tensor([0.0059, 0.0051, 0.0071, 0.0043, 0.0079], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-1469.6753, -1303.5968, -1776.8250, -1087.4623, -1977.7734],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.1304, -0.1027, -0.1314, -0.1167, -0.1289], grad_fn=<DivBackward0>)\n",
      "data tensor([32529.0306, 26245.2563, 32788.5509, 29429.4423, 32266.7483],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-249518.4710, -255635.9526, -249562.5969, -252233.7249, -250283.2373],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Z_b +O_b :  tensor([22.0567, 16.9950, 23.0564, 19.9437, 19.8121], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([1.4352e-04, 1.0056e-04, 7.0687e-05, 1.4110e-04, 1.4452e-04],\n",
      "       grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([-10.0275,  -7.2141,  -5.1218, -10.6643, -10.1827])\n",
      "log_fact div :  tensor([1.9365, 1.8859, 1.8673, 1.7902, 1.9202], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-135295.9321)\n",
      "expdiv :  tensor([0.0053, 0.0044, 0.0031, 0.0055, 0.0062], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-370.0363, -317.5658, -226.3836, -416.1858, -439.7228],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.9419, -0.8904, -0.8704, -0.7958, -0.9266], grad_fn=<DivBackward0>)\n",
      "data tensor([65809.6335, 63878.9420, 63070.1580, 60145.5884, 65288.0700],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-69866.3624, -71741.7699, -72457.2795, -75577.1938, -70457.7675],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "Z_b +O_b :  tensor([22.0567, 16.9950, 23.0564, 19.9437, 19.8121], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([1.4352e-04, 1.0056e-04, 7.0687e-05, 1.4110e-04, 1.4452e-04],\n",
      "       grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([-10.0275,  -7.2141,  -5.1218, -10.6643, -10.1827])\n",
      "log_fact div :  tensor([1.9365, 1.8859, 1.8673, 1.7902, 1.9202], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-135295.9321)\n",
      "expdiv :  tensor([0.0053, 0.0044, 0.0031, 0.0055, 0.0062], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-370.0363, -317.5658, -226.3836, -416.1858, -439.7228],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.9419, -0.8904, -0.8704, -0.7958, -0.9266], grad_fn=<DivBackward0>)\n",
      "data tensor([65809.6335, 63878.9420, 63070.1580, 60145.5884, 65288.0700],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-69866.3624, -71741.7699, -72457.2795, -75577.1938, -70457.7675],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Z_b +O_b :  tensor([18.3088, 29.0724, 27.3818, 24.0572, 17.4314], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([0.0021, 0.0021, 0.0025, 0.0014, 0.0026], grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([ -8.8786,  -9.2739, -10.4660,  -5.9153, -11.4021])\n",
      "log_fact div :  tensor([0.9668, 0.9171, 0.9612, 0.9491, 0.9165], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-4013.9365)\n",
      "expdiv :  tensor([0.0513, 0.0602, 0.0451, 0.0418, 0.0266], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-212.8806, -263.3764, -188.4038, -176.7971, -116.6525],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.0202,  0.0206, -0.0088,  0.0077,  0.0542], grad_fn=<DivBackward0>)\n",
      "data tensor([  83.7463,  -89.9836,   36.8461,  -32.6175, -237.4090],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-4151.9494, -4376.5703, -4175.9602, -4229.2664, -4379.4000],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "Z_b +O_b :  tensor([18.3088, 29.0724, 27.3818, 24.0572, 17.4314], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([0.0021, 0.0021, 0.0025, 0.0014, 0.0026], grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([ -8.8786,  -9.2739, -10.4660,  -5.9153, -11.4021])\n",
      "log_fact div :  tensor([0.9668, 0.9171, 0.9612, 0.9491, 0.9165], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-4013.9365)\n",
      "expdiv :  tensor([0.0513, 0.0602, 0.0451, 0.0418, 0.0266], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-212.8806, -263.3764, -188.4038, -176.7971, -116.6525],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.0202,  0.0206, -0.0088,  0.0077,  0.0542], grad_fn=<DivBackward0>)\n",
      "data tensor([  83.7463,  -89.9836,   36.8461,  -32.6175, -237.4090],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-4151.9494, -4376.5703, -4175.9602, -4229.2664, -4379.4000],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Z_b +O_b :  tensor([59.2370, 54.5822, 67.2833, 64.5471, 58.1854], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([0.0010, 0.0006, 0.0004, 0.0002, 0.0015], grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([ -6.0154,  -6.2588,  -5.4888,  -6.4342, -12.1538])\n",
      "log_fact div :  tensor([0.3727, 0.2044, 0.1701, 0.0607, 0.2756], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-2185.5761)\n",
      "expdiv :  tensor([0.9385, 0.9508, 0.9787, 0.9877, 0.9505], grad_fn=<DivBackward0>)\n",
      "exo  tensor([ -5502.8256, -10164.3348, -12572.7219, -35572.7594,  -7537.6134],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.3123, -0.1559, -0.1492, -0.0486, -0.2276], grad_fn=<DivBackward0>)\n",
      "data tensor([1830.9092, 1666.0604, 1917.0696, 1749.0926, 1804.9881],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([ -5863.5078, -10690.1093, -12846.7171, -36015.6771,  -7930.3551],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "Z_b +O_b :  tensor([59.2370, 54.5822, 67.2833, 64.5471, 58.1854], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([0.0010, 0.0006, 0.0004, 0.0002, 0.0015], grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([ -6.0154,  -6.2588,  -5.4888,  -6.4342, -12.1538])\n",
      "log_fact div :  tensor([0.3727, 0.2044, 0.1701, 0.0607, 0.2756], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-2185.5761)\n",
      "expdiv :  tensor([0.9385, 0.9508, 0.9787, 0.9877, 0.9505], grad_fn=<DivBackward0>)\n",
      "exo  tensor([ -5502.8256, -10164.3348, -12572.7219, -35572.7594,  -7537.6134],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([-0.3123, -0.1559, -0.1492, -0.0486, -0.2276], grad_fn=<DivBackward0>)\n",
      "data tensor([1830.9092, 1666.0604, 1917.0696, 1749.0926, 1804.9881],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([ -5863.5078, -10690.1093, -12846.7171, -36015.6771,  -7930.3551],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n",
      "Z_b +O_b :  tensor([14.5640, 15.9653, 22.1511, 17.7549,  9.5899], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([0.0009, 0.0009, 0.0005, 0.0007, 0.0008], grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([-14.3995, -13.8630,  -7.6192,  -9.7919, -12.3467])\n",
      "log_fact div :  tensor([0.9252, 0.9296, 0.9898, 0.9794, 0.9831], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-14304.8879)\n",
      "expdiv :  tensor([0.0539, 0.0138, 0.0193, 0.0170, 0.0068], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-832.7172, -212.8955, -278.7721, -248.1552,  -98.2875],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([ 0.0200,  0.0556, -0.0096,  0.0030,  0.0093], grad_fn=<DivBackward0>)\n",
      "data tensor([-309.9973, -855.9373,  139.1327,  -43.5513, -134.6540],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-15462.0019, -15387.5838, -14452.1466, -14606.3864, -14550.1761],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "Z_b +O_b :  tensor([14.5640, 15.9653, 22.1511, 17.7549,  9.5899], grad_fn=<SumBackward1>)\n",
      "norm_W div  :  tensor([0.0009, 0.0009, 0.0005, 0.0007, 0.0008], grad_fn=<DivBackward0>)\n",
      "norm_W :  tensor([-14.3995, -13.8630,  -7.6192,  -9.7919, -12.3467])\n",
      "log_fact div :  tensor([0.9252, 0.9296, 0.9898, 0.9794, 0.9831], grad_fn=<DivBackward0>)\n",
      "log_fact tensor(-14304.8879)\n",
      "expdiv :  tensor([0.0539, 0.0138, 0.0193, 0.0170, 0.0068], grad_fn=<DivBackward0>)\n",
      "exo  tensor([-832.7172, -212.8955, -278.7721, -248.1552,  -98.2875],\n",
      "       grad_fn=<NegBackward>)\n",
      "datadiv tensor([ 0.0200,  0.0556, -0.0096,  0.0030,  0.0093], grad_fn=<DivBackward0>)\n",
      "data tensor([-309.9973, -855.9373,  139.1327,  -43.5513, -134.6540],\n",
      "       grad_fn=<SumBackward1>)\n",
      "res :  tensor([-15462.0019, -15387.5838, -14452.1466, -14606.3864, -14550.1761],\n",
      "       grad_fn=<AddBackward0>)\n",
      "expres !  tensor([0., 0., 0., 0., 0.], grad_fn=<ExpBackward>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.],\n",
      "        [0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = n//5\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "\n",
    "for Y_b, covariates_b, O_b in model.get_batch(model.batch_size): \n",
    "    W = torch.randn(5, model.batch_size,q)\n",
    "    model.fit_batch(Y_b, covariates_b, O_b,W)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(model.get_Sigma().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(true_Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''   def compute_single_log_like(self, i, acc):\n",
    "        N_iter = int(1/acc)\n",
    "        E = 0 \n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            E -= 1/2*SLA.norm(W)**2\n",
    "            E -= np.sum(np.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            E+= np.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            #print('E : ', E)\n",
    "        E/= N_iter\n",
    "        return E\n",
    "    \n",
    "    def batch_log_like(self,acc): \n",
    "        batch_E = 0\n",
    "        for i in range(10): \n",
    "            batch_E += self.compute_single_log_like(i,acc) \n",
    "        return batch_E\n",
    "    \n",
    "    def single_grad_beta_log_like(self,i, acc): \n",
    "        N_iter = int(1/acc)\n",
    "        grad = 0\n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(np.exp(self.O[i,:]+ self.covariates[i,:]@self.beta+self.C@W)).reshape(1,-1)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(self.Y[i,:].reshape(1,-1))\n",
    "        return grad/N_iter\n",
    "'''\n",
    "'''\n",
    "fonctions pour tester les gradients avec des W que l'on simule qu'une seule fois \n",
    "    def batch_grad_beta(self, acc): \n",
    "        batch_grad = 0\n",
    "        for i in range(10): \n",
    "            batch_grad += self.single_grad_beta_log_like(i,acc) \n",
    "        return batch_grad\n",
    "        \n",
    "        def single_likelihood(self,i,W): \n",
    "            ## W should be an array of size q \n",
    "            \n",
    "\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "\n",
    "        norm_W = TLA.norm(W)**2\n",
    "        log_fact = -torch.sum(log_stirling(Y_i))\n",
    "        Z_i = x_i@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_i +Z_i))\n",
    "        data_term = torch.sum(Y_i*(O_i+Z_i))\n",
    "        return torch.exp(log_fact + exp_term + data_term-1/2*norm_W)\n",
    "            \n",
    "    def single_grad_beta(self,i,W): \n",
    "        likeli = self.single_likelihood(i,W)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:].reshape(1,-1)\n",
    "        O_i = self.O[i,:]\n",
    "        exp = torch.exp(O_i + x_i@self.beta + W@(self.C.T))\n",
    "        return likeli*(x_i.T@(-exp+Y_i))\n",
    "    \n",
    "    def single_fit(self,i,W): \n",
    "        loss = self.single_likelihood(i,W)\n",
    "        loss.backward()\n",
    "        print('error : ', torch.norm(self.beta.grad-self.single_grad_beta(i,W)))\n",
    "    def batch_likelihood_test(self,Y_b,covariates_b, O_b, W): \n",
    "        norm_W = torch.sum(torch.norm(W, dim = 1)**2)\n",
    "        log_fact = torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b))\n",
    "        return torch.exp(-log_fact-norm_W/2 +exp_term + data_term)\n",
    "    \n",
    "    def batch_grad_beta_test(self,Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood_test(Y_b,covariates_b, O_b, W)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return likelihood*(covariates_b.T@(-torch.exp(Z_b)+Y_b))\n",
    "        \n",
    "   '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA_bis(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        #self.C = torch.clone(true_C)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def single_likelihood_test(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        W = torch.randn(N_samples, self.n, self.q)\n",
    "        likelihood +=  self.batch_likelihood(self.Y,self.covariates, self.O,W)\n",
    "        return likelihood/self.n\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def batch_grad_beta(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0\n",
    "        log_like = torch.sum(self.batch_likelihood(Y_b,covariates_b, O_b,W, somme = False), axis = 1)\n",
    "        first_term = -torch.exp(O_b +covariates_b@self.beta + W@(self.C.T))\n",
    "        second_term = Y_b\n",
    "        grad =  torch.sum(torch.multiply(log_like.reshape(-1,1,1),((covariates_b.T)@(first_term + second_term))), axis = 0)\n",
    "        # the for loop here does the same, just a sanity check\n",
    "        '''\n",
    "        grad = 0\n",
    "        for k in range(N_samples): \n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k])#/N_samples\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*(covariates_b.T)@(exp_term + Y_b)\n",
    "        '''\n",
    "        return grad/W.shape[0]\n",
    "    \n",
    "    \n",
    "    def batch_grad_C(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0 \n",
    "        for k in range(W.shape[0]):\n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k], somme = True)\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*((exp_term + 0*Y_b).T@W[k])\n",
    "        return grad/W.shape[0]\n",
    "            \n",
    "    def fit(self, N_iter, acc): \n",
    "        optim_beta = torch.optim.Rprop([self.beta], lr = 0.3)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = 0.3)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q) \n",
    "                if False : \n",
    "                    optim_C.zero_grad()\n",
    "                    #print('MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    grad_C = self.batch_grad_C(Y_b, covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    #self.C.grad =  -grad_C/torch.norm(grad_C)\n",
    "                    print('loss : ', loss.item())\n",
    "                    optim_C.step()\n",
    "                else : \n",
    "                    optim_beta.zero_grad()\n",
    "                    print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    grad_beta = self.batch_grad_beta(Y_b,covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    self.beta.grad = -grad_beta/torch.mean(torch.abs(grad_beta))\n",
    "                    optim_beta.step()\n",
    "            print('----------------------------------------------------------------------new_epoch')\n",
    "            \n",
    "            \n",
    "    def batch_likelihood(self,Y_batch,covariates_batch, O_batch, W, somme = True ): \n",
    "        '''\n",
    "        computes the approximation of the likelihood of a batch. \n",
    "        \n",
    "        args : \n",
    "                'Y_batch' : tensor of size(batch_size, p)\n",
    "                'covariates_batch' : tensor of size(batch_size, d)\n",
    "                'O_batch' : tensor of size(batch_size, p)\n",
    "                'acc' : float. the accuracy you want. The lower the accuracy, the lower the algorithm. \n",
    "                        we will sampThe size of tensor a (1000) must match the size of tensor b (20) at non-singleton dimension 2les 1/acc times. \n",
    "        returns : \n",
    "                the approximation of the likelihood. \n",
    "        ''' \n",
    "    \n",
    "        last_dim = len(W.shape)-1\n",
    "        if last_dim >1 : \n",
    "            N_samples = W.shape[0] # number of samples of W \n",
    "        else : N_samples = 1\n",
    "        #N_samples = W.shape[0]\n",
    "        Z = covariates_batch@self.beta + W@(self.C.T)\n",
    "        norm_W = TLA.norm(W, dim = last_dim)**2\n",
    "        log_fact =  torch.sum(log_stirling(Y_batch), axis = 1) # the factorial term \n",
    "        poiss_like =  - torch.sum(torch.exp(O_batch+Z), axis = last_dim) # first term of the poisson likelihood\n",
    "                                                                         #the normalising term with the exponential \n",
    "        poiss_like += torch.sum((O_batch+Z)*Y_batch, axis = last_dim)    # second term of the poisson likelihood\n",
    "        \n",
    "            \n",
    "        if somme : \n",
    "            # If we want the true likelihood\n",
    "            # We first take the exponential of the sum of the logs and then divide by the Number of samples we took.  \n",
    "            return torch.sum(torch.exp(-log_fact -1/2*norm_W+poiss_like))/N_samples \n",
    "        #for some purposes, we may want only the exponential and not the sum\n",
    "        else : return torch.exp(-log_fact -1/2*norm_W+poiss_like)/N_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE beta :  1.9102846328682492\n",
      "MSE beta :  1.8096150648046705\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  1.8552132061091728\n",
      "MSE beta :  1.9129760878867497\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.0701245395137264\n",
      "MSE beta :  2.3871577710512315\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.6920821053624366\n",
      "MSE beta :  3.339558541508027\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  4.266740013933548\n",
      "MSE beta :  5.829849607680941\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  8.225635687097924\n",
      "MSE beta :  9.993858093434689\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  13.342151645474365\n",
      "MSE beta :  14.396301048193923\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  17.317854779004286\n",
      "MSE beta :  19.84766625799734\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  23.838020380241492\n",
      "MSE beta :  24.570160465086182\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  27.37239572319345\n",
      "MSE beta :  28.576635872398175\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  29.592638528944445\n",
      "MSE beta :  30.05920858997783\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.101195695338298\n",
      "MSE beta :  31.85275115268177\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  32.91296705718656\n",
      "MSE beta :  34.02862881189437\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  34.21566166371409\n",
      "MSE beta :  33.66280883232819\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  33.16962321568695\n",
      "MSE beta :  32.483863490851355\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.87436034427889\n",
      "MSE beta :  31.822704369688143\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.48744476301318\n",
      "MSE beta :  31.168103517498643\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.860666924171813\n",
      "MSE beta :  30.878538073475863\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.748499069595027\n",
      "MSE beta :  30.507270106335973\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.25545053219896\n",
      "MSE beta :  30.294877734221195\n",
      "----------------------------------------------------------------------new_epoch\n"
     ]
    }
   ],
   "source": [
    "model_bis = MC_PLNPCA_bis(q,n//2) \n",
    "model_bis.init_data(Y_sampled, O,covariates )\n",
    "model_bis.fit(20,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
