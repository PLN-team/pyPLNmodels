{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import seaborn as sns \n",
    "import torch \n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\mathbf{x}_{i}\\beta +W_{i}\\mathbf{C}^{\\top}, \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the log likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta} P_{\\theta}(Y)$$\n",
    "\n",
    "But we need to integrate out $W$ in order to compute the quantity inside the max : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i}\\right) &=\\int P_{\\theta}\\left(Y_{i}, W\\right) d W \\\\\n",
    "&=\\int P_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This integral being untractable, we are going to approximate it with Monte Carlo methods : \n",
    "\n",
    "$$\n",
    "\\int p_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W \\approx \\frac{1}{K} \\sum_{k = 1 }^Kp_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)\n",
    "$$\n",
    "$$W_{i,k} \\sim \\mathcal N (0, I_q)$$\n",
    "\n",
    "The larger the $K$ the better the approximation.  \n",
    "\n",
    "Let's compute $p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)$. \n",
    "\n",
    "\n",
    "First, \n",
    "\n",
    "$$\n",
    "P\\left(W_{i,k}\\right)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2}\\left\\|W_{i,k}\\right\\|_{2}^{2}\\right)\n",
    "$$ \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical purposes, we may want to use a logarithmic scale and apply the exponential function after. Indeed, $Y_{ij}$ can go up to a thousand, and computing this factorial would give infinite values. \n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=\\sum_{j=1}^{p} - \\ln \\left(Y_{i j} !\\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+Z_{i j}\\right)\n",
    "$$\n",
    "\n",
    "We are going tu use the Stirling Formula in order to compute the log of the factorial, to avoid computing directly the factorial.  \n",
    "\n",
    "We now need to compute the gradients. Since \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i}| W_{i,k}\\right) \\nabla_{\\theta} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\n",
    "$$\n",
    "\n",
    "We get : \n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-x_{i}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ x_{i}^{\\top}Y_i\\right]\n",
    "$$\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right)\\left[-W_{i,k}^{\\top} \\exp \\left(0_{i}+x_{i}^{\\top} \\beta+W_{i,k}^{\\top} C^{\\top}\\right)+ W_{i,k}^{\\top}Y_i\\right]\n",
    "$$\n",
    "This is if we take only one sample $Y_i$. If we take the whole dataset (or a mini-batch), we get (writed in matrix form) :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X^{\\top} \\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-W_k^{\\top} \\exp \\left(0+X^{\\top} \\beta+W_k^{\\top} C^{\\top}\\right)+ W_k^{\\top}Y\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$C :  (p,q)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be smaller (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*0.95**np.arange(block_size)\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.98**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    w,v = SLA.eigh(Sigma)\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:]))\n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 50;  p = 10\n",
    "q = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.manual_seed(0)\n",
    "true_Sigma = torch.from_numpy(build_block_Sigma(p,12))\n",
    "true_C = torch.from_numpy(C_from_Sigma(true_Sigma, q))\n",
    "true_beta =torch.randn((d, p))\n",
    "\n",
    "covariates = torch.randn((n,d))\n",
    "O =  1+torch.zeros((n,p))\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled, Z_sampled  = sample_model.sample(true_Sigma,true_beta, O, covariates)\n",
    "Y_sampled = torch.from_numpy(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes anything since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        #self.C = torch.clone(true_C)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def single_likelihood_test(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        '''\n",
    "        computes the likelihood of the whole dataset. \n",
    "        '''\n",
    "        likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        for Y_b, covariates_b, O_b in self.get_batch(self.batch_size): \n",
    "            W = torch.randn(N_samples, self.n, self.q)\n",
    "            likelihood +=  self.batch_likelihood(Y_b,covariates_b,O_b,W)\n",
    "        return likelihood\n",
    "            \n",
    "    def fit(self, N_iter, acc,C_optim = False): \n",
    "        '''\n",
    "        fit the data. DOes not work yet. You can choose to optimize beta or C\n",
    "        '''\n",
    "        optim_beta = torch.optim.SGD([self.beta], lr = 0.008)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = 0.008)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q) \n",
    "                if C_optim : \n",
    "                    optim_C.zero_grad()\n",
    "                    print('MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    grad_C = self.grad_batch_C(Y_b, covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    diff= torch.norm(grad_C+self.C.grad)/(torch.norm(grad_C)+torch.norm(self.C.grad))\n",
    "                    if diff <0  : \n",
    "                        print('mine', grad_C)\n",
    "                        print('true : ', self.C.grad)\n",
    "                    #self.C.grad =  -grad_C/torch.norm(grad_C)\n",
    "                    print('loss : ', loss.item())\n",
    "                    optim_C.step()\n",
    "                else : \n",
    "                    optim_beta.zero_grad()\n",
    "                    print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    grad_beta = self.grad_batch_beta(Y_b,covariates_b, O_b,W)\n",
    "                    #print('grad_beta : ', grad_beta)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    print('loss ', loss.item())\n",
    "                    #print('mine :', grad_beta)\n",
    "                    #print('true : ', self.beta.grad)\n",
    "                    #self.beta.grad = -grad_beta/torch.mean(torch.abs(grad_beta))\n",
    "                    optim_beta.step()\n",
    "                    #print('beta : ',self.beta )\n",
    "            print('----------------------------------------------------------------------new_epoch')\n",
    "            print('likelihood : ', self.compute_likelihood(acc))\n",
    "\n",
    "    \n",
    "    def batch_likelihood(self, Y_b,covariates_b, O_b, W, mean = True): \n",
    "        norm_W = torch.sum(torch.norm(W,dim = 2)**2, axis = 1) \n",
    "        log_fact = torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b),axis = (1,2))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b),axis = (1,2))\n",
    "        result = (-1/2*norm_W -0*log_fact +exp_term+data_term) \n",
    "        '''\n",
    "        print('norm_W div  : ', -1/2*norm_W/result )\n",
    "        print('norm_W : ', -1/2*norm_W )\n",
    "        print('log_fact div : ', -log_fact/result)\n",
    "        print('log_fact', -log_fact)\n",
    "        print('expdiv : ', exp_term/result)\n",
    "        print('exo ', exp_term)\n",
    "        print('data', data_term/result )\n",
    "        print('data',data_term)\n",
    "        print('res : ', result )\n",
    "        '''\n",
    "        if mean : \n",
    "            return torch.mean(torch.exp(-1/2*norm_W -log_fact +exp_term+data_term))\n",
    "        else : \n",
    "            return torch.exp(-1/2*norm_W -log_fact +exp_term+data_term)\n",
    "    def grad_batch_beta(self, Y_b,covariates_b, O_b, W):\n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return torch.mean(likelihood.reshape(-1,1,1)*(covariates_b.unsqueeze(2).T@(-torch.exp(Z_b)+Y_b)), axis = 0) \n",
    "        \n",
    "    def grad_batch_C(self, Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood(Y_b,covariates_b, O_b, W, mean = False)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return torch.mean(likelihood.reshape(-1,1,1)*((-torch.exp(Z_b)+Y_b).permute(0,2,1)@W), axis = 0) \n",
    "        \n",
    "    def fit_batch(self, Y_b,covariates_b, O_b, W): \n",
    "        optim = torch.optim.Rprop([self.beta, self.C], lr = 0.01)\n",
    "        optim.zero_grad()\n",
    "        loss = self.batch_likelihood(Y_b, covariates_b, O_b, W)\n",
    "        loss.backward()\n",
    "        print('loss : ', loss )\n",
    "        grad_C = self.grad_batch_C(Y_b, covariates_b, O_b, W)\n",
    "        true_grad = self.C.grad\n",
    "        diff = torch.norm(grad_C-true_grad)\n",
    "        print('diff : ', diff)\n",
    "        print('my_grad : ', grad_C)\n",
    "        print('true : ', true_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n",
      "loss :  tensor(0., grad_fn=<MeanBackward0>)\n",
      "diff :  tensor(0., grad_fn=<CopyBackwards>)\n",
      "my_grad :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]], grad_fn=<MeanBackward1>)\n",
      "true :  tensor([[0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "batch_size = n//5\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "#model.single_fit(0, )\n",
    "#for Y_b, covariates_b, O_b in model.get_batch(model.batch_size):\n",
    "#    model.batch_grad_C(Y_b, covariates_b, O_b,0.01)\n",
    "#%time model.fit(10,0.01)\n",
    "a = 0 \n",
    "for Y_b, covariates_b, O_b in model.get_batch(model.batch_size): \n",
    "    W = torch.randn(20, model.batch_size,q)\n",
    "    model.fit_batch(Y_b, covariates_b, O_b,W)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i,k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{i,k}^{\\top} C^{\\top} _{j}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE beta :  2.178603606843364\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606843364\n",
      "loss  -1.2286840788301581e-125\n",
      "MSE beta :  2.178603606843364\n",
      "loss  -3.5677822278337966e-291\n",
      "MSE beta :  2.178603606843364\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606843364\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606843364\n",
      "loss  -1.1360148530702182e-216\n",
      "MSE beta :  2.178603606843364\n",
      "loss  -1.5218688044033887e-12\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -1.0943937292820952e-65\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -8.303847994079184e-185\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -1.1386125233925054e-36\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -5.00710769668713e-41\n",
      "MSE beta :  2.178603606843334\n",
      "loss  -1.037750406843424e-08\n",
      "MSE beta :  2.1786036067979793\n",
      "loss  -1.7032147395730424e-104\n",
      "MSE beta :  2.1786036067979793\n",
      "loss  -4.6824866876790114e-20\n",
      "MSE beta :  2.1786036067979793\n",
      "loss  -4.508062152721695e-16\n",
      "MSE beta :  2.1786036067979793\n",
      "loss  -5.959218910055874e-53\n",
      "MSE beta :  2.1786036067979793\n",
      "loss  -2.532182760264736e-178\n",
      "MSE beta :  2.1786036067979793\n",
      "loss  -6.11630787438521e-12\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -2.5729860160753554e-45\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -3.216689925095489e-117\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -7.608764521627987e-80\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -1.909010076604876e-24\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -2.0619660822792126e-33\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -7.039987337593593e-78\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -8.791771420216908e-264\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -3.823655344427442e-29\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -2.48125570116297e-17\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -3.1860197126270005e-52\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -1.1584782520784104e-157\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -3.77147555700479e-27\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -2.896006703166648e-101\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -1.956547392959441e-63\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -4.718218991597362e-139\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -1.3168972871700017e-44\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -1.438779301752191e-123\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -7.7003378e-316\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -3.6804444528777867e-209\n",
      "MSE beta :  2.1786036067979566\n",
      "loss  -1.4455798162017757e-12\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -1.6949903142152716e-65\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -5.125684284477349e-180\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -3.744045639247942e-36\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -3.695991099271549e-41\n",
      "MSE beta :  2.178603606797929\n",
      "loss  -5.001903531321722e-09\n",
      "MSE beta :  2.1786036067766377\n",
      "loss  -4.5274706153182275e-105\n",
      "MSE beta :  2.1786036067766377\n",
      "loss  -8.970365260842173e-20\n",
      "MSE beta :  2.1786036067766377\n",
      "loss  -1.867552806899499e-15\n",
      "MSE beta :  2.1786036067766377\n",
      "loss  -7.813197510528328e-54\n",
      "MSE beta :  2.1786036067766377\n",
      "loss  -2.5448453149737723e-179\n",
      "MSE beta :  2.1786036067766377\n",
      "loss  -6.006265990962599e-12\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -8.636374893283807e-45\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -6.123479491398866e-119\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -8.535451653588165e-78\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -2.4215378873647654e-24\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -3.617893251443318e-33\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -8.710185843542265e-78\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -3.3679614771242555e-271\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -1.7796175944286336e-29\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -3.431973884677735e-17\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -6.945521345572394e-50\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -4.459606681567224e-280\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -1.6300404959656117e-166\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -2.630568028382053e-29\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -3.5234082231500175e-104\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -6.015465268700113e-68\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -9.064028182210729e-132\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -3.475901152798429e-41\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -7.878261915273773e-127\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -6.0695374456573035e-220\n",
      "MSE beta :  2.1786036067766155\n",
      "loss  -1.1762518376788962e-12\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -1.5162163712388682e-66\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -7.511903838706828e-185\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -6.98478516064071e-36\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -1.8005817658977431e-41\n",
      "MSE beta :  2.1786036067765924\n",
      "loss  -4.163825951096889e-09\n",
      "MSE beta :  2.1786036067569157\n",
      "loss  -6.422015938298241e-108\n",
      "MSE beta :  2.1786036067569157\n",
      "loss  -3.507750343188698e-20\n",
      "MSE beta :  2.1786036067569157\n",
      "loss  -4.313066765893523e-16\n",
      "MSE beta :  2.1786036067569157\n",
      "loss  -1.3288051598862521e-51\n",
      "MSE beta :  2.1786036067569157\n",
      "loss  -6.199854563702612e-174\n",
      "MSE beta :  2.1786036067569157\n",
      "loss  -6.459204064562979e-12\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -3.9065869277773244e-45\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -3.5919243722927047e-120\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -1.1124787509862358e-75\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -3.0921209291698197e-24\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -4.4371833245252905e-34\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -2.357454400594079e-78\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -1.8700837475728374e-267\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -2.840995978370593e-29\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -6.4427268813796354e-18\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -3.130562167823215e-57\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -7.809235604339264e-168\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -4.59186402312141e-29\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -1.2978119459693544e-97\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -1.2746178376734212e-64\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -3.889893284423618e-123\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -2.3441141822971184e-49\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -5.458462201482291e-127\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -1.871932618662743e-214\n",
      "MSE beta :  2.1786036067568917\n",
      "loss  -1.3435778048416852e-12\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -4.0054780860126983e-66\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -3.628840049566253e-181\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -1.3420723127566061e-36\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -4.2574505828147935e-41\n",
      "MSE beta :  2.178603606756864\n",
      "loss  -3.673225058518101e-09\n",
      "MSE beta :  2.1786036067416883\n",
      "loss  -6.3004366340557786e-114\n",
      "MSE beta :  2.1786036067416883\n",
      "loss  -1.0035105998309178e-19\n",
      "MSE beta :  2.1786036067416883\n",
      "loss  -1.267030019980188e-16\n",
      "MSE beta :  2.1786036067416883\n",
      "loss  -3.2638019421346525e-52\n",
      "MSE beta :  2.1786036067416883\n",
      "loss  -4.067632526938617e-177\n",
      "MSE beta :  2.1786036067416883\n",
      "loss  -6.652752855332176e-12\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -6.760838610227389e-47\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -2.2865041878663987e-115\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -3.263530575345015e-73\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -2.6201510596488204e-24\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -2.452090085113905e-33\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -6.123121351821671e-78\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -4.1596914215664535e-271\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -9.974357066018339e-30\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -2.829028998245922e-17\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -4.5720760302723884e-55\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -1.607475983848239e-176\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -7.487625654659869e-27\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -6.765356248159336e-101\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -5.178094452062319e-64\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -3.551973108818932e-130\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -1.2427790211298824e-43\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -3.085268491604836e-126\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -1.030136267392603e-309\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -3.4814758845318615e-224\n",
      "MSE beta :  2.1786036067416665\n",
      "loss  -1.6786358368024784e-12\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -6.7294060605483264e-68\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -1.56413654477436e-174\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -5.3650452325756945e-36\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -1.0643638816245458e-41\n",
      "MSE beta :  2.178603606741633\n",
      "loss  -5.948198759961657e-09\n",
      "MSE beta :  2.1786036067155767\n",
      "loss  -1.1052461497366169e-94\n",
      "MSE beta :  2.1786036067155767\n",
      "loss  -5.537285921876907e-20\n",
      "MSE beta :  2.1786036067155767\n",
      "loss  -4.1054747658968296e-16\n",
      "MSE beta :  2.1786036067155767\n",
      "loss  -3.71257103963809e-53\n",
      "MSE beta :  2.1786036067155767\n",
      "loss  -1.080518589738762e-174\n",
      "MSE beta :  2.1786036067155767\n",
      "loss  -5.576158120841662e-12\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -8.396652582491678e-46\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.2206784898518897e-113\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.1598789215490863e-76\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -2.2174475941961402e-24\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -3.0594054417214408e-33\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -7.344642486816452e-77\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -8.80999031369311e-267\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -4.649025402199404e-29\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.2909283593235732e-16\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -8.271239207646562e-54\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -4.650912409555145e-157\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -4.952703543510526e-30\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.0209545042225417e-100\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -2.6712951208394147e-67\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.0237076221112892e-140\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.615781213388882e-48\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.175481373317149e-124\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -3.6e-322\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -3.3790216588493505e-217\n",
      "MSE beta :  2.1786036067155585\n",
      "loss  -1.7796421454129023e-12\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -1.93850496552094e-67\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -2.61320222943353e-178\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -9.374498501006452e-37\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -3.4646263382221424e-41\n",
      "MSE beta :  2.1786036067155234\n",
      "loss  -5.023477797521687e-09\n",
      "MSE beta :  2.1786036066942955\n",
      "loss  -2.0127464446203808e-103\n",
      "MSE beta :  2.1786036066942955\n",
      "loss  -4.4555291700438106e-20\n",
      "MSE beta :  2.1786036066942955\n",
      "loss  -1.1349335278791735e-15\n",
      "MSE beta :  2.1786036066942955\n",
      "loss  -2.4073428544257294e-53\n",
      "MSE beta :  2.1786036066942955\n",
      "loss  -1.029048444813214e-178\n",
      "MSE beta :  2.1786036066942955\n",
      "loss  -7.78951173307809e-12\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -6.158312027340395e-46\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -3.653571683859681e-117\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -1.0760457790659677e-80\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -1.218640309889e-311\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -3.2783020523237216e-24\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -5.5718806004185225e-34\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -4.735845866537592e-78\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -9.867062729669257e-264\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -2.5980750096440517e-29\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -2.4975045590736645e-18\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -1.563534775800083e-56\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -6.255138277052883e-159\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -5.4560502691252374e-27\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -2.087954963549213e-97\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -3.4573423126742843e-65\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -2.5478198094631723e-128\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -4.21851357221297e-48\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -6.404991548392849e-125\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -1.1991440379946637e-301\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -7.417964500361263e-222\n",
      "MSE beta :  2.1786036066942693\n",
      "loss  -1.6051460958112731e-12\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -1.6828950843350122e-65\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -5.938309347070177e-184\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -4.4084242664340806e-36\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -1.6023966915801566e-41\n",
      "MSE beta :  2.1786036066942365\n",
      "loss  -9.085210340664868e-09\n",
      "MSE beta :  2.1786036066601127\n",
      "loss  -1.0212351420247408e-102\n",
      "MSE beta :  2.1786036066601127\n",
      "loss  -8.034315930847572e-20\n",
      "MSE beta :  2.1786036066601127\n",
      "loss  -2.4789484288869158e-16\n",
      "MSE beta :  2.1786036066601127\n",
      "loss  -9.234781028251157e-53\n",
      "MSE beta :  2.1786036066601127\n",
      "loss  -8.712776141707756e-176\n",
      "MSE beta :  2.1786036066601127\n",
      "loss  -6.098998029706405e-12\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -7.368836935324773e-46\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -4.003932431685251e-112\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -9.749712133073768e-83\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -2.0801373458656368e-297\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -3.53040850985917e-24\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -8.37617158867079e-34\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -8.886155425083626e-78\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -1.7602247266368135e-267\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -6.418251665806641e-29\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -1.302270474583388e-17\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -3.891212397733404e-52\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -2.4138932894422775e-172\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -8.343880209261805e-29\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -3.566496799478278e-103\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -9.037005653116218e-67\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -1.691304400695955e-122\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -5.801511191501046e-43\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -8.182728747757487e-124\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -6.997485762591526e-303\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -9.185543864449904e-212\n",
      "MSE beta :  2.178603606660091\n",
      "loss  -2.2982080871327292e-12\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -2.8230062737439367e-66\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -5.476458949776537e-186\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -3.7489974101740665e-36\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -6.702523612813723e-41\n",
      "MSE beta :  2.1786036066600456\n",
      "loss  -5.954836726790411e-09\n",
      "MSE beta :  2.1786036066340317\n",
      "loss  -3.554043180138305e-105\n",
      "MSE beta :  2.1786036066340317\n",
      "loss  -8.900415703026383e-20\n",
      "MSE beta :  2.1786036066340317\n",
      "loss  -4.454615499072435e-16\n",
      "MSE beta :  2.1786036066340317\n",
      "loss  -2.8498949007818185e-52\n",
      "MSE beta :  2.1786036066340317\n",
      "loss  -5.515477073565185e-175\n",
      "MSE beta :  2.1786036066340317\n",
      "loss  -4.316465140087008e-12\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -4.9576459971102134e-45\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -4.364683710232675e-114\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -3.54044090418903e-77\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -2.5122873483151874e-24\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.1496861324514611e-33\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -3.391887233164268e-77\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -7.193193262939412e-262\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -2.3082297794091298e-29\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.5598123915444233e-16\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -8.367005907570867e-52\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.2582537250252613e-169\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.4658463501843674e-28\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -2.2167083811291883e-107\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -5.062858274922654e-66\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.2025166403895656e-144\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.1959575715153903e-56\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -7.877757423856231e-124\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.445e-320\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.2551965542353989e-222\n",
      "MSE beta :  2.178603606634018\n",
      "loss  -1.5560444541596062e-12\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -1.9314655339244175e-67\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -1.0396877850198588e-169\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -4.959045587818613e-36\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -2.501069039690942e-41\n",
      "MSE beta :  2.1786036066339856\n",
      "loss  -3.975641104611704e-09\n",
      "MSE beta :  2.17860360661859\n",
      "loss  -9.38022540057953e-102\n",
      "MSE beta :  2.17860360661859\n",
      "loss  -3.945432511380607e-20\n",
      "MSE beta :  2.17860360661859\n",
      "loss  -7.741689855132302e-16\n",
      "MSE beta :  2.17860360661859\n",
      "loss  -6.41010963532303e-53\n",
      "MSE beta :  2.17860360661859\n",
      "loss  -2.2716953338464524e-176\n",
      "MSE beta :  2.17860360661859\n",
      "loss  -9.553433044294276e-12\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -1.1295689338865106e-45\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -1.5696871451612788e-112\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -7.463072581497604e-82\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -2.423919644379252e-24\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -1.2378471240112641e-33\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -6.532027870086754e-78\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -3.439595745872026e-266\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -1.8864061079998062e-29\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -1.6314916087020406e-17\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -6.795541757342919e-54\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -8.98874300631163e-168\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -1.562784844130627e-27\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -2.510873831404852e-102\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -9.198002766007608e-70\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -3.9991818563499265e-123\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss  -3.955345823852766e-43\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -5.300248008642762e-123\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -7.54040250209876e-299\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -6.16133271130358e-221\n",
      "MSE beta :  2.1786036066185583\n",
      "loss  -1.181396997496997e-12\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -1.1605222545350768e-67\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -0.0\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -6.476237049038388e-187\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -3.8762406283889594e-36\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -2.825551726754349e-41\n",
      "MSE beta :  2.1786036066185335\n",
      "loss  -6.3630233478965265e-09\n",
      "MSE beta :  2.178603606594526\n",
      "loss  -6.125141359750127e-107\n",
      "MSE beta :  2.178603606594526\n",
      "loss  -4.346363980688227e-20\n",
      "MSE beta :  2.178603606594526\n",
      "loss  -5.374736979609556e-16\n",
      "MSE beta :  2.178603606594526\n",
      "loss  -1.5592923335481196e-52\n",
      "MSE beta :  2.178603606594526\n",
      "loss  -5.074385727680688e-179\n",
      "MSE beta :  2.178603606594526\n",
      "loss  -5.272250199131114e-12\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -6.025854359556392e-46\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -3.278977113962581e-116\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -1.7885706652907774e-76\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -6.6e-322\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -3.4016170401628145e-24\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -1.2483621090144101e-33\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -5.38715894662645e-77\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -6.192934240854133e-267\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -2.53497279716985e-29\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -6.985223873133663e-17\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -4.947885573652709e-54\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -2.593507791960399e-168\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -1.8340536924944804e-29\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -1.7440513802953401e-102\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -6.008387589505226e-68\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -1.8284135520580346e-126\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -3.708597320378808e-40\n",
      "MSE beta :  2.178603606594509\n",
      "loss  -0.0\n",
      "----------------------------------------------------------------------new_epoch\n",
      "likelihood :  tensor(inf, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "model = MC_PLNPCA(q,batch_size)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "model.fit(10,0.001, C_optim = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(model.get_Sigma().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 275,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.heatmap(true_Sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''   def compute_single_log_like(self, i, acc):\n",
    "        N_iter = int(1/acc)\n",
    "        E = 0 \n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            E -= 1/2*SLA.norm(W)**2\n",
    "            E -= np.sum(np.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            E+= np.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            #print('E : ', E)\n",
    "        E/= N_iter\n",
    "        return E\n",
    "    \n",
    "    def batch_log_like(self,acc): \n",
    "        batch_E = 0\n",
    "        for i in range(10): \n",
    "            batch_E += self.compute_single_log_like(i,acc) \n",
    "        return batch_E\n",
    "    \n",
    "    def single_grad_beta_log_like(self,i, acc): \n",
    "        N_iter = int(1/acc)\n",
    "        grad = 0\n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(np.exp(self.O[i,:]+ self.covariates[i,:]@self.beta+self.C@W)).reshape(1,-1)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(self.Y[i,:].reshape(1,-1))\n",
    "        return grad/N_iter\n",
    "'''\n",
    "'''\n",
    "fonctions pour tester les gradients avec des W que l'on simule qu'une seule fois \n",
    "    def batch_grad_beta(self, acc): \n",
    "        batch_grad = 0\n",
    "        for i in range(10): \n",
    "            batch_grad += self.single_grad_beta_log_like(i,acc) \n",
    "        return batch_grad\n",
    "        \n",
    "        def single_likelihood(self,i,W): \n",
    "            ## W should be an array of size q \n",
    "            \n",
    "\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "\n",
    "        norm_W = TLA.norm(W)**2\n",
    "        log_fact = -torch.sum(log_stirling(Y_i))\n",
    "        Z_i = x_i@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_i +Z_i))\n",
    "        data_term = torch.sum(Y_i*(O_i+Z_i))\n",
    "        return torch.exp(log_fact + exp_term + data_term-1/2*norm_W)\n",
    "            \n",
    "    def single_grad_beta(self,i,W): \n",
    "        likeli = self.single_likelihood(i,W)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:].reshape(1,-1)\n",
    "        O_i = self.O[i,:]\n",
    "        exp = torch.exp(O_i + x_i@self.beta + W@(self.C.T))\n",
    "        return likeli*(x_i.T@(-exp+Y_i))\n",
    "    \n",
    "    def single_fit(self,i,W): \n",
    "        loss = self.single_likelihood(i,W)\n",
    "        loss.backward()\n",
    "        print('error : ', torch.norm(self.beta.grad-self.single_grad_beta(i,W)))\n",
    "    def batch_likelihood_test(self,Y_b,covariates_b, O_b, W): \n",
    "        norm_W = torch.sum(torch.norm(W, dim = 1)**2)\n",
    "        log_fact = torch.sum(log_stirling(Y_b))\n",
    "        Z_b = covariates_b@self.beta + W@(self.C.T)\n",
    "        exp_term = -torch.sum(torch.exp(O_b+Z_b))\n",
    "        data_term = torch.sum(Y_b * (O_b + Z_b))\n",
    "        return torch.exp(-log_fact-norm_W/2 +exp_term + data_term)\n",
    "    \n",
    "    def batch_grad_beta_test(self,Y_b,covariates_b, O_b, W): \n",
    "        likelihood =  self.batch_likelihood_test(Y_b,covariates_b, O_b, W)\n",
    "        Z_b = O_b + covariates_b@self.beta + W@(self.C.T)\n",
    "        return likelihood*(covariates_b.T@(-torch.exp(Z_b)+Y_b))\n",
    "        \n",
    "   '''\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA_bis(): \n",
    "    \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        #self.C = torch.clone(true_C)\n",
    "        self.C.requires_grad_(True)\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        self.beta.requires_grad_(True)\n",
    "        \n",
    "        \n",
    "    \n",
    "    def single_likelihood_test(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc): \n",
    "        likelihood = 0\n",
    "        N_samples = int(1/acc)\n",
    "        W = torch.randn(N_samples, self.n, self.q)\n",
    "        likelihood +=  self.batch_likelihood(self.Y,self.covariates, self.O,W)\n",
    "        return likelihood/self.n\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def batch_grad_beta(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0\n",
    "        log_like = torch.sum(self.batch_likelihood(Y_b,covariates_b, O_b,W, somme = False), axis = 1)\n",
    "        first_term = -torch.exp(O_b +covariates_b@self.beta + W@(self.C.T))\n",
    "        second_term = Y_b\n",
    "        grad =  torch.sum(torch.multiply(log_like.reshape(-1,1,1),((covariates_b.T)@(first_term + second_term))), axis = 0)\n",
    "        # the for loop here does the same, just a sanity check\n",
    "        '''\n",
    "        grad = 0\n",
    "        for k in range(N_samples): \n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k])#/N_samples\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*(covariates_b.T)@(exp_term + Y_b)\n",
    "        '''\n",
    "        return grad/W.shape[0]\n",
    "    \n",
    "    \n",
    "    def batch_grad_C(self,Y_b,covariates_b, O_b,W): \n",
    "        grad = 0 \n",
    "        for k in range(W.shape[0]):\n",
    "            log_like = self.batch_likelihood(Y_b,covariates_b, O_b,W[k], somme = True)\n",
    "            exp_term  = -torch.exp(O_b +covariates_b@self.beta + W[k]@(self.C.T))\n",
    "            grad +=  log_like*((exp_term + 0*Y_b).T@W[k])\n",
    "        return grad/W.shape[0]\n",
    "            \n",
    "    def fit(self, N_iter, acc): \n",
    "        optim_beta = torch.optim.Rprop([self.beta], lr = 0.3)\n",
    "        optim_C = torch.optim.Rprop([self.C], lr = 0.3)\n",
    "        N_samples = int(1/acc)\n",
    "        for i in range(N_iter):\n",
    "            for Y_b, covariates_b, O_b in self.get_batch(self.batch_size):\n",
    "                W = torch.randn(N_samples,self.batch_size, self.q) \n",
    "                if False : \n",
    "                    optim_C.zero_grad()\n",
    "                    #print('MSE Sigma: ', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "                    grad_C = self.batch_grad_C(Y_b, covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    #self.C.grad =  -grad_C/torch.norm(grad_C)\n",
    "                    print('loss : ', loss.item())\n",
    "                    optim_C.step()\n",
    "                else : \n",
    "                    optim_beta.zero_grad()\n",
    "                    print('MSE beta : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                    grad_beta = self.batch_grad_beta(Y_b,covariates_b, O_b,W)\n",
    "                    loss = -self.batch_likelihood(Y_b,covariates_b, O_b,W)\n",
    "                    loss.backward()\n",
    "                    self.beta.grad = -grad_beta/torch.mean(torch.abs(grad_beta))\n",
    "                    optim_beta.step()\n",
    "            print('----------------------------------------------------------------------new_epoch')\n",
    "            \n",
    "            \n",
    "    def batch_likelihood(self,Y_batch,covariates_batch, O_batch, W, somme = True ): \n",
    "        '''\n",
    "        computes the approximation of the likelihood of a batch. \n",
    "        \n",
    "        args : \n",
    "                'Y_batch' : tensor of size(batch_size, p)\n",
    "                'covariates_batch' : tensor of size(batch_size, d)\n",
    "                'O_batch' : tensor of size(batch_size, p)\n",
    "                'acc' : float. the accuracy you want. The lower the accuracy, the lower the algorithm. \n",
    "                        we will sampThe size of tensor a (1000) must match the size of tensor b (20) at non-singleton dimension 2les 1/acc times. \n",
    "        returns : \n",
    "                the approximation of the likelihood. \n",
    "        ''' \n",
    "    \n",
    "        last_dim = len(W.shape)-1\n",
    "        if last_dim >1 : \n",
    "            N_samples = W.shape[0] # number of samples of W \n",
    "        else : N_samples = 1\n",
    "        #N_samples = W.shape[0]\n",
    "        Z = covariates_batch@self.beta + W@(self.C.T)\n",
    "        norm_W = TLA.norm(W, dim = last_dim)**2\n",
    "        log_fact =  torch.sum(log_stirling(Y_batch), axis = 1) # the factorial term \n",
    "        poiss_like =  - torch.sum(torch.exp(O_batch+Z), axis = last_dim) # first term of the poisson likelihood\n",
    "                                                                         #the normalising term with the exponential \n",
    "        poiss_like += torch.sum((O_batch+Z)*Y_batch, axis = last_dim)    # second term of the poisson likelihood\n",
    "        \n",
    "            \n",
    "        if somme : \n",
    "            # If we want the true likelihood\n",
    "            # We first take the exponential of the sum of the logs and then divide by the Number of samples we took.  \n",
    "            return torch.sum(torch.exp(-log_fact -1/2*norm_W+poiss_like))/N_samples \n",
    "        #for some purposes, we may want only the exponential and not the sum\n",
    "        else : return torch.exp(-log_fact -1/2*norm_W+poiss_like)/N_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE beta :  1.9102846328682492\n",
      "MSE beta :  1.8096150648046705\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  1.8552132061091728\n",
      "MSE beta :  1.9129760878867497\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.0701245395137264\n",
      "MSE beta :  2.3871577710512315\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  2.6920821053624366\n",
      "MSE beta :  3.339558541508027\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  4.266740013933548\n",
      "MSE beta :  5.829849607680941\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  8.225635687097924\n",
      "MSE beta :  9.993858093434689\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  13.342151645474365\n",
      "MSE beta :  14.396301048193923\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  17.317854779004286\n",
      "MSE beta :  19.84766625799734\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  23.838020380241492\n",
      "MSE beta :  24.570160465086182\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  27.37239572319345\n",
      "MSE beta :  28.576635872398175\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  29.592638528944445\n",
      "MSE beta :  30.05920858997783\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.101195695338298\n",
      "MSE beta :  31.85275115268177\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  32.91296705718656\n",
      "MSE beta :  34.02862881189437\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  34.21566166371409\n",
      "MSE beta :  33.66280883232819\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  33.16962321568695\n",
      "MSE beta :  32.483863490851355\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.87436034427889\n",
      "MSE beta :  31.822704369688143\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  31.48744476301318\n",
      "MSE beta :  31.168103517498643\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.860666924171813\n",
      "MSE beta :  30.878538073475863\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.748499069595027\n",
      "MSE beta :  30.507270106335973\n",
      "----------------------------------------------------------------------new_epoch\n",
      "MSE beta :  30.25545053219896\n",
      "MSE beta :  30.294877734221195\n",
      "----------------------------------------------------------------------new_epoch\n"
     ]
    }
   ],
   "source": [
    "model_bis = MC_PLNPCA_bis(q,n//2) \n",
    "model_bis.init_data(Y_sampled, O,covariates )\n",
    "model_bis.fit(20,0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
