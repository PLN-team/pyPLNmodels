{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import torch \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\beta \\mathbf{x}_{i}+\\mathbf{C} W_{i}, \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the log likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta} P_{\\theta}(Y)$$\n",
    "\n",
    "But we need to integrate out $W$ in order to compute the quantity inside the max : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i}\\right) &=\\int P_{\\theta}\\left(Y_{i}, W\\right) d W \\\\\n",
    "&=\\int P_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This integral being untractable, we are going to approximate it with Monte Carlo methods : \n",
    "\n",
    "$$\n",
    "\\int p_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W \\approx \\frac{1}{K} \\sum_{k = 1 }^Kp_{\\theta}\\left(Y_{i} \\mid W_{k}\\right) p\\left(W_k\\right)\n",
    "$$\n",
    "$$W_k \\sim \\mathcal N (0, I_q)$$\n",
    "\n",
    "The larger the $K$ the better the approximation.  \n",
    "\n",
    "Let's compute $p_{\\theta}\\left(Y_{i} \\mid W_{k}\\right) p\\left(W_k\\right)$. \n",
    "\n",
    "\n",
    "First, \n",
    "\n",
    "$$\n",
    "P\\left(W_{k}\\right)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2}\\left\\|W_{k}\\right\\|_{2}^{2}\\right)\n",
    "$$ \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{k}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{k}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{j}+W_{k}^{\\top} C_{j}\n",
    "$$\n",
    "\n",
    "For numerical purposes, we may want to use a logarithmic scale and apply the exponential function after. Indeed, $Y_{ij}$ can go up to a thousand, and computing this factorial would give infinite values. \n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(Y_{i} \\mid W_{k}\\right)=\\sum_{j=1}^{p} - \\ln \\left(Y_{i j} !\\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+Z_{i j}\\right)\n",
    "$$\n",
    "\n",
    "We are going tu use the Stirling Formula in order to compute the log of the factorial, to avoid computing directly the factorial.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$C :  (p,q)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be smaller (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    \n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*0.95**np.arange(block_size)\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.98**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    w,v = SLA.eigh(Sigma)\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:]))\n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 4 # nb of cavariates\n",
    "n = 200; p = 20\n",
    "q = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bastien/Documents/Stage/PLNpy/utils.py:212: UserWarning: Casting complex values to real discards the imaginary part (Triggered internally at  /opt/conda/conda-bld/pytorch_1614378062065/work/aten/src/ATen/native/Copy.cpp:219.)\n",
      "  root = torch.from_numpy(SLA.sqrtm(self.Sigma)).double()\n"
     ]
    }
   ],
   "source": [
    "#torch.manual_seed(0)\n",
    "true_Sigma = torch.from_numpy(build_block_Sigma(p,8))\n",
    "true_C = torch.from_numpy(C_from_Sigma(true_Sigma, q))\n",
    "true_beta =torch.randn((d, p))\n",
    "\n",
    "covariates = torch.randn((n,d))\n",
    "O =  1+torch.zeros((n,p))\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled, Z_sampled  = sample_model.sample(true_Sigma,true_beta, O, covariates)\n",
    "Y_sampled = torch.from_numpy(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes anything since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA(): \n",
    "    \n",
    "    def __init__(self,q): \n",
    "        self.q = q\n",
    "        pass\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        \n",
    "        self.Y = Y \n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))\n",
    "        self.beta = torch.randn(self.d,self.p)\n",
    "        \n",
    "        #self.beta = torch.clone(true_beta)\n",
    "        #self.C = torch.clone(true_C)\n",
    "    \n",
    "    def single_likelihood(self,i,acc): \n",
    "        '''\n",
    "        computes the likelihood of a single point. Useless since We can compute it efficiently\n",
    "        with batch_likelihood(). This is more a beta version. \n",
    "        '''\n",
    "        N_iter = int(1/acc)\n",
    "        Y_i = self.Y[i,:]\n",
    "        x_i = self.covariates[i,:]\n",
    "        O_i = self.O[i,:]\n",
    "        E = 0\n",
    "        for _ in range(N_iter):\n",
    "            W = torch.randn(self.q)\n",
    "            log_fact = torch.sum(log_stirling(Y_i))\n",
    "            norm_W = 1/2*torch.norm(W)**2\n",
    "            poiss_like = - torch.sum(torch.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            #print('shape : ',((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:]).shape)\n",
    "            poiss_like += torch.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            E+= torch.exp(-log_fact -norm_W + poiss_like)\n",
    "        return E/N_iter \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "    def batch_likelihood(self,Y_batch,covariates_batch, O_batch, acc): \n",
    "        '''\n",
    "        computes the approximation of the likelihood of a batch. \n",
    "        \n",
    "        args : \n",
    "                'Y_batch' : tensor of size(batch_size, p)\n",
    "                'covariates_batch' : tensor of size(batch_size, d)\n",
    "                'O_batch' : tensor of size(batch_size, p)\n",
    "                'acc' : float. the accuracy you want. The lower the accuracy, the lower the algorithm. \n",
    "                        we will samples 1/acc times. \n",
    "        returns : \n",
    "                the approximation of the likelihood. \n",
    "        '''\n",
    "        N_samples = int(1/acc)\n",
    "        E_batch = 0\n",
    "        W = torch.randn(N_samples, self.batch_size, self.q)\n",
    "        Z = covariates_batch@self.beta + W@(self.C.T)\n",
    "        norm_W = TLA.norm(W, dim = 2)\n",
    "        log_fact =  torch.sum(log_stirling(Y_batch), axis = 1) # the factorial term \n",
    "        poiss_like =  - torch.sum(torch.exp(O_batch+Z), axis = 2) # first term of the poisson likelihood\n",
    "                                                                       #the normalising term with the exponential \n",
    "        poiss_like += torch.sum((O_batch+Z)*Y_batch, axis = 2)    # second ter of the poisson likelihood\n",
    "        E_batch = torch.sum(torch.exp(-log_fact -1/2*norm_W+poiss_like))   # we sum each logarithm and then take the exponetial\n",
    "        return E_batch/N_samples # divide by the Number of samples we took.  \n",
    "    \n",
    "\n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        nb_full_batch, last_batch_size  = self.n//batch_size, self.n % batch_size  \n",
    "        self.batch_size = batch_size\n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*batch_size: (i+1)*batch_size]], \n",
    "                    self.covariates[indices[i*batch_size: (i+1)*batch_size]],\n",
    "                    self.O[indices[i*batch_size: (i+1)*batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "        \n",
    "    def compute_likelihood(self, acc, batch_size = n//20): \n",
    "        likelihood = 0\n",
    "        for Y_batch,covariates_batch, O_batch in self.get_batch(batch_size): \n",
    "            likelihood +=  self.batch_likelihood(Y_batch,covariates_batch, O_batch,acc)\n",
    "        return likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4.35 s, sys: 20.4 ms, total: 4.37 s\n",
      "Wall time: 1.39 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(3.5150e-09)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = MC_PLNPCA(q)\n",
    "model.init_data(Y_sampled, O, covariates)\n",
    "%time model.compute_likelihood(0.0001)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2228"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_oaks = read_csv('oaks_counts.csv').to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    " '''   def compute_single_log_like(self, i, acc):\n",
    "        N_iter = int(1/acc)\n",
    "        E = 0 \n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            E -= 1/2*SLA.norm(W)**2\n",
    "            E -= np.sum(np.exp(self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)) \n",
    "            E+= np.sum((self.O[i,:]+self.beta.T@self.covariates[i,:]+ self.C@W)*self.Y[i,:])\n",
    "            #print('E : ', E)\n",
    "        E/= N_iter\n",
    "        return E\n",
    "    \n",
    "    def batch_log_like(self,acc): \n",
    "        batch_E = 0\n",
    "        for i in range(10): \n",
    "            batch_E += self.compute_single_log_like(i,acc) \n",
    "        return batch_E\n",
    "    \n",
    "    def single_grad_beta_log_like(self,i, acc): \n",
    "        N_iter = int(1/acc)\n",
    "        grad = 0\n",
    "        for _ in range(N_iter): \n",
    "            W = np.random.randn(q)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(np.exp(self.O[i,:]+ self.covariates[i,:]@self.beta+self.C@W)).reshape(1,-1)\n",
    "            grad += self.covariates[i,:].T.reshape(-1,1)@(self.Y[i,:].reshape(1,-1))\n",
    "        return grad/N_iter\n",
    "    \n",
    "    def batch_grad_beta(self, acc): \n",
    "        batch_grad = 0\n",
    "        for i in range(10): \n",
    "            batch_grad += self.single_grad_beta_log_like(i,acc) \n",
    "        return batch_grad\n",
    "        \n",
    "   '''\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
