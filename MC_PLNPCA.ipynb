{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import numpy.linalg as NLA \n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import seaborn as sns \n",
    "import torch \n",
    "from pandas import read_csv\n",
    "import time\n",
    "import scipy.integrate as integrate \n",
    "from matplotlib import cm\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import threading\n",
    "from tqdm import tqdm\n",
    "#torch.set_default_dtype(torch.float64)\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import concurrent.futures\n",
    "from numba import vectorize , int32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.9.0+cu102'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "We first define some functions that will help implement and visualize the model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be different (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*toeplitz(0.7**np.arange(block_size))\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.7**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    ''' \n",
    "    get the best matrix of size (p,q) when Sigma is of size (p,p). i.e. reduces norm(Sigma-C@C.T)\n",
    "    args : \n",
    "        Sigma : np.array of size (p,p). Should be positive definite and symmetric.\n",
    "        q : int. The number of columns you want in your matrix C. \n",
    "        \n",
    "    returns : C_reduct : np.array of size (p,q) that contains the q eigenvectors with largest eigenvalues. \n",
    "    '''\n",
    "    w,v = SLA.eigh(Sigma) # get the eigenvaluues and eigenvectors\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:])) # we take only the q best. \n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(dict_models,name_doss , save = False):\n",
    "    '''\n",
    "    function to compare models. It will plot the MSE of Sigma and beta according to the true parameters and \n",
    "    the log_likelihood in the y axis with the runtime in the x-axis.\n",
    "    \n",
    "    args : \n",
    "        'dict_models' dict with key the name of the model and values the model (MC_PLNPCA object where you\n",
    "                      have called model.fit_IMPS()).\n",
    "        'name_doss' : str. the name of the file you want to save the graphic. \n",
    "        'save' : bool. If True, the figure will be saved. If false, won't be saved. \n",
    "        \n",
    "    returns : \n",
    "            None but displays the figure. you can save the figure if save = True\n",
    "    '''\n",
    "    best_model = MC_PLNPCA(q, batch_size, true_value = True)\n",
    "    best_model.init_data(Y_sampled, O, covariates)\n",
    "    \n",
    "    \n",
    "    #get the max of all the likelihoods for a nice plot. We intialize the max with the likelihood of the \n",
    "    # model that has the true parameters (unknown in practice) \n",
    "    best_log_like = best_model.compute_mean_log_likelihood(0.0001)\n",
    "    max_ = best_log_like\n",
    "    for name,model in dict_models.items():\n",
    "        current_max = np.max(np.array(model.log_likelihood_list))\n",
    "        max_ = max(max_,current_max)\n",
    "    fig,ax = plt.subplots(3,1,figsize = (15,15))\n",
    "    \n",
    "    \n",
    "    \n",
    "    ax[0].axhline(best_MSE_Sigma, label = 'MLE latent layer ')\n",
    "    ax[1].axhline(best_MSE_beta, label = 'MLE latent layer')\n",
    "    for name,model in dict_models.items() : \n",
    "        \n",
    "\n",
    "        abscisse = model.running_times\n",
    "        plt.subplots_adjust(hspace = 0.4)\n",
    "        ax[0].plot(abscisse, model.MSE_Sigma_list, label = name)\n",
    "        ax[0].legend()\n",
    "        ax[0].set_title('MSE Sigma')\n",
    "        \n",
    "        ax[0].set_ylabel('MSE')\n",
    "        ax[0].set_yscale('log')\n",
    "        \n",
    "        ax[1].plot(abscisse, model.MSE_beta_list, label = name)\n",
    "        ax[1].legend()\n",
    "        ax[1].set_title('MSE beta') \n",
    "        \n",
    "\n",
    "        ax[1].set_ylabel('MSE')\n",
    "        ax[1].set_yscale('log')\n",
    "        \n",
    "        ax[2].plot(abscisse, np.array(model.log_likelihood_list)-max_, label = name )\n",
    "        ax[2].legend()\n",
    "        ax[2].set_title('log_likelihood')\n",
    "        ax[2].set_ylabel('log_likelihood')\n",
    "        ax[2].set_yscale('symlog')\n",
    "    \n",
    "    ax[2].axhline(np.array(best_log_like)-max_, c = 'red', label = 'best likelihood')\n",
    "    plt.suptitle(name_doss)\n",
    "    if save : \n",
    "        plt.savefig(name_doss)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def show(model,name_doss , save = False):\n",
    "    '''\n",
    "    function to show one model. It will plot the MSE of Sigma and beta according to the true parameters and \n",
    "    the log_likelihood in the y axis with the runtime in the x-axis.Will also plot the norm of the gradients and \n",
    "    the variance of the weights used for the importance sampling. \n",
    "    \n",
    "    args : \n",
    "        'model' MC_PLNPCA object where you have called model.fit_IMPS()\n",
    "        'name_doss' : str. the name of the file you want to save the graphic. \n",
    "        'save' : bool. If True, the graphic will be saved. If false, won't be saved. \n",
    "        \n",
    "    returns : \n",
    "            None but displays the figure. It can also save the figure if save = True. \n",
    "    '''\n",
    "    best_model = MC_PLNPCA(q, batch_size, true_value = True)\n",
    "    best_model.init_data(Y_sampled, O, covariates)\n",
    "    fig,ax = plt.subplots(7,1,figsize = (15,15))\n",
    "    \n",
    "    best_log_like = best_model.compute_mean_log_likelihood(0.0001)\n",
    "    max_ = max(best_log_like,np.max(np.array(model.log_likelihood_list)))\n",
    "    \n",
    "        \n",
    "    d = model.beta.shape[0]\n",
    "    #MSE of Sigma\n",
    "    abscisse = model.running_times\n",
    "    plt.subplots_adjust(hspace = 0.4)\n",
    "    ax[0].plot(abscisse, model.MSE_Sigma_list)\n",
    "    ax[0].set_title('MSE Sigma')\n",
    "    ax[0].axhline(best_MSE_Sigma, label = 'MLE latent layer ', c = 'red', linestyle = '--')\n",
    "    ax[0].set_xlabel('Seconds')\n",
    "    ax[0].set_ylabel('MSE')\n",
    "    ax[0].set_yscale('log')\n",
    "    \n",
    "    #MSE of beta. \n",
    "    ax[1].plot(abscisse, model.MSE_beta_list)\n",
    "    ax[1].set_title('MSE beta') \n",
    "    ax[1].axhline(best_MSE_beta, label = 'MLE latent layer', c = 'red', linestyle = '--')\n",
    "    ax[1].set_ylabel('MSE')\n",
    "    ax[1].set_yscale('log')\n",
    "    ax[1].legend()\n",
    "    \n",
    "    #log likelihood of the model. will also plot the log likelihood of the model with the true parameters. \n",
    "    ax[2].plot(abscisse, np.array(model.log_likelihood_list)-max_)\n",
    "    ax[2].set_title('log_likelihood')\n",
    "    ax[2].set_ylabel('log_likelihood')\n",
    "    ax[2].set_xlabel('Seconds')\n",
    "    best_log_like = best_model.compute_mean_log_likelihood(0.0001)\n",
    "    #variational_log_like = variational_model.compute_mean_log_likelihood(0.0001)[0].item()\n",
    "    #ax[2].axhline(variational_log_like, c = 'black', label = 'variaitional likelihood')\n",
    "    ax[2].axhline(best_log_like-max_, c = 'red', label = 'best likelihood')\n",
    "    ax[2].set_yscale('symlog')\n",
    "    ax[2].legend()\n",
    "    \n",
    "    # plot the L1 norm of the gradients. \n",
    "    ax[3].plot(abscisse, model.norm_grad_log_C_list, label = 'norm grad loglike C')\n",
    "    ax[3].plot(abscisse, model.norm_grad_log_beta_list, label = 'norm grad loglike beta')\n",
    "    ax[3].set_title('Norm Gradients loglike ')\n",
    "    ax[3].set_ylabel('L1 norm')\n",
    "    ax[3].set_yscale('log')\n",
    "    ax[3].legend()\n",
    "    \n",
    "\n",
    "    #here we plot the variance for the 3 integrals. We set a threshold from which we consider that the integral \n",
    "    # is misestimate if the variance of the weights is above. \n",
    "    threshold = 0.9\n",
    "    # integral for beta \n",
    "    ax[4].scatter( np.linspace(0,abscisse[-1], model.var_weights['beta'].shape[0]), model.var_weights['beta'],s = 1, label = 'variance of the weights for beta', c = 'black')\n",
    "    percentage_bad_beta = np.sum(model.var_weights['beta']>threshold)/(model.var_weights['beta'].shape[0])\n",
    "    ax[4].axhline(model.acc, c = 'red', label = 'reference')\n",
    "    ax[4].set_title('Variance beta, pourcentage of bad approximations : '+ str(np.round(percentage_bad_beta,3)))\n",
    "    ax[4].set_yscale('log')\n",
    "    ax[4].legend()\n",
    "    \n",
    "    #integral for C_exp\n",
    "    ax[5].scatter( np.linspace(0,abscisse[-1], model.var_weights['C_exp'].shape[0]), model.var_weights['C_exp'],s = 1, label = 'variance of the weights for C_exp')\n",
    "    percentage_bad_exp_C = np.sum(model.var_weights['C_exp']>threshold)/(model.var_weights['C_exp'].shape[0])\n",
    "    ax[5].axhline(model.acc, c = 'red', label = 'reference')\n",
    "    ax[5].set_title('Variance exp_C, pourcentage of bad approximations : '+ str(np.round(percentage_bad_exp_C,3)))\n",
    "    ax[5].set_yscale('log')\n",
    "    ax[5].legend()\n",
    "    \n",
    "    #integral for C_W\n",
    "    ax[6].scatter( np.linspace(0,abscisse[-1], model.var_weights['C_W'].shape[0]), model.var_weights['C_W'],s = 1, label = 'variance of the weights for W', c = 'red')\n",
    "    percentage_bad_C_W= np.sum(model.var_weights['C_W']>threshold)/(model.var_weights['C_W'].shape[0])\n",
    "    ax[6].scatter( np.linspace(0,abscisse[-1], model.var_weights['C_W'].shape[0]), model.var_weights['C_W'],s = 1, label = 'variance of the weights for W', c = 'red')\n",
    "    ax[6].set_title('Variance C_W, pourcentage of bad approximations : '+ str(np.round(percentage_bad_C_W,3)))\n",
    "    ax[6].axhline(model.acc, c = 'red', label = 'reference')\n",
    "    ax[6].set_yscale('log')\n",
    "    ax[6].legend()\n",
    "    \n",
    "    if save : \n",
    "        plt.savefig(name_doss)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_lr(optimizer,N_iter, acc, lrs, q, batch_size, average,var_beta, var_C):\n",
    "    '''\n",
    "    function that is supposed to find the best lr within a grid of lr (lrs) for \n",
    "    an optimizer to fit the MC_PLNPCA method. the metric used\n",
    "    to discriminate each lr is the log likelihood. We parallelized the code to go faster. \n",
    "    \n",
    "    args: otpimizer : should be an object of torch.optim., for example torch.optim.Rprop. \n",
    "    \n",
    "        N_iter : int. The number of iterations you want to do when running the .fit_IMPS method.\n",
    "    \n",
    "        acc : float positive. the accuracy you want when running the .fit_IMPS method. \n",
    "        \n",
    "        lrs : list. the grid of lr.\n",
    "        \n",
    "        q : the dimension q you want for the model. should be small.  \n",
    "        \n",
    "        batch_size : int. The batch_size you want  when running the .fit_IMPS method.\n",
    "        \n",
    "        average : int. the average argument for the model. \n",
    "        \n",
    "        var_beta : positive float. the variance you want for the importance law for the integral that determines the gradient\n",
    "                  of beta. \n",
    "                  \n",
    "        var_C : positive float. The variance you want for the importance law for the integral\n",
    "                that determines the gradient of C. \n",
    "        \n",
    "        \n",
    "    returns : the best lr according to the metric\n",
    "    '''\n",
    "    #initiate some lists. \n",
    "    metrics = list()\n",
    "    models = list()\n",
    "    ts = list()\n",
    "    # launch the models with the right lr. \n",
    "    for lr in lrs: \n",
    "        model = MC_PLNPCA(q,batch_size, average = average, is_perfect_mean= False,true_value = False, var_beta = var_beta, var_C = var_C)\n",
    "        t = threading.Thread(target =model.fit_IMPS,\n",
    "                            args = [Y_sampled, O,covariates, N_iter          ,acc, lr     , optimizer] )\n",
    "        #t =threading.Thread(target = model.fit_torch,\n",
    "        #      args = [Y_sampled, O,covariates, N_iter          ,acc, lr     , optimizer] )\n",
    "        t.start()\n",
    "        ts.append(t)\n",
    "        models.append(model)\n",
    "        \n",
    "    # join each threading. \n",
    "    for t in ts: \n",
    "        t.join()\n",
    "    # get the metrics for each model. \n",
    "    for i, model in enumerate(models) :\n",
    "        metrics.append(np.mean(np.array(model.log_likelihood_list)))\n",
    "    metrics = np.nan_to_num(np.array(metrics),-100000)\n",
    "    argmax = np.argmax(metrics)#argmax of the metrics \n",
    "    best_lr = lrs[argmax]\n",
    "    return best_lr, dict(zip(lrs,models)), models[argmax]  #the best lr, the dict in case \n",
    "                                                           # we need it to comopare, and the best model. \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "lrs = [0.01, 0.02,0.03, 0.1,0.2,0.3,0.5,1.]#,1.2,1.5,2]\n",
    "vars_ = [0.01,0.1,0.5,1,1.5,2,4,10]\n",
    "#%time best_var_beta, dict_vars_beta, best_var_beta_model = find_best_var_beta(torch.optim.Adamax, 30,0.001,0.05, vars_, q ,batch_size, n//2)\n",
    "#%time best_var_C, dict_vars_C, best_var_C_model = find_best_var_C(torch.optim.Adamax, 30,0.001,0.04, vars_, q ,batch_size, n//2)\n",
    "#%time best_lr,dict_lrs,best_model = find_best_lr(torch.optim.Adamax, 30, 0.01,lrs, q , batch_size , average = n//2,var_beta = best_var_beta, var_C = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_optimizers = {'Adamax' : torch.optim.Adamax,\n",
    "                   'Adadelta': torch.optim.Adadelta, \n",
    "                   'Adam': torch.optim.Adam, \n",
    "                   'Adagrad' : torch.optim.Adagrad,\n",
    "                  'Rprop' : torch.optim.Rprop, \n",
    "                   'RMSprop' : torch.optim.RMSprop,\n",
    "                  'SGD' : torch.optim.SGD}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def launch_optimizers_and_lrs(dict_optimizers,N_iter,N_iter_test, acc, q,batch_size):\n",
    "    '''\n",
    "    launch some optimizers to fit the .fit_IMPS method of the MC_PLNPCA object to compare them after. \n",
    "    We first also take the right learning rate for each optimizer. may take some time and some memory.  \n",
    "    \n",
    "    args : \n",
    "         dict_optimizer : dictionnarie. dict that contains the optimizer. example: {'Rprop': torch.optim.Rprop}\n",
    "         \n",
    "         N_iter : int. the number of iteration you want the models to do for each optimizer. \n",
    "         \n",
    "         N_iter_test : int. the number of iteration you want to do to find the best learning rate. \n",
    "         \n",
    "         acc : positive float. the accuracy you want when running the .fit_IMPS method. \n",
    "             \n",
    "         q : int. the dimension you want for the MC_PLNPCA object. \n",
    "         \n",
    "         batch_size : int. the batch_size you want. \n",
    "         \n",
    "    return : a dict of models. The key are the name of the optimizer, the values are the models (MC_PLNPCA object) trained with \n",
    "            the corresponding optimizer and the best lr (for each optimizer).\n",
    "    '''\n",
    "    \n",
    "    models = list()\n",
    "    lrs = [0.001,0.01, 0.02,0.03, 0.1,0.2,0.3,0.5]\n",
    "    ts_lr = list()\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        params = []\n",
    "        for name,optimizer in dict_optimizers.items(): \n",
    "            params.append([optimizer, N_iter_test,acc,lrs, q,batch_size ,n//4])\n",
    "        #did not have the choice to do this since I need to get the returned value back. \n",
    "        futures = [executor.submit(find_best_lr, param[0],param[1],param[2],param[3],param[4],param[5],param[6]) for param in params]\n",
    "        best_lrs = [f.result()[0] for f in futures]\n",
    "    \n",
    "    del futures\n",
    "    print('best_lrs ', best_lrs)\n",
    "    ts = list()    \n",
    "    # lauch all the model with the right optimizer in a parallelized fashion. \n",
    "    \n",
    "    for i, (name, optimizer) in enumerate(dict_optimizers.items()): \n",
    "        model = MC_PLNPCA(q,batch_size)\n",
    "        print(' lr chosen : ', best_lrs[i], ' for the optimizer ', optimizer)\n",
    "        t = threading.Thread(target =model.fit_IMPS,\n",
    "              args = [Y_sampled, O,covariates, N_iter          ,acc, best_lrs[i]     , optimizer] )\n",
    "        t.start()\n",
    "        ts.append(t)\n",
    "        models.append(model)\n",
    "    # join each threading. \n",
    "    for t in ts : \n",
    "        t.join()\n",
    "    # return the dict with the corresponding values and keys. \n",
    "    return dict(zip(dict_optimizers.keys(), models))\n",
    "\n",
    "\n",
    "def launch_optimizers(dict_optimizers,N_iter,best_lrs, acc, q,batch_size):\n",
    "    '''\n",
    "    launch some optimizers to fit the .fit_IMPS method of the MC_PLNPCA object to compare them after. \n",
    "    args : \n",
    "         dict_optimizer : dictionnarie. dict that contains the optimizer. example: {'Rprop': torch.optim.Rprop}\n",
    "         \n",
    "         N_iter : int. the number of iteration you want the models to do for each optimizer. \n",
    "         \n",
    "         best_lrs : list of float. the (best)learning rate of each optimizer.\n",
    "    \n",
    "         acc : positive float. the accuracy you want when running the .fit_IMPS method. \n",
    "         \n",
    "         q : int. the dimension you want for the MC_PLNPCA object. \n",
    "         \n",
    "         batch_size : int. the batch_size you want. \n",
    "         \n",
    "    return : a dict of model. The key are the name of the optimizer, the values are the models trained with \n",
    "            the corresponding optimizer. \n",
    "    '''\n",
    "    print('best_lrs ', best_lrs)\n",
    "    ts = list()  \n",
    "    models = list()\n",
    "    # lauch all the model with the right optimizer in a parallelized fashion. \n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        params = []\n",
    "        for i,(name,optimizer)  in enumerate(dict_optimizers.items()): \n",
    "            \n",
    "            params.append([Y_sampled, O,covariates,N_iter, acc, best_lrs[i], optimizer])\n",
    "        #did not have the choice to do this since I need to get the returned value back. \n",
    "        futures = [executor.submit(MC_PLNPCA(q,batch_size,average = n//4).fit_IMPS, param[0],param[1],param[2],param[3],param[4],param[5],param[6]) for param in params]\n",
    "        models =  [f.result() for f in futures]\n",
    "    # return the dict with the corresponding values and keys. \n",
    "    return dict(zip(dict_optimizers.keys(), models))\n",
    "\n",
    "#%time dict_optim_launched = launch_optimizers_and_lrs(dict_optimizers,50,15, 0.005,q,30)\n",
    "#%time dict_optim_launched = launch_optimizers(dict_optimizers,50,15, 0.005,q,30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_g(N_samples, mu_prop, var_prop):\n",
    "    '''\n",
    "    function that samples some gaussians with the right mean and the right dimensions. \n",
    "    \n",
    "    arg : \n",
    "        N_samples : int. the number of samples you want. For a MC approximation, the larger N_samples, \n",
    "                    the better the approximation. \n",
    "        mu_prop : torch.tensor of size (batch_size, p,q,q) or (batch_size,p,q) or (batch_size,q,q). It depends\n",
    "                  on the dimension of the integral you want to compute. \n",
    "        var_prop : float positive. the variance you want. It will be shared along all the samples. \n",
    "                We can easily make one variance for each (the dimension can be of the same dimension as mu_prop)\n",
    "    returns: \n",
    "        a gaussian with mean mu_prop and variance var_prop. The dimension will be : (N_samples, mu_prop.shape). \n",
    "    '''\n",
    "    len_mu = len(mu_prop.shape)\n",
    "    print('mu_prop . shape : ', mu_prop.shape)\n",
    "    # case where we whant to integrate a vector of integrals\n",
    "    if len_mu == 3: \n",
    "        gaussian =  torch.randn(N_samples, 1,1,1)\n",
    "        return gaussian*var_prop + mu_prop\n",
    "    # case where we want to integrate a matrice of integrals\n",
    "    elif len_mu == 4 : \n",
    "        gaussians =  torch.randn(N_samples,1,1,1,1)\n",
    "        return gaussians*var_prop + mu_prop\n",
    "def log_density_g(V, mu, var):\n",
    "    '''\n",
    "    computes the log of the density of a gaussian, given the mean and the variance.\n",
    "    \n",
    "    args : \n",
    "        V : a gaussain of any size. It should be a gaussian of mean mu and variance var.\n",
    "        mu : a vector of means (for example of size (bath_size, p,q,q) if you want to estimate the integral of \n",
    "                                exp(CW)W)\n",
    "        var : positive float. The variance you want for the importance law. \n",
    "        \n",
    "    returns: the log density of the gaussian. if the size of V is (N_samples, batch_size, p,q,q), the resulting size\n",
    "            will be (N_samples, batch_size,p,q) (We just take the norm wrt the last dimension)\n",
    "    '''\n",
    "    norm = -1/(2*var)*torch.norm(V-mu, dim = -1)**2\n",
    "    return norm   \n",
    "\n",
    "def dict_log_density_g(dict_V, dict_mu, var): \n",
    "    '''\n",
    "    same as log_density_g but do it for a dict. each value of the dict is a \n",
    "    gaussian similar as the argument of log_density_g. same for mu_prop. \n",
    "    \n",
    "    returns : a dict. Each key is the result of log_density_g for the value of the argument. var is shared along \n",
    "    the dict. \n",
    "    '''\n",
    "    log_g = {}\n",
    "    for name, V in dict_V.items():\n",
    "        log_g[name] =  -1/(2*var[name])*torch.norm(V-dict_mu[name], dim = -1)**2\n",
    "    return log_g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_choice_and_mean(mean_i, k,l,p,q):\n",
    "    '''\n",
    "    simple function to determine which integral we are trying to compute\n",
    "    given the shape of the mean. Indeed, all the integral sizes are different, \n",
    "    so that we can deduce the integral from the shape. \n",
    "    if size is (p,q,q), then it is C_exp, if (p,q) then it is beta, if (q,q) then it is C_W. \n",
    "    We return the name of the integral and the corresponding mean for the right index (k,l) in dim = 3\n",
    "    or k in dim = 2. \n",
    "    args : \n",
    "        mean_i : torch.tensor.mean of a sample i, the length of its shape should be either 2 or 3. \n",
    "        \n",
    "        k,l : int. indexes. We will return mean_i[k] or mean_i[k,l] according to the shape of mean_i\n",
    "        \n",
    "        p,q : int. The dimensions of the model. \n",
    "        \n",
    "    returns : string, float. the string is here to tell you which integral to take. The float is the mean \n",
    "    that will be used to plot the gaussian. \n",
    "    '''\n",
    "    if len(mean_i.shape)>2: \n",
    "        choice = 'C_exp'\n",
    "        mean = mean_i[k,l]\n",
    "    elif mean_i.shape[0] == p : \n",
    "        choice = 'beta'\n",
    "        mean = mean_i[k]\n",
    "    elif mean_i.shape[0] == q : \n",
    "        choice = 'C_W'\n",
    "        mean = mean_i[k]\n",
    "    return choice, mean\n",
    "\n",
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes nothing since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorized_log_likelihood(Y_b,covariates_b, O_b, W, C,beta,verbose = False):\n",
    "    '''\n",
    "    computes the log likelihood of one batch in a vectorize fashion. We compute the likelihood\n",
    "    of each sample in the batch and then take the sum of all the logs. ( We can't compute the log likelihood directly). \n",
    "\n",
    "    args : \n",
    "        Y_b : tensor of size (n_batch,p) \n",
    "        covariates_b : tensor of size (n_batch,d) \n",
    "        O_b : tensor of size (n_batch,p) \n",
    "        W : torch of size (n_samples, n_batch,q). should be a gaussian. \n",
    "        verbose : bool. If true, will plot some variables. \n",
    "\n",
    "    returns : the log_likelihood. \n",
    "    '''\n",
    "    # see formula above for more details. \n",
    "    norm_W = -1/2*torch.norm(W,dim = 2)**2 # the norm of W\n",
    "    log_fact = -torch.sum(log_stirling(Y_b), axis = 1) # the factorial term. \n",
    "    Z_b = covariates_b@beta + W@(C.T) # Z, latent variable\n",
    "    exp_term = -torch.sum(torch.exp(O_b+Z_b),axis = 2) # exponential term\n",
    "    data_term = torch.sum(Y_b * (O_b + Z_b),axis = 2) # term with the data counts Y \n",
    "    sum_of_logs = (norm_W +log_fact +exp_term+data_term) # sum of all the logs\n",
    "    if verbose :\n",
    "        print('norm_W : ', norm_W.numpy(),'\\n')\n",
    "        print('log_factoriel', log_fact.numpy(),'\\n')\n",
    "        print('cov : ', covariates_b)\n",
    "        print('beta : ', beta)\n",
    "        print('covbeta ; ', covariates_b@beta)\n",
    "        print('C',C)\n",
    "        print('Z_b : ', covariates_b@beta + W@(C.T), '\\n')\n",
    "        print('exp_Z_b,', torch.exp(Z_b))\n",
    "        print('exp_term ', exp_term.detach().numpy())\n",
    "        print('data ',data_term.detach().numpy(),'\\n')\n",
    "        print('somme_of_logs ( = norm_W +log_fact + exp_term + data) : ', sum_of_logs.detach().numpy(),'\\n' )\n",
    "        print('somme -max : ',(sum_of_logs-torch.max(sum_of_logs)).detach().numpy(),'\\n' )\n",
    "        print('result = exp(somme)  ', torch.exp(sum_of_logs).detach().numpy(),'\\n')\n",
    "    log = torch.log(torch.mean(torch.exp(sum_of_logs),axis = 0))\n",
    "    nb_notinf = torch.sum(log.isfinite())\n",
    "    #print('perc_not inf', nb_notinf/self.batch_size)\n",
    "    return torch.sum(torch.nan_to_num(log,neginf = 0 ))/nb_notinf \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main class of the notebook. \n",
    "Most of the methods are useless, but I keep them just in case. If you want to know what I am doing, \n",
    "just look at the .fit_IMPS method and the method it calls. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_PLNPCA(): \n",
    "    '''\n",
    "    MC_PLNPCA class. \n",
    "    \n",
    "    This class has method that tries to fit the PLN model with Monte Carlo approximation. \n",
    "    More precisely, we use Importance sampling to evaluate the gradients, and we use Monte Carlo\n",
    "    to get the log_likelihood. \n",
    "    \n",
    "    We need to estimate 3 integrals to get the gradients for beta and C. \n",
    "    The 3 integrals are named C_exp, C_W and beta. \n",
    "        - C_exp : the integral of exp(CW)W that is needed to compute the gradient of C\n",
    "        - C_W : the integral of W, that is needed to compute the gradient of C.\n",
    "        - beta : the integral of exp(CW), that is needed to compute the gradient of beta.\n",
    "    Most of the time, we will try to put all those 3 parts in a dict for a clearer code. \n",
    "    \n",
    "    '''\n",
    "    def __init__(self,q, batch_size, true_value = False, average = 10, is_perfect_mean = False,var_beta = 1,var_C = 1, zero_mean = False): \n",
    "        '''\n",
    "        initialization. An initialization that fits the data is done in the init_data method. \n",
    "        args : \n",
    "             q : int. the dimension q in the PLN model. \n",
    "             batch_size : int. the number of samples of Y you want to estimate the gradients. \n",
    "                     Note that Y will be set when calling the .fit_IMPS method only, and batch_size should\n",
    "                     be lower than Y.shape[0].\n",
    "             true_value : bool. If True, the algorithm will initialize beta and C with the true value. \n",
    "                         This can be done since we know here the true parameters. The purpose of this is\n",
    "                         for comparison. \n",
    "             average : int. Since the algorithm is stochastic, we take an average of the last\n",
    "                       parameter infered for a more precise parameter ( it has a smooth property when \n",
    "                       plotting the MSE). \n",
    "             is_perfect_mean : bool : There are two ways for estimating the integrals, one is the 'perfect' one that \n",
    "                         uses a gaussian that fits very well each coordinate of the integral, the other uses a gaussian \n",
    "                         that takes the right means for only one coordinate and use this for all the other coordinates. \n",
    "                         The perfect one is more precise but much slower. \n",
    "             var_beta : positive float. The variance you want to take for the importance law for the \n",
    "                        estimation of the integral of beta. \n",
    "             var_C : positive float. The varaince you want to take for the importance law for the \n",
    "                     estimation of the integralS of C (C_W and C_exp)\n",
    "             zero_mean : bool. If True, the means that we take for the gaussians for importance sampling will \n",
    "                         be full of zeros. This is here to chek the efficiency of taking different means than\n",
    "                         zero. \n",
    "        returns : a MC_PLNPCA object with the right parameters, and some list and dicts initialized for after. \n",
    "        '''\n",
    "        self.q = q\n",
    "        self.batch_size = batch_size\n",
    "        self.average = average\n",
    "        self.is_perfect_mean = is_perfect_mean\n",
    "        self.var = {'beta':  var_beta, 'C_W' : var_C, 'C_exp' : var_C}\n",
    "        self.var_beta = var_beta\n",
    "        self.var_C = var_C\n",
    "        # to keep records of the running_times, the norm of the gradiens, \n",
    "        #the log_likelihood, some MSE and some weights\n",
    "        self.running_times = list()\n",
    "        self.norm_grad_log_C_list = list() \n",
    "        self.norm_grad_log_beta_list = list()\n",
    "        self.MSE_Sigma_list = list()\n",
    "        self.MSE_beta_list = list()\n",
    "        self.log_likelihood_list = list()\n",
    "        self.var_weights = {'beta': np.array([]), 'C_exp': np.array([]), 'C_W' : np.array([])}\n",
    "        self.var_weights_bis = np.array([])\n",
    "        self.true_value = true_value\n",
    "        self.cmpt = 0 \n",
    "        self.last_likelihoods = list()\n",
    "        self.zero_mean = zero_mean\n",
    "    \n",
    "    def init_data(self, Y,O,covariates): \n",
    "        '''\n",
    "        Initialise some usefuls variables given the data. \n",
    "        We also initialise C and beta. \n",
    "        '''\n",
    "        torch.manual_seed(0)\n",
    "        self.Y = Y.float()\n",
    "        self.covariates = covariates \n",
    "        self.O = O \n",
    "        self.n = Y.shape[0] \n",
    "        self.p = Y.shape[1]\n",
    "        self.d = self.covariates.shape[1]\n",
    "        noise = torch.randn(self.p) \n",
    "        self.Sigma =  (torch.diag(noise**2)+ 1e-1)\n",
    "        self.last_betas = torch.zeros(self.average,d,p) # init of the average of the last betas\n",
    "        self.last_Cs = torch.zeros(self.average, p, q) #init of the average of the last Sigmas\n",
    "        # init C with an array of size (p,q)\n",
    "        self.C = torch.clone(torch.from_numpy(C_from_Sigma(self.Sigma,self.q)))*0\n",
    "        # init beta \n",
    "        self.beta = torch.randn(self.d,self.p)*0\n",
    "        # if we want to take the true value of C and beta \n",
    "        if self.true_value : \n",
    "            self.C = torch.clone(true_C)\n",
    "            self.beta = torch.clone(true_beta)\n",
    "        #setting some gradients for optimization. \n",
    "        self.C.requires_grad_(False)\n",
    "        self.beta.requires_grad_(False)\n",
    "        self.C_mean = torch.clone(self.C)\n",
    "        self.beta_mean = torch.clone(self.beta)\n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma. We do this since we only optimize C. \n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "\n",
    "    def get_batch(self,batch_size, save_batch_size = True): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always 0)\n",
    "        '''\n",
    "        #np.random.seed(2)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        #if we want to set the batch size of the model to the given batch_size \n",
    "        if save_batch_size : \n",
    "            self.batch_size = batch_size\n",
    "        # get the number of batches and the size of the last one. \n",
    "        nb_full_batch, last_batch_size  = self.n//batch_size, self.n % batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*batch_size: (i+1)*batch_size]], \n",
    "                    self.covariates[indices[i*batch_size: (i+1)*batch_size]],\n",
    "                    self.O[indices[i*batch_size: (i+1)*batch_size]], \n",
    "                    ) \n",
    "        if last_batch_size != 0 : \n",
    "            if save_batch_size : \n",
    "                self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]]\n",
    "                    )\n",
    "         \n",
    "    def get_mean_log_likelihood(self):\n",
    "        '''\n",
    "        get the mean of the likelihood of the model. We do this since it is very volatile, so taking the mean\n",
    "        is more robust.\n",
    "        We need to have called the .fit_IMPS() method. \n",
    "        '''\n",
    "        length = len(self.likelihood_list)\n",
    "        #we take the last values to have a good approximation but we don't want to take too many values\n",
    "        #since otherwise it will be biased ( the first values are smaller than the last one). \n",
    "        return torch.mean(torch.tensor([x  for x in self.log_likelihood_list[length//10:] if np.isnan(x) == False]))\n",
    "    \n",
    "    \n",
    "    def compute_mean_log_likelihood(self, acc): \n",
    "        '''\n",
    "        computes the mean of the log_likelihood of the whole dataset. Since we can only estimate\n",
    "        the likelihood of some samples, we estimate the likelihood of each sample ( n samples in total),\n",
    "        We take the log and then sum each logs. Since some likelihood results in numerical zero, \n",
    "        We take the mean of the samples that have non zero likelihood. \n",
    "        \n",
    "        args : \n",
    "                acc : float lower than 1 and positive. The accuracy you want. We will sample 1/acc samples.\n",
    "                \n",
    "        returns : \n",
    "                the mean of the loglikelihood of each sample. \n",
    "        '''\n",
    "        log_like = 0\n",
    "        N_samples = int(1/acc)\n",
    "        cmpt = 0 \n",
    "        for Y_b,covariates_b,O_b in self.get_batch(self.batch_size): \n",
    "            cmpt+=1\n",
    "            W = torch.randn(N_samples, self.batch_size, self.q)\n",
    "            log_like += self.vectorized_log_likelihood(Y_b,covariates_b,O_b,W).item()\n",
    "        return log_like/cmpt\n",
    "                \n",
    "\n",
    "    def vectorized_log_likelihood(self, Y_b,covariates_b, O_b, W, verbose = False):\n",
    "        return vectorized_log_likelihood(Y_b,covariates_b,O_b,W,self.C,self.beta,verbose)\n",
    "\n",
    "    \n",
    "        \n",
    "    def each_log_P_WgivenY(self,Y_b,covariates_b,O_b,W): \n",
    "        '''       \n",
    "        computes the log of the density of W given Y_b. Y_b is a tensor of size (n_batch,p).\n",
    "        args : \n",
    "            Y_b : tensor of size (n_batch,p) \n",
    "            covariates_b : tensor of size (n_batch,d) \n",
    "            O_b : tensor of size (n_batch,p) \n",
    "            W : torch of size (n_batch,q). should be a gaussian. \n",
    "            \n",
    "        returns : the log of the density of (W,Y_b) given Y_b\n",
    "        '''\n",
    "        A_b = O_b + W@(self.C.T) +covariates_b@self.beta # temporary variable, for a clearer code. \n",
    "        return -1/2*torch.norm(W,dim = -1)**2+torch.sum(-torch.exp(A_b)+W@(self.C.T)*Y_b, axis = -1) # we sum over the last axis\n",
    "        #return -1/2*torch.norm(W,dim = 1)**2+torch.sum(-torch.exp(A_b)+W@(self.C.T)*Y_b, axis = 1) # we sum over the last axis\n",
    "\n",
    "    def batch_log_P_WYgivenY(self,Y_b,covariates_b,O_b,V_b): #useless, need to delete this.\n",
    "        '''\n",
    "        same as each_log_P_WYgivenY but do it for W of size (n_samples, n_batch, self.q)\n",
    "        \n",
    "        args : \n",
    "            Y_b : tensor of size (n_batch,p) \n",
    "            covariates_b : tensor of size (n_batch,d) S\n",
    "            O_b : tensor of size (n_batch,p) \n",
    "            V_b : tensor of size (N_samples, n_batch, self.q). Should be a gaussian. \n",
    "            \n",
    "        returns : a tensor of size (N_samples, n_batch) that contains the log of the density of WYgivenY\n",
    "                for each samples W_i of size (n_batch,self.q) and each (Y_i,covariates_i,0_i) in the batch.  \n",
    "        '''\n",
    "        A_b = O_b + V_b@(self.C.T) +covariates_b@self.beta # temporary variable\n",
    "        #return -1/2*torch.norm(V_b,dim = -1)**2+torch.sum(-torch.exp(A_b)+V_b@(self.C.T)*Y_b, axis = 2)\n",
    "    \n",
    "    def log_P_WgivenY(self,Y_i,covariates_i, O_i, W):\n",
    "        '''\n",
    "        computes the log of the density of (W,Y_i) given Y_i. It should look like a gaussian. \n",
    "        args : \n",
    "            Y_i : tensor of size p \n",
    "            covariates_i : tensor of size d \n",
    "            O_i : tensor of size p \n",
    "            W : torch of size q. should be a gaussian. \n",
    "            \n",
    "        return : the log of the probability of (W,Y_i) given Y_i. \n",
    "        '''\n",
    "        A_i = O_i + W.reshape(1,-1)@(self.C.T) +covariates_i@self.beta # temporary variable, for a clearer code. \n",
    "        return -1/2*torch.norm(W)**2 + torch.sum(-torch.exp(A_i)+W.reshape(1,-1)@(self.C.T)*Y_i)\n",
    "                    \n",
    "    def p_u_theta(self, Y_i, covariates_i, O_i, W_i): \n",
    "        '''\n",
    "        computes the unormalized density for one W_i of size (q)\n",
    "        '''\n",
    "        A_i = torch.exp(O_i + self.C@W_i +covariates_i@self.beta)\n",
    "        return torch.exp(-torch.norm(W_i)**2/2 -torch.sum(A_i)+ Y_i.double()@self.C@W_i)\n",
    "            \n",
    "    def personalized_batch_log_f(self,Y_b,covariates_b,O_b,dict_V_b):\n",
    "        '''\n",
    "        computes the log of f where f is the unormalized density of interest. \n",
    "        for each value V_b in dict_V_b, it computes the log density f. See the formula above\n",
    "        for more details. The values V_b has means that are personalized for each coordinate in the integral\n",
    "        (i.e. we change the importance law for each coordinate in the integral) to have a lower variance. \n",
    "        args : \n",
    "              Y_b : batch of Y_i : torch.tensor of size (batch_size, p)\n",
    "              covariates_b : batch of covariates__i : torch.tensor of size (batch_size, p)\n",
    "              O_b : batch of O_i : torch.tensor of size (batch_size, p)\n",
    "              dict_V_b : dict. key : 'beta', 'C_exp', 'C_W'. for each key, the corresponding values\n",
    "                        are gaussians of size (N_samples, self.batch_size, (integral size), q). \n",
    "        returns: \n",
    "                a dict with the same keys but the values are the log density of f of the given value. \n",
    "        '''\n",
    "        log_f = {}\n",
    "        # some terms can be computed before so that we don't compute them two times.\n",
    "        OplusXB = (O_b + covariates_b@self.beta)\n",
    "        YC = (Y_b.double()@self.C)\n",
    "        \n",
    "        for name, V_b in dict_V_b.items():\n",
    "            #C_exp integral\n",
    "            if len(V_b.shape)== 5:\n",
    "                A_b = torch.exp(OplusXB.unsqueeze(0).unsqueeze(2).unsqueeze(2) + torch.matmul(V_b, self.C.T))\n",
    "                big_term = torch.matmul(V_b, YC.unsqueeze(0).unsqueeze(2).unsqueeze(4)).squeeze()\n",
    "                ###### a enlever, here only for q = 1. \n",
    "                big_term = big_term.reshape(big_term.shape[0], big_term.shape[1],big_term.shape[2], self.q) \n",
    "            # C_W and beta integrals\n",
    "            elif len(V_b.shape) == 4 :\n",
    "                A_b = torch.exp(OplusXB.unsqueeze(0).unsqueeze(2) + torch.matmul(V_b, self.C.T))\n",
    "                big_term = torch.matmul(V_b, YC.unsqueeze(0).unsqueeze(3)).squeeze()\n",
    "            if len(big_term.shape) == 2: \n",
    "                big_term = big_term.unsqueeze(2)\n",
    "            log_f[name] = -1/2*torch.norm(V_b, dim = -1)**2 -torch.sum(A_b, dim = -1) + big_term\n",
    "        return log_f \n",
    "        \n",
    "    \n",
    "    def get_batch_weights(self, dict_log_f, dict_log_g):\n",
    "        '''\n",
    "        computes a batch of the weights from batches of log_f and log_g. More efficient function \n",
    "        than get_weights or get_weights_m. We compute it for a dict just for a clearer code \n",
    "        (for the 3 integrals).\n",
    "        args : \n",
    "              log_f : dict of torch.tensor of size (N_samples, batch_size)\n",
    "              log_g : dict of torch.tensor of size (N_samples, batch_size)\n",
    "        returns : the weights, dict of torch.tensor of size (N_samples, batch_size)\n",
    "        '''\n",
    "        weights = {}\n",
    "        for name,log_f in dict_log_f.items():\n",
    "            log_weights = log_f-dict_log_g[name]\n",
    "            tmp = torch.exp(log_weights -torch.max(log_weights, axis = 0 )[0])\n",
    "            weights[name] = tmp / torch.sum(tmp, axis = 0) # normalization\n",
    "        return weights\n",
    "\n",
    "\n",
    "    def show_Sigma(self):\n",
    "        '''\n",
    "        Simple function that shows Sigma with the heatmap method of seaborn. \n",
    "        '''\n",
    "        sns.heatmap(((self.C_mean)@(self.C_mean.T)).detach().numpy())\n",
    "        plt.show()\n",
    "        \n",
    "    def average_params(self,log_like): \n",
    "        '''\n",
    "        method that averages the parameters in order to smooth the variance. \n",
    "        We will take, for example, the last self.average betas computed to make \n",
    "        a better approximation of beta. We will do the same for C.\n",
    "        This function adds the last parameters computed to update the mean of the parameter. \n",
    "        If we have not enough betas or Sigmas (less than self.average), the mean will be on those parameters. \n",
    "        args : \n",
    "            log_like : the likelihood computed with the current parameters.\n",
    "        \n",
    "        returns : \n",
    "                None but update the mean of the last self.average parameters.  \n",
    "        '''\n",
    "        \n",
    "        self.cmpt +=1 # to keep track of the number of parameters we have for the mean\n",
    "        # remove the oldest parameters and add the more recent one.\n",
    "        self.last_betas[1:self.average] = torch.clone(self.last_betas[0: self.average-1].detach())\n",
    "        self.last_betas[0] = torch.clone(self.beta.detach())\n",
    "        self.last_Cs[1:self.average] = torch.clone(self.last_Cs[0: self.average-1].detach())\n",
    "        self.last_Cs[0] = torch.clone(self.C.detach())\n",
    "        self.last_likelihoods.append(log_like)\n",
    "        # update the mean of the parameter\n",
    "        \n",
    "        # if we have enough parameters \n",
    "        if self.cmpt > self.average : \n",
    "            del(self.last_likelihoods[0])\n",
    "            self.C_mean = torch.mean(self.last_Cs, axis = 0)\n",
    "            self.beta_mean = torch.mean(self.last_betas, axis = 0)\n",
    "            return np.mean(np.array(self.last_likelihoods))\n",
    "        \n",
    "        # if we don't have enough parameters. \n",
    "        else : \n",
    "            self.C_mean = torch.sum(self.last_Cs, axis = 0)/self.cmpt\n",
    "            self.beta_mean = torch.sum(self.last_betas, axis = 0)/self.cmpt\n",
    "            return np.mean(np.array(self.last_likelihoods))\n",
    "    def keep_records(self,log_like): \n",
    "        '''\n",
    "        function that keep some records in order to plot the evolution after.\n",
    "        '''\n",
    "        average_log_like = self.average_params(log_like)\n",
    "        # keep the records of the norm of the gradients and the MSE wrt the true parameter\n",
    "        self.norm_grad_log_beta_list.append((torch.norm(self.beta.grad)/(self.d*self.p)).item())\n",
    "        self.norm_grad_log_C_list.append((torch.norm(self.C.grad)/(self.p*self.q)).item())\n",
    "        self.MSE_beta_list.append(torch.mean((self.beta_mean -true_beta)**2).item())\n",
    "        self.MSE_Sigma_list.append(torch.mean((self.C_mean@(self.C_mean.T) -true_Sigma)**2).item())\n",
    "        self.running_times.append(time.time()-self.t0)\n",
    "        #print('average log_like : ', average_log_like)\n",
    "        self.log_likelihood_list.append(average_log_like)\n",
    "        \n",
    "    def fit_torch(self,Y, O, covariates,  N_iter, acc,lr,class_optimizer = torch.optim.Rprop ):\n",
    "        '''\n",
    "        other function to fit the model that uses only torch.\n",
    "        Seems to not work, but interesting since \n",
    "        it finds better maxima for some values. We will need to compare this with other methods. Trouble for the beginning\n",
    "        since the likelihood is not computable for any sample ( at least for the beginnig, not at convergence)\n",
    "        but trouble at convergence and not at the beginning ! (Why ? ) \n",
    "        '''\n",
    "        self.t0= time.time()# to keep track of the time. \n",
    "        self.acc = acc\n",
    "        self.init_data(Y,O, covariates)# initialize the data. \n",
    "        optim = class_optimizer([self.beta,self.C], lr = lr) # optimizer on C and beta\n",
    "        optim.zero_grad() # We do this since it is possible that beta and C have gradients. \n",
    "        N_samples = int(1/acc) # We will sample 1/acc gaussians\n",
    "        for i in range(N_iter):\n",
    "            for (Y_b, covariates_b, O_b) in self.get_batch(self.batch_size):\n",
    "                loss = -self.vectorized_log_likelihood(Y_b,covariates_b,O_b,  torch.randn(10,self.batch_size, self.q),verbose = False)\n",
    "                loss.backward()\n",
    "                self.keep_records(-loss.item())\n",
    "                optim.step()\n",
    "                optim.zero_grad()\n",
    "                #print('beta_MSE : ', torch.mean((self.beta-true_beta)**2).item())\n",
    "                #print('----------------------------Sigma_MSE', torch.mean((self.get_Sigma()-true_Sigma)**2).item())\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "    def sample_g(self,N_samples, mu_prop,var_prop):\n",
    "        '''\n",
    "        function that samples some gaussians with the right mean and the right dimensions. \n",
    "\n",
    "        arg : \n",
    "            N_samples : int. the number of samples you want. For a MC approximation, the larger N_samples, \n",
    "                        the better the approximation. \n",
    "            mu_prop : torch.tensor of size (batch_size, p,q,q) or (batch_size,p,q) or (batch_size,q,q). It depends\n",
    "                      on the dimension of the integral you want to compute. \n",
    "            var_prop : float positive. the variance you want. It will be shared along all the samples. \n",
    "                    We can easily make one variance for each (the dimension can be of the same dimension as mu_prop)\n",
    "        returns: \n",
    "            a gaussian with mean mu_prop and variance var_prop. The dimension will be : (N_samples, mu_prop.shape). \n",
    "        '''\n",
    "        # case where we whant to integrate a vector of integrals\n",
    "        gaussians = torch.randn(N_samples,self.batch_size,1,self.q)\n",
    "        #gaussians = torch.randn(N_samples, 1,1,1)\n",
    "        #gaussians2 = torch.randn(N_samples,self.batch_size,1,self.q)*var_prop\n",
    "        #gaussians3 = torch.randn(N_samples,self.batch_size,1,self.q)*var_prop\n",
    "        return gaussians*self.var['beta'] + mu_prop['beta'].unsqueeze(0), (gaussians*self.var['C_exp']).unsqueeze(3) + mu_prop['C_exp'].unsqueeze(0),gaussians*self.var['C_W'] + mu_prop['C_W'].unsqueeze(0)\n",
    "\n",
    "    def personalized_log_P_WgivenY_single(self,Y_b,covariates_b,O_b,W): \n",
    "        ## W of size (batch_size,p,q) or (batch_size, q, q)\n",
    "        #CW = torch.matmul(W.unsqueeze(2), self.C.unsqueeze(0).unsqueeze(3)).squeeze()\n",
    "        CW_inside = torch.matmul( W, self.C.T.unsqueeze(0))\n",
    "        ## unsqueeze the second dimensions (try to replace the dimension p of W by any number, you'll see)\n",
    "        exp_term = torch.exp(O_b.unsqueeze(1) + CW_inside+(covariates_b@self.beta).unsqueeze(1))\n",
    "        #careful here ! we take the sum over the axis -1. just replace the p of W to any number to find that \n",
    "        #it is the right ax to sum over. \n",
    "        '''\n",
    "        if torch.isnan(exp_term).any().item() : \n",
    "            print('beta', self.beta)\n",
    "            print('C : ', self.C)\n",
    "            print('exp term : ', exp_term) \n",
    "        '''\n",
    "        #else : print('ok')\n",
    "        return -1/2*torch.norm(W,dim = -1)**2+ torch.sum(-exp_term+CW_inside*(Y_b.unsqueeze(1)), axis = -1)\n",
    "    \n",
    "    def personalized_log_P_WgivenY_double(self, Y_b, covariates_b, O_b, W):\n",
    "        # W of size (batch_size,p,q,q)\n",
    "        CW_inside = torch.matmul(W, self.C.T.unsqueeze(0).unsqueeze(0))\n",
    "        exp_term = torch.exp((O_b+covariates_b@self.beta).unsqueeze(1).unsqueeze(1)+CW_inside)\n",
    "        return -1/2*torch.norm(W,dim = -1)**2+ torch.sum(-exp_term+CW_inside*(Y_b.unsqueeze(1).unsqueeze(1)), axis = -1)\n",
    "        \n",
    "\n",
    "    \n",
    "    \n",
    "    def personalized_batch_MC_term(self, Y_b,covariates_b, O_b,V_b, mu_prop, var_prop):\n",
    "        '''\n",
    "        does the same as batch_MC_term but here we take v_b as a dict that contains the\n",
    "        gaussians for each integrals (personalized fashion). The major difference with this approach\n",
    "        is the dimension of V_b, that is (N_samples, N_batch, (integral size), q) as opposed to \n",
    "        batch_MC_term : dimension is (N_samples, batch_size,q) where we don't try to have an importance \n",
    "        law that fits the data. \n",
    "        \n",
    "              Y_b : batch of Y_i : torch.tensor of size (batch_size, p)\n",
    "              covariates_b : batch of covariates__i : torch.tensor of size (batch_size, p)\n",
    "              O_b : batch of O_i : torch.tensor of size (batch_size, p)\n",
    "              V_b : dict. each key contains a gaussian \n",
    "                  of size (N_samples, batch_size, (integral size), q)\n",
    "                  where integral is the integral the key refers to.\n",
    "            \n",
    "        returns : the  3 integrals we want. \n",
    "            tuple(torch.tensor of size (n_batch,p), torch.tensor of size (n_batch,q), \n",
    "            torch.tensor of size (n_batch, p,q))\n",
    "        '''\n",
    "        # for more details on the formula, see above.\n",
    "        # get the log of both densities\n",
    "        log_f = self.personalized_batch_log_f(Y_b,covariates_b,O_b,V_b)\n",
    "        \n",
    "        log_g = dict_log_density_g(V_b,mu_prop,self.var)\n",
    "        #get the normalized weights\n",
    "        weights = self.get_batch_weights(log_f,log_g)\n",
    "        # next we compute the integrals. the formula are a little bit tricky (especially\n",
    "        # because we added the integral size at the shape of V_b), print the shape of each \n",
    "        # matrice helps for comprehension\n",
    "        # compute the phi for the integral of beta and the importance sampling formula\n",
    "        phi_beta  = torch.exp(torch.matmul(V_b['beta'].unsqueeze(3), self.C.reshape(1,1,self.p,self.q,1))).squeeze()\n",
    "        inside_beta = torch.mul(weights['beta'], phi_beta)\n",
    "        IMPS_beta = torch.sum(inside_beta, axis = 0)\n",
    "        \n",
    "        # same for C_exp\n",
    "        CtimesV = torch.matmul(V_b['C_exp'].unsqueeze(4), self.C.unsqueeze(0).unsqueeze(1).unsqueeze(3).unsqueeze(5)).squeeze()\n",
    "        canonicalV =  torch.matmul(V_b['C_exp'].unsqueeze(4),torch.eye(self.q).reshape(1,1,1,self.q,self.q,1)).squeeze()\n",
    "        phi_C_exp = torch.multiply(canonicalV,torch.exp(CtimesV))\n",
    "        if len(phi_C_exp.shape) == 3 : \n",
    "            phi_C_exp = phi_C_exp.unsqueeze(3)\n",
    "        inside_C_exp = torch.mul(weights['C_exp'], phi_C_exp)\n",
    "\n",
    "        \n",
    "        #first =  torch.exp(torch.matmul(self.C, V_b['C_exp'][s,n_b,k,l].unsqueeze(1)))\n",
    "        #second  = V_b['C_exp'][s,n_b,k,l].unsqueeze(0)\n",
    "        IMPS_C_exp = torch.sum(inside_C_exp, axis = 0 )\n",
    "        \n",
    "        #same for C_W\n",
    "        canonicalW = torch.matmul(V_b['C_W'].unsqueeze(3), torch.eye(self.q).reshape(1,1,self.q,self.q,1)).squeeze()\n",
    "        if len(canonicalW.shape) == 2 : \n",
    "            canonicalW = canonicalW.unsqueeze(2)\n",
    "        inside_C_W = torch.mul(weights['C_W'], canonicalW)\n",
    "        IMPS_C_W = torch.sum(inside_C_W, axis = 0)\n",
    "        \n",
    "        #keep some records for the var\n",
    "        self.keep_var_records(weights)\n",
    "        return IMPS_beta, IMPS_C_W, IMPS_C_exp\n",
    "    \n",
    "    \n",
    "\n",
    "    #def find_each_mean(self, Y_b, covariates_b, O_b,lr = 0.5, N_iter_max = 100,class_optimizer = torch.optim.Rprop):\n",
    "    #    W_beta = torch.randn    \n",
    "    \n",
    "    \n",
    "    def get_conditional_density_j(self,j,k,Y_i,covariates_i,O_i,best_mean_i, N_points, l = 0): \n",
    "        # l is here only if the choice is C_exp\n",
    "        choice,best_mean = get_choice_and_mean(best_mean_i,k,l,self.p,self.q)\n",
    "        \n",
    "        mask = torch.zeros(best_mean.shape[0], dtype = torch.bool)\n",
    "        mask[j]= True\n",
    "        \n",
    "        def target(W_1D):\n",
    "            W = best_mean.masked_scatter(mask,torch.Tensor([W_1D])).detach()\n",
    "            if choice == 'C_exp' : \n",
    "                phi = torch.matmul(torch.exp(torch.matmul(self.C, W.unsqueeze(1))),W.unsqueeze(0))[k,l]\n",
    "            if choice == 'C_W' : \n",
    "                phi = torch.matmul(self.C[k], W)\n",
    "            if choice == 'beta': \n",
    "                phi = torch.exp(torch.matmul(self.C[k], W))\n",
    "            return phi, self.log_P_WgivenY(Y_i,covariates_i,O_i,W)\n",
    "        \n",
    "        abscisse = np.linspace(best_mean[j].detach().numpy()-5,best_mean[j].detach().numpy()+5,N_points*2)\n",
    "        list_log = []\n",
    "        list_phi = []\n",
    "        for w in abscisse :\n",
    "            phi,log_P = target(w)\n",
    "            list_phi.append(phi)\n",
    "            list_log.append(log_P)\n",
    "        arr_log = np.array(list_log)\n",
    "        arr_log-= np.max(list_log)\n",
    "        list_density = np.array(list_phi)*np.exp(arr_log)\n",
    "        return abscisse,list_density\n",
    "    \n",
    "    def plot_conditional(self, Y_i, covariates_i, O_i, gaussian_mean_i, nb_to_plot, N_points = 100,l=0):\n",
    "        fig = plt.figure(figsize = (10,10))\n",
    "        axes = fig.subplots(nb_to_plot, self.q)\n",
    "        choice,_ = get_choice_and_mean(gaussian_mean_i, 0,0,self.p,self.q)\n",
    "        \n",
    "        for k in range(nb_to_plot): \n",
    "            for j in range(self.q): \n",
    "                current_mean = gaussian_mean_i[k]\n",
    "                if choice == 'C_exp' : \n",
    "                    current_mean = current_mean[l]\n",
    "                abscisse, density = self.get_conditional_density_j(j,k,Y_i,covariates_i,O_i,gaussian_mean_i,N_points)\n",
    "                true_max = abscisse[np.argmax(np.abs(density))]\n",
    "                normal_density = np.exp(np.array([-1/(2*self.var[choice]**2)*(w-current_mean[j].item())**2 for w in abscisse]))\n",
    "                normal_density*= np.max(np.abs(density))/np.max(normal_density)#/np.max(normal_density)# We renormalize by some constants to plot similar things. \n",
    "                axes[k,j].plot(abscisse, density, label = 'W|Y density')\n",
    "                axes[k,j].plot(abscisse, normal_density, label = 'Gaussian density')\n",
    "                axes[k,j].axvline(current_mean[j], c = 'green', label = 'maximum found')# the max we found\n",
    "                axes[k,j].axvline(true_max, c = 'black', label = 'true_max')# the true max. \n",
    "                #except : \n",
    "                #    pass\n",
    "                \n",
    "        # We display the labels above and once. \n",
    "        lines, labels = fig.axes[0].get_legend_handles_labels()\n",
    "        fig.suptitle(choice)\n",
    "        fig.legend(lines, labels, loc = 'upper right')\n",
    "        plt.show()\n",
    "    \n",
    "    \n",
    "    \n",
    "    def get_batch_grads(self, Y_b, covariates_b, O_b, esp_beta, esp_C_W, esp_C_exp):\n",
    "        '''\n",
    "        get the gradients for a batch thanks to the integrals we computed before. for the formula, \n",
    "        see above.\n",
    "        args : \n",
    "              Y_b : torch.tensor of size (n_batch,p)\n",
    "              covariates_b : torch.tensor of size (n_batch,p)\n",
    "              O_b : torch.tensor of size (n_batch,p)\n",
    "              esp_beta : p-dimensional integral we computed, torch.tensor of size(n_batch, p)\n",
    "              esp_C_W :  q-dimensional integral we computed, torch.tensor of size (n_batch,q)\n",
    "              esp_C_exp :(p,q) dimensional integral we computed, torch.tensor of size(n_batch,p,q)   \n",
    "        returns : the gradients wrt beta and C \n",
    "        '''\n",
    "        # for details on the formulas, see above. here we take -grad so that we can plug \n",
    "        # the gradiens in a pytorch optimizer \n",
    "        beta_grad = -torch.bmm(covariates_b.unsqueeze(2), (Y_b - torch.mul(torch.exp(O_b + torch.mm(covariates_b, self.beta)),esp_beta)).unsqueeze(1))\n",
    "        first_term = torch.matmul(Y_b.float().unsqueeze(2), esp_C_W.float().unsqueeze(1))\n",
    "        C_grad = -first_term+ torch.multiply(esp_C_exp, torch.exp(O_b + covariates_b@self.beta).unsqueeze(2))\n",
    "        return beta_grad, C_grad\n",
    "    \n",
    "    def get_true_esp_C_exp_2D(self, Y_i, covariates_i, O_i):\n",
    "        '''\n",
    "        function that get true integral of C_exp. We use numerical integration from scipy\n",
    "        to computes this. Note that this may work only if the Y_i is low. Scipy can't \n",
    "        compute it if it is two large. \n",
    "        '''\n",
    "        \n",
    "        def p_u(x,y): \n",
    "            W_i = torch.tensor([x,y])\n",
    "            return self.p_u_theta(Y_i, covariates_i, O_i,W_i)\n",
    "        normalizer,res_norm = integrate.dblquad(p_u, -10,10,-10,10)\n",
    "        true_esp = torch.empty((p,q))\n",
    "        for k in tqdm(range(self.p)):\n",
    "            for l in range(self.q):\n",
    "                def integrande_kl(x,y):\n",
    "                    W = torch.tensor([x,y])\n",
    "                    res = p_u(x,y)* torch.exp(((self.C@(W))[k]))*W[l]/normalizer\n",
    "                    if torch.abs(res)> -0.0001 : \n",
    "                        print(' x :', x)\n",
    "                        print('y :', y )\n",
    "                        print('res :' ,res)\n",
    "                    return res\n",
    "                true_int, res = integrate.dblquad(integrande_kl, -10,10,-10,10)\n",
    "                true_esp[k,l] = true_int \n",
    "        return true_esp\n",
    "    \n",
    "    def get_true_esp_C_exp_1D(self, Y_i, covariates_i,O_i):\n",
    "        def p_u(x): \n",
    "            W_i = torch.tensor([x])\n",
    "            return self.p_u_theta(Y_i, covariates_i, O_i,W_i)\n",
    "        normalizer,res_norm = integrate.quad(p_u, -10,10)\n",
    "        true_esp_C_exp = torch.empty((p,q))\n",
    "        true_esp_C_W = torch.empty(self.q)\n",
    "        true_esp_beta = torch.empty(self.p)\n",
    "        \n",
    "        for k in range(self.p):\n",
    "            def integrande_beta(x): \n",
    "                W = torch.tensor([x])\n",
    "                return p_u(x)*torch.exp((self.C@(W))[k])/normalizer\n",
    "            def integrande_C_W(x): \n",
    "                W = torch.tensor([x])\n",
    "                return p_u(x)*W[k]/normalizer\n",
    "            \n",
    "            true_esp_beta[k],_ = integrate.quad(integrande_beta, -10,10)\n",
    "            if k < self.q : \n",
    "                true_esp_C_W[k],_ = integrate.quad(integrande_C_W, -10,10)\n",
    "            \n",
    "            for l in range(self.q):\n",
    "                def integrande_kl(x):\n",
    "                    W = torch.tensor([x])\n",
    "                    res = p_u(x)* torch.exp(((self.C@(W))[k]))*W[l]/normalizer\n",
    "                    return res\n",
    "                true_esp_C_exp[k,l],_ = integrate.quad(integrande_kl, -10,10)\n",
    "        return true_esp_beta, true_esp_C_W, true_esp_C_exp\n",
    "    \n",
    "\n",
    "    def find_each_mean(self,Y_b,covariates_b, O_b, lr = 0.5, N_iter_max = 100,class_optimizer = torch.optim.Rprop):\n",
    "        '''\n",
    "        method to compute the mean of the best gaussian approximation of W|Y_b. We will compute n_batch * q \n",
    "        approximation. \n",
    "        \n",
    "        args : \n",
    "            Y_b : tensor of size (n_batch,p) \n",
    "            covariates_b : tensor of size (n_batch,d) \n",
    "            O_b : tensor of size (n_batch,p) \n",
    "            lr : float. positive. the learning rate of the optimizer\n",
    "            N_iter_max : the number of iteration you want to do. \n",
    "            class_optimizer : torch.optim.optimizer object.  the class of the optimizer. should be like \n",
    "                              torch.optim.Rprop. \n",
    "        returns : \n",
    "                W : the parameter that maximizes the log of the probability of P_WY given Y.\n",
    "        '''\n",
    "        \n",
    "        d_W = {}\n",
    "        d_W['beta'] = torch.randn(self.batch_size,self.p,self.q,requires_grad = True)\n",
    "        d_W['C_W'] = torch.randn(self.batch_size,self.q,self.q, requires_grad = True)\n",
    "        d_W['C_exp'] = torch.randn(self.batch_size,self.p, self.q, self.q, requires_grad = True)\n",
    "        def beta_loss(W): \n",
    "            log_density =  self.personalized_log_P_WgivenY_single(Y_b,covariates_b, O_b, W)\n",
    "            CW_outside = torch.matmul(W.unsqueeze(2),self.C.unsqueeze(0).unsqueeze(3)).squeeze() \n",
    "            return CW_outside+log_density\n",
    "        def C_W_loss(W): \n",
    "            log_density =  self.personalized_log_P_WgivenY_single(Y_b,covariates_b, O_b, W)\n",
    "            W_outside = torch.diagonal(W, dim1 = -2, dim2= -1)\n",
    "            return torch.log(torch.abs(W_outside))+ log_density  \n",
    "        def C_exp_loss(W): \n",
    "            log_density =  self.personalized_log_P_WgivenY_double(Y_b,covariates_b, O_b, W)\n",
    "            exp_term = torch.exp(torch.matmul(W.unsqueeze(3),self.C.unsqueeze(0).unsqueeze(2).unsqueeze(4))).squeeze()\n",
    "            canonicalW = torch.matmul(W.unsqueeze(3),torch.eye(self.q).reshape(1,1,self.q,self.q,1)).squeeze()\n",
    "            final = torch.multiply(exp_term, canonicalW)\n",
    "            final = final.reshape(final.shape[0],final.shape[1], self.q) ########enlever, juste pour q=1. \n",
    "            return torch.log(torch.abs(final))+ log_density \n",
    "                             \n",
    "        loss = {'beta': beta_loss, 'C_W': C_W_loss, 'C_exp': C_exp_loss}\n",
    "        optimizer = {'beta':  class_optimizer([d_W['beta']], lr = lr),\n",
    "                     'C_W': class_optimizer([d_W['C_W']], lr = lr),\n",
    "                    'C_exp': class_optimizer([d_W['C_exp']], lr = lr)}\n",
    "        i = 0                              \n",
    "        while i < N_iter_max:  #and delta > tol :\n",
    "            for  name,W in d_W.items(): \n",
    "                 ((loss[name](W).mean())*(-1)).backward()\n",
    "    \n",
    "                 optimizer[name].step()\n",
    "                 optimizer[name].zero_grad()\n",
    "                 i+=1\n",
    "        return d_W\n",
    "    \n",
    "    def find_each_mean_1d(self,Y_b,covariates_b, O_b, lr = 0.5, N_iter_max = 100,class_optimizer = torch.optim.Rprop): \n",
    "        d_W = {}\n",
    "        d_W['beta'] = torch.randn(self.batch_size,self.q,requires_grad = True)\n",
    "        d_W['C_W'] = torch.randn(self.batch_size,self.q, requires_grad = True)\n",
    "        d_W['C_exp'] = torch.randn(self.batch_size,self.q, requires_grad = True)\n",
    "        def beta_loss(W): \n",
    "            log_density =  self.each_log_P_WgivenY(Y_b,covariates_b, O_b, W)\n",
    "            CW = torch.matmul(self.C[0].reshape(1,1,self.q), W.unsqueeze(2)).squeeze()\n",
    "            #print('CW_outside shape', CW.shape)\n",
    "            #print('log_density shape', log_density.shape)\n",
    "            return CW + log_density  \n",
    "            CW_outside = torch.matmul(W.unsqueeze(2),self.C.unsqueeze(0).unsqueeze(3)).squeeze() \n",
    "\n",
    "            return CW_outside+log_density\n",
    "        def C_W_loss(W): \n",
    "            log_density =  self.each_log_P_WgivenY(Y_b,covariates_b, O_b, W)\n",
    "            W_outside = W[:, 0 ] \n",
    "            return torch.log(torch.abs(W_outside))+ log_density  \n",
    "        def C_exp_loss(W): \n",
    "            log_density =  self.each_log_P_WgivenY(Y_b,covariates_b, O_b, W)\n",
    "            exp_term = torch.exp(torch.matmul(self.C[0].reshape(1,1,self.q), W.unsqueeze(2)).squeeze())\n",
    "            canonical_W = W[:,0]\n",
    "            return torch.log(torch.abs(torch.multiply(exp_term,canonical_W))) + log_density\n",
    "                             \n",
    "        loss = {'beta': beta_loss, 'C_W': C_W_loss, 'C_exp': C_exp_loss}\n",
    "        optimizer = {'beta':  class_optimizer([d_W['beta']], lr = lr),\n",
    "                     'C_W': class_optimizer([d_W['C_W']], lr = lr),\n",
    "                    'C_exp': class_optimizer([d_W['C_exp']], lr = lr)}\n",
    "        i = 0                              \n",
    "        while i < N_iter_max:  #and delta > tol :\n",
    "            for  name,W in d_W.items(): \n",
    "                 ((loss[name](W).mean())*(-1)).backward()\n",
    "    \n",
    "                 optimizer[name].step()\n",
    "                 optimizer[name].zero_grad()\n",
    "                 i+=1\n",
    "        return d_W\n",
    "\n",
    "    \n",
    "    \n",
    "    def get_error_reduction(self, mu_b,mu_b_single): \n",
    "        mean = {}\n",
    "        var = {}\n",
    "        max_ = {}\n",
    "        for (key,mu),mu_single in zip(mu_b.items(),mu_b_single.values()):\n",
    "            tl = mu_single.unsqueeze(1)\n",
    "            if key == 'C_exp': \n",
    "                tl = tl.unsqueeze(1)\n",
    "            diff = torch.abs(tl-mu)\n",
    "            mean[key] = np.round(torch.mean(diff).item(),3)\n",
    "            var[key] = np.round(torch.var(diff).item(),3)\n",
    "            max_[key] = np.round(torch.max(diff).item(),3)\n",
    "            argmax = torch.argmax(diff)\n",
    "            #print(key, argmax)\n",
    "        return mean,var,max_\n",
    "    \n",
    "    \n",
    "    def batch_MC_term(self, Y_b,covariates_b, O_b,V_b,mu_b):\n",
    "        # for more details on the formula, see above.\n",
    "        log_f = self.batch_log_f(Y_b,covariates_b, O_b, V_b)\n",
    "        log_g = self.batch_log_g(V_b,mu_b)\n",
    "        weights = self.get_batch_weights({'':log_f},{'' :log_g})['']\n",
    "        \n",
    "        phi_beta  = torch.exp(torch.matmul(V_b, self.C.T))\n",
    "        inside_beta = torch.mul(weights.unsqueeze(2), phi_beta)\n",
    "        IMPS_beta = torch.sum(inside_beta, axis = 0)\n",
    "        inside_C_W = torch.mul(weights.unsqueeze(2), V_b)\n",
    "        IMPS_C_W = torch.sum(inside_C_W, axis = 0)\n",
    "        phi_exp_C = torch.matmul(torch.exp(torch.matmul(V_b, self.C.T)).unsqueeze(3),V_b.unsqueeze(2))\n",
    "        inside_exp_C = torch.mul(weights.unsqueeze(2).unsqueeze(3), phi_exp_C)\n",
    "        IMPS_C_exp = torch.sum(inside_exp_C, axis = 0 )\n",
    "        \n",
    "        var = torch.var(weights, axis = 0)/self.acc\n",
    "        self.var_weights_bis = np.concatenate((self.var_weights_bis, var.detach().numpy()))\n",
    "        return IMPS_beta, IMPS_C_W, IMPS_C_exp\n",
    "    \n",
    "    def single_batch_MC_term(self, Y_b,covariates_b, O_b, V_b, mu_b): \n",
    "        '''\n",
    "        does the same as MC_term but for a batch for a more efficient computation.\n",
    "              Y_b : batch of Y_i : torch.tensor of size (batch_size, p)\n",
    "              covariates_b : batch of covariates__i : torch.tensor of size (batch_size, p)\n",
    "              O_b : batch of O_i : torch.tensor of size (batch_size, p)\n",
    "              V_b : the gaussian : torch.tensor of size (N_samples, N_bathc,q)\n",
    "            \n",
    "        returns : the  3 integrals we want. \n",
    "            tuple(torch.tensor of size (n_batch,p), torch.tensor of size (n_batch,q), \n",
    "            torch.tensor of size (n_batch, p,q))\n",
    "        '''\n",
    "        log_f = self.batch_log_f(Y_b,covariates_b, O_b, V_b)\n",
    "        log_g = self.batch_log_g(V_b,mu_b)\n",
    "        \n",
    "        weights = self.get_batch_weights(log_f,log_g)\n",
    "        \n",
    "        phi_beta  = torch.exp(torch.matmul(V_b['beta'], self.C.T))\n",
    "        inside_beta = torch.mul(weights['beta'].unsqueeze(2), phi_beta)\n",
    "        IMPS_beta = torch.sum(inside_beta, axis = 0)\n",
    "        \n",
    "        inside_C_W = torch.mul(weights['C_W'].unsqueeze(2), V_b['C_W'])\n",
    "        IMPS_C_W = torch.sum(inside_C_W, axis = 0)\n",
    "        \n",
    "        phi_exp_C = torch.matmul(torch.exp(torch.matmul(V_b['C_exp'], self.C.T)).unsqueeze(3),V_b['C_exp'].unsqueeze(2))\n",
    "        inside_exp_C = torch.mul(weights['C_exp'].unsqueeze(2).unsqueeze(3), phi_exp_C)\n",
    "        IMPS_C_exp = torch.sum(inside_exp_C, axis = 0 )\n",
    "        \n",
    "        self.keep_single_var_records(weights)\n",
    "        \n",
    "        return IMPS_beta, IMPS_C_W, IMPS_C_exp\n",
    "    \n",
    "    def keep_single_var_records(self, dict_weights): \n",
    "        for name, weights in dict_weights.items():\n",
    "            var = torch.var(weights, axis = 0 )/self.acc\n",
    "            self.var_weights[name] = np.concatenate((self.var_weights[name], torch.nan_to_num(var, nan = 1).detach().numpy())) \n",
    "        \n",
    "        \n",
    "        \n",
    "    def keep_var_records(self, dict_weights):\n",
    "        '''\n",
    "        small function that keep records of the variance of the weights. \n",
    "        for each value of the dict, we have a variance of size (integral size), so we take the \n",
    "        mean along those axis to plot it. \n",
    "        args : \n",
    "              dict_weights : dict of weights (only to have a clearer code).\n",
    "                            each key represent the name of the integral \n",
    "                            the weights were computed for, each value is the \n",
    "                            corresponding weights. \n",
    "        return : None but keep records of the variance. \n",
    "        '''\n",
    "        # we browse the dict\n",
    "        for name,weights in dict_weights.items():\n",
    "            # we don't take the mean for the same axis for C_exp or the other, we \n",
    "            # need to have a separate case\n",
    "            if name == 'C_exp' : \n",
    "                var = torch.mean(torch.var(weights, axis = 0)/self.acc, axis = (-1,-2))\n",
    "            else : \n",
    "                var = torch.mean(torch.var(weights, axis = 0)/self.acc, axis = -1)\n",
    "            # concatenate the weights. \n",
    "            self.var_weights[name] = np.concatenate((self.var_weights[name], torch.nan_to_num(var, nan = 1).detach().numpy()))\n",
    "            \n",
    "    \n",
    "    \n",
    "    def batch_log_g(self, dict_V_b, dict_mu_b): \n",
    "        '''\n",
    "        computes the  likelihood of the density from which we can sample : g. we do this for a batch \n",
    "        for more efficient calculus.We take \n",
    "        args : \n",
    "              V_b : the gaussian, torch.tensor of size (N_samples, batch_size, q)\n",
    "              \n",
    "        return : the density for each sample in each bach : torch.tensor of size (N_samples, batch_size)\n",
    "        '''\n",
    "        log_g = {}\n",
    "        for (name,V_b), mu_b in zip(dict_V_b.items(), dict_mu_b.values()):\n",
    "            log_g[name] = - 1/(2*self.var[name])*torch.norm(V_b-mu_b, dim = 2)**2\n",
    "        return log_g\n",
    "\n",
    "    def batch_log_f(self, Y_b, covariates_b, O_b, dict_V_b): \n",
    "        '''\n",
    "        computes the log of the density of f, the density from which we can't sample. takes a batch for\n",
    "        efficient calculus. \n",
    "        \n",
    "        args : \n",
    "              Y_b : batch of Y_i : torch.tensor of size (batch_size, p)\n",
    "              covariates_b : batch of covariates__i : torch.tensor of size (batch_size, p)\n",
    "              O_b : batch of O_i : torch.tensor of size (batch_size, p)\n",
    "              V_b : the gaussian : torch.tensor of size (N_samples, N_bathc,q)\n",
    "            \n",
    "        returns : the log of f for each sample and each variable in the batch: torch.tensor of size \n",
    "        (N_samples, batch_size)\n",
    "        '''\n",
    "        log_f = {}\n",
    "        OplusXB = (O_b + covariates_b@self.beta).unsqueeze(0)\n",
    "        YC = (Y_b.double()@self.C).unsqueeze(2)\n",
    "        for name,V_b in dict_V_b.items():\n",
    "            A_b = torch.exp( OplusXB + torch.matmul(V_b, self.C.T)) # temporary variable\n",
    "            big_term = torch.matmul(V_b.unsqueeze(2),YC ).squeeze()\n",
    "            # see formula above for more details, but here we take the norm for the last dimension. \n",
    "            #, then we take the sum over the axis that has dimension p, and we add the last term (big_term)\n",
    "            log_f[name] = -1/2*torch.norm(V_b, dim = 2)**2 -torch.sum(A_b, dim = 2) + big_term  \n",
    "        return log_f \n",
    "    \n",
    "    \n",
    "    def fit_IMPS(self,Y, O, covariates,  N_iter, acc,lr,class_optimizer = torch.optim.Rprop, optim_beta = True, optim_C = True): \n",
    "        '''\n",
    "        method that fit the parameters C and beta  of the model according to the data Y,O, covariates. \n",
    "        We maximize the log likelihood thanks to a gradient ascent. The gradients are computed thanks to \n",
    "        importance sampling. \n",
    "        \n",
    "        args : \n",
    "                Y : the counts, should be int. tensor of size(n,p)\n",
    "                O : the offsets : tensor of size (n,p)\n",
    "                covariates : the covariates of the model. tensor of size(n,d)\n",
    "                N_iter : int : the number of iteration you want to do. \n",
    "                acc : float : the accuracy you want when you compute the gradients and the likelihood. We will \n",
    "                sample 1/int samples.\n",
    "                lr : float, positive. the learning rate of the optimizer for C and beta.\n",
    "                class_optimizer : the optimizer you want, ex : torch.optim.Rprop. It should be a torch.optim object\n",
    "        returns : \n",
    "                None, but the parameters C and beta are optimized. \n",
    "        '''\n",
    "        self.t0= time.time()# to keep track of the time. \n",
    "        self.acc = acc\n",
    "        self.init_data(Y,O, covariates)# initialize the data. \n",
    "        optim = class_optimizer([self.beta,self.C], lr = lr) # optimizer on C and beta\n",
    "        optim.zero_grad() # We do this since it is possible that beta and C have gradients. \n",
    "        N_samples = int(1/acc) # We will sample 1/acc gaussians\n",
    "        for i in tqdm(range(N_iter)):\n",
    "            for (Y_b, covariates_b, O_b) in self.get_batch(self.batch_size):\n",
    "                dumb_mu = 1\n",
    "                V_b  = {}\n",
    "                V_b_single = {}\n",
    "                V_orig = torch.randn(N_samples, self.batch_size,q)\n",
    "                # estimate the integrals with importance sampling.\n",
    "                if self.zero_mean : \n",
    "                    mu_b_single = {'beta' : torch.zeros(self.batch_size,self.q), 'C_W': torch.zeros(self.batch_size,self.q), 'C_exp':torch.zeros(self.batch_size,self.q)}\n",
    "                else :\n",
    "                    #for each integral, find the variance for one coordinate only \n",
    "                    mu_b_single = self.find_each_mean_1d(Y_b,covariates_b,O_b)\n",
    "                    \n",
    "                for key,mu_single in mu_b_single.items(): \n",
    "                    V_b_single[key] = V_orig*self.var[key] +mu_single.unsqueeze(0)\n",
    "                #if self.is_perfect_mean == False, then we will take something approximate but fast. \n",
    "                if self.is_perfect_mean == False: \n",
    "                    esp_beta, esp_C_W,esp_C_exp = self.single_batch_MC_term(Y_b, covariates_b, O_b,V_b_single,mu_b_single)  \n",
    "                # if self.is_perfect_mean == True, then we will compute perfectly each one of the \n",
    "                # best mean for each coordinate of each integral.\n",
    "                else : \n",
    "                    mu_b = self.find_each_mean(Y_b,covariates_b,O_b)\n",
    "                    V_b['beta'], V_b['C_exp'], V_b['C_W'] = self.sample_g(N_samples, mu_b,self.var)\n",
    "                    esp_beta,esp_C_W,esp_C_exp = self.personalized_batch_MC_term(Y_b,covariates_b, O_b, V_b, mu_b, self.var)\n",
    "                #slf.plot_conditional(Y_b[0],covariates_b[0],O_b[0],mu_to_plot[0], self.q)\n",
    "                #self.plot_conditional(Y_b[0],covariates_b[0],O_b[0],mu_b['beta'][0], self.q)\n",
    "                #self.plot_conditional(Y_b[0],covariates_b[0],O_b[0],mu_b['C_W'][0], self.q)\n",
    "                #true_esp_beta, true_esp_C_W, true_esp_C_exp = self.get_true_esp_C_exp_1D(Y_b[0],covariates_b[0],O_b[0])\n",
    "                #if we want to compare with the true integrals. \n",
    "                '''\n",
    "                self.true_esp_C_exp = self.get_true_esp_pq(Y_b[self.to_watch], covariates_b[self.to_watch],O_b[self.to_watch])\n",
    "                print(' estimation of the pq integral for one sample ', plot_estimation_pq(\n",
    "                    self.inside_exp_C.detach().numpy()[:,self.to_watch]*int(1/self.acc), self.true_esp_C_exp))\n",
    "                '''\n",
    "                # computes the gradients thanks to the integrals\n",
    "                beta_grad, C_grad = self.get_batch_grads(Y_b, covariates_b, O_b, esp_beta, esp_C_W,esp_C_exp)\n",
    "                # computes the mean of the gradients since we computed the gradients for a batch\n",
    "                # some conditions to know if we want to optimize beta and C. This is only for learning \n",
    "                # rate optimization for C or beta : we fix one parameter and see which learning rate \n",
    "                # does the best. \n",
    "                # if false, we won't optimize C. \n",
    "                if optim_C == True : \n",
    "                    self.C.grad = torch.mean(C_grad, axis = 0)/3\n",
    "                else : \n",
    "                    self.C.grad = torch.zeros(self.C.shape)\n",
    "                if optim_beta == True : \n",
    "                    self.beta.grad = torch.mean(beta_grad, axis = 0) \n",
    "                else :\n",
    "                    self.beta.grad = torch.zeros(self.beta.shape)\n",
    "                log_like = vectorized_log_likelihood(Y_b, covariates_b, O_b, torch.randn(100,self.batch_size, self.q),self.C_mean,self.beta_mean)\n",
    "                self.keep_records(log_like.item())\n",
    "                optim.step()# optimization step \n",
    "                optim.zero_grad() # erase the gradients so that they don't accumulate.\n",
    "        return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_P_WgivenY(self,Y_i,covariates_i, O_i, W,C,beta):\n",
    "    '''\n",
    "    computes the log of the density of (W,Y_i) given Y_i. It should look like a gaussian. \n",
    "    args : \n",
    "        Y_i : tensor of size p \n",
    "        covariates_i : tensor of size d \n",
    "        O_i : tensor of size p \n",
    "        W : torch of size q. should be a gaussian. \n",
    "\n",
    "    return : the log of the probability of (W,Y_i) given Y_i. \n",
    "    '''\n",
    "    A_i = O_i + W.reshape(1,-1)@(C.T) +covariates_i@beta # temporary variable, for a clearer code. \n",
    "    return -1/2*torch.norm(W)**2 + torch.sum(-torch.exp(A_i)+W.reshape(1,-1)@(C.T)*Y_i)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\"> \n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_{C} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)=\\left[Y_{i}- \\exp \\left(O_i +  \\beta^{\\top} X_{i}+CW_{i}{ }\\right)\\right]  W_{i}^{\\top}}\n",
    "$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\exp \\left(-\\frac{1}{2}\\left\\|W_{i}\\right\\|^{2}-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CW_{i}\\right)+Y_{i}^{\\top} CW_{i}\\right)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "d = 2\n",
    "n = 10;  p = 3\n",
    "q = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/bastien/Documents/Stage/PLNpy/utils.py:210: UserWarning: torch.cholesky is deprecated in favor of torch.linalg.cholesky and will be removed in a future PyTorch release.\n",
      "L = torch.cholesky(A)\n",
      "should be replaced with\n",
      "L = torch.linalg.cholesky(A)\n",
      "and\n",
      "U = torch.cholesky(A, upper=True)\n",
      "should be replaced with\n",
      "U = torch.linalg.cholesky(A.transpose(-2, -1).conj()).transpose(-2, -1).conj() (Triggered internally at  /pytorch/aten/src/ATen/native/BatchLinearAlgebra.cpp:1284.)\n",
      "  chol = torch.cholesky(self.Sigma)\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "true_true_Sigma = 1*torch.from_numpy(build_block_Sigma(p,q))/2 #+ 0.5*torch.eye(p)\n",
    "true_C = torch.from_numpy(C_from_Sigma(true_true_Sigma, q))\n",
    "true_Sigma = true_C@(true_C.T)\n",
    "true_beta =torch.randn((d, p))/1\n",
    "\n",
    "covariates = torch.randn((n,d))/1\n",
    "O =  0+torch.zeros((n,p))\n",
    "sample_model = sample_PLN()\n",
    "Y_sampled, Z_sampled  = sample_model.sample(true_Sigma,true_beta, O, covariates)\n",
    "Y_sampled = torch.from_numpy(Y_sampled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(eps) : \n",
    "    return torch.mean(torch.abs(torch.log(Y_sampled + eps*(Y_sampled == 0))-Z_sampled))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\Sigma$ MLE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLE :\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true Sigma\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWQAAAD4CAYAAADbyJysAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8QVMy6AAAACXBIWXMAAAsTAAALEwEAmpwYAAASe0lEQVR4nO3df4xlZ13H8fen291ESwOBSi27W7bGVeRXUclWUpFWU9w2kJWEP7YgTUhxgqEKxhirf0A0xpgQSSQU141umibSxgjVjVn6IwZdpBa3bWrbbSlulmrHATelCBRNysx8/eOe2sN05t47M3f2nnt4v5qTved5zj3P05vNt99+z3POSVUhSZq+c6Y9AUnSgAFZkjrCgCxJHWFAlqSOMCBLUkecu9UDfPep0y7j2GI/8Io3T3sK0kQsPvuf2ew51hNztl/wI5seb5LMkCWpI7Y8Q5aks2p5adoz2DADsqR+WVqc9gw2zIAsqVeqlqc9hQ0zIEvql2UDsiR1gxmyJHXEDF/Uc9mbpH6p5fG3IZLsTvK5JI8lOZnkg6sckyQfT3IqyUNJfqrVtz/J403fjeNM3QxZUq/U5FZZLAK/WVUPJDkfuD/J3VX1aOuYq4G9zXYZ8KfAZUm2ATcBVwHzwIkkR1d89wUMyJL6ZUIX9arqq8BXm8/fTvIYsBNoB9UDwC01eLD8vUlekuQiYA9wqqpOAyS5rTnWgCzp+8gWXNRLsgf4SeCLK7p2Ak+29uebttXaLxs1jgFZUr+s46JekjlgrtV0uKoOrzjmRcCngQ9V1bdWnmKV09aQ9qEMyJL6ZR0ZchN8D6/Vn2Q7g2D8l1X1mVUOmQd2t/Z3AQvAjjXah3KVhaR+WVocfxsiSYC/AB6rqo+tcdhR4LpmtcXPAN9sas8ngL1JLkmyAzjYHDuUGbKkfpncnXqXA+8BHk7yYNP2u8DFAFV1CDgGXAOcAv4HeG/Tt5jkBuBOYBtwpKpOjhrQgCypV6omc2NIVf0Tq9eC28cU8IE1+o4xCNhjMyBL6hdvnZakjvDhQpLUEWbIktQRS9+d9gw2zIAsqV8sWUhSR1iykKSOMEOWpI4wIEtSN5QX9SSpI6whS1JHWLKQpI4wQ5akjjBDlqSOMEOWpI5YnNhbp886A7KkfjFDlqSOsIYsSR3R5ww5yauAA8BOBq+xXgCOVtVjWzw3SVq/CWbISY4AbwPOVNVrV+n/LeDdze65wE8AP1RVTyd5Avg2sAQsVtUbR4039K3TSX4buI3Be6X+hcGbVAPcmuTGcf+lJOmsqeXxt9FuBvavOVTVR6vqDVX1BuB3gH+sqqdbh1zZ9I8MxjA6Q74eeE1Vfc/N4Uk+BpwE/mi1LyWZA+YAPvnHf8D7rrt2nLlI0uZNcJVFVR1PsmfMw68Fbt3MeKMC8jLwCuDfV7Rf1PStqqoOA4cBvvvU6drMBCVpXWr8kNNOHhuHm/i1Lkl+kEEmfUN7JsBdSQr4s3HOOyogfwj4+yT/BjzZtF0M/OiKgSWpG9ZRQ24nj5v0duALK8oVl1fVQpKXA3cn+VJVHR92kqEBuaruSPJjwD4GF/UCzAMnqmppc/OXpC0wnWVvB1lRrqiqhebPM0luZxBHNx6Qm5MtA/dufJ6SdBad5WVvSV4MvAX45VbbecA5VfXt5vNbgd8fdS7XIUvql6XJ/c97kluBK4ALkswDHwG2A1TVoeawdwB3VdV3Wl+9ELg9CQzi7Keq6o5R4xmQJfXLBEsWVTVyiVhV3cxgeVy77TRw6XrHMyBL6hdvnZakjujzrdOSNEtqeXZvfTAgS+oXSxaS1BETXGVxthmQJfWLGbIkdYQBWZI6Yh0PF+oaA7KkfjFDlqSOcNmbJHWEqywkqRvKkoUkdYQlC0nqCJ9lIUkdYYYsSR2x6EU9SeqGGS5ZnDPtCUjSRC3X+NsISY4kOZPkkTX6r0jyzSQPNtuHW337kzye5FSSG8eZuhmypF6Z8LK3m4FPALcMOebzVfW2dkOSbcBNwFXAPHAiydGqenTYYGbIkvplghlyVR0Hnt7ALPYBp6rqdFU9C9wGHBj1JQOypH5ZR0BOMpfkvtY2t4ER35TkX5N8NslrmradwJOtY+abtqEsWUjql3XcOl1Vh4HDmxjtAeCVVfVMkmuAvwH2AlltuFEnM0OW1Cu1XGNvmx6r6ltV9Uzz+RiwPckFDDLi3a1DdwELo85nhiypX87ijSFJfhj4r6qqJPsYJLlfB/4b2JvkEuA/gYPAu0adz4AsqV8muMoiya3AFcAFSeaBjwDbAarqEPBO4FeTLAL/CxysqgIWk9wA3AlsA45U1clR4xmQJfXLBDPkqrp2RP8nGCyLW63vGHBsPeMZkCX1i8+ykKRuqKXZvXV6ywPyD7zizVs9xPe9D/gbb7mbFj4/7SloXGbIktQNk1jONi0GZEn9YkCWpI6Y3RKyAVlSv9Ti7EZkA7KkfpndeGxAltQvXtSTpK4wQ5akbjBDlqSuMEOWpG6oxWnPYOMMyJJ6pcyQJakjDMiS1A1myJLUEQZkSeqIWlrthc+zwbdOS+qVWh5/GyXJkSRnkjyyRv+7kzzUbPckubTV90SSh5M8mOS+ceZuhiypV2p5ohnyzQzemXfLGv1fAd5SVd9IcjVwGLis1X9lVT017mAGZEm9MskaclUdT7JnSP89rd17gV2bGc+ShaReqcrYW5K5JPe1trlNDH098Nn2VIC7ktw/7nnNkCX1ynoy5Ko6zKDMsClJrmQQkH+21Xx5VS0keTlwd5IvVdXxYecxIEvqleWzvMoiyeuBPweurqqvP9deVQvNn2eS3A7sA4YGZEsWknqlljP2tllJLgY+A7ynqr7caj8vyfnPfQbeCqy6UqPNDFlSr0xylUWSW4ErgAuSzAMfAbYDVNUh4MPAy4BPJgFYrKo3AhcCtzdt5wKfqqo7Ro1nQJbUKzXBxyFX1bUj+t8HvG+V9tPApS/8xnAGZEm9MuF1yGeVAVlSr1QZkCWpE5Zm+FkWBmRJvWKGLEkdYQ1ZkjpikqsszjYDsqReMUOWpI5YWp7dG5ANyJJ6xZKFJHXEsqssJKkbZnnZ24aLLUneO8mJSNIkVI2/dc1mqt+/t1ZH+yn8y8vf2cQQkrQ+y5Wxt64ZWrJI8tBaXQweL7eq9lP4z92xs4P/HZLUV31eZXEh8IvAN1a0B7jnhYdL0nTNcgY4KiD/HfCiqnpwZUeSf9iKCUnSZnSxFDGuoQG5qq4f0veuyU9Hkjbn+3KVhSR10fI6tlGSHElyJsmq78PLwMeTnEryUJKfavXtT/J403fjOHM3IEvqlSJjb2O4Gdg/pP9qYG+zzQF/CpBkG3BT0/9q4Nokrx41mDeGSOqVxQmWLKrqeJI9Qw45ANxSVQXcm+QlSS4C9gCnmnfrkeS25thHh41nhiypV9aTIbfvmWi2uXUOtxN4srU/37St1T6UGbKkXhmnNvyc9j0TG7RaOl5D2ocyIEvqlTFrw5MyD+xu7e8CFoAda7QPZclCUq9McpXFGI4C1zWrLX4G+GZVfRU4AexNckmSHcDB5tihzJAl9crSBDPkJLcCVwAXJJkHPgJsB6iqQ8Ax4BrgFPA/wHubvsUkNwB3AtuAI1V1ctR4BmRJvTLJNzhV1bUj+gv4wBp9xxgE7LEZkCX1yvLZrSFPlAFZUq/0+eFCkjRTJnSxbioMyJJ6ZTmWLCSpE5amPYFNMCBL6pVJrrI42wzIknrFVRaS1BGuspCkjrBkIUkd4bI3SeqIJTNkSeoGM2RJ6ggDsiR1xARfqXfWGZAl9YoZsiR1hLdOS1JHzPI6ZN+pJ6lXJvlOvST7kzye5FSSG1fp/60kDzbbI0mWkry06XsiycNN333jzN0MWVKvTKqGnGQbcBNwFYO3S59IcrSqHn3umKr6KPDR5vi3A79RVU+3TnNlVT017phmyJJ6pdaxjbAPOFVVp6vqWeA24MCQ468Fbt3E1A3IkvplOeNvSeaS3Nfa5lqn2gk82dqfb9peIMkPAvuBT7eaC7gryf0rzrsmSxaSemU9qyyq6jBweI3u1S4PrpVYvx34wopyxeVVtZDk5cDdSb5UVceHzceA3AM3LXx+2lPovc+99E3TnoLGtDy5B3DOA7tb+7uAhTWOPciKckVVLTR/nklyO4MSyNCAbMlCUq9McJXFCWBvkkuS7GAQdI+uPCjJi4G3AH/bajsvyfnPfQbeCjwyakAzZEm9Mqn8uKoWk9wA3AlsA45U1ckk72/6DzWHvgO4q6q+0/r6hcDtGbxw9VzgU1V1x6gxDciSemWSt05X1THg2Iq2Qyv2bwZuXtF2Grh0veMZkCX1ymJm9yVOBmRJvTK74diALKlnfNqbJHXEBJe9nXUGZEm9Mrvh2IAsqWcsWUhSRyzNcI5sQJbUK2bIktQRZYYsSd1ghixJHeGyN0nqiNkNxwZkST2zOMMh2YAsqVe8qCdJHeFFPUnqCDNkSeoIM2RJ6oilmt0M2ZecSuqVZWrsbZQk+5M8nuRUkhtX6b8iyTeTPNhsHx73u6sxQ5bUK5OqISfZBtwEXAXMAyeSHK2qR1cc+vmqetsGv/s9zJAl9cryOrYR9gGnqup0VT0L3AYcGHMaG/quAVlSr6ynZJFkLsl9rW2udaqdwJOt/fmmbaU3JfnXJJ9N8pp1fvd7WLKQ1CvrKVlU1WHg8BrdWfX03+sB4JVV9UySa4C/AfaO+d0XMEOW1CtLVWNvI8wDu1v7u4CF9gFV9a2qeqb5fAzYnuSCcb67GgOypF6Z4CqLE8DeJJck2QEcBI62D0jyw0nSfN7HIKZ+fZzvrsaShaRemdSNIVW1mOQG4E5gG3Ckqk4meX/Tfwh4J/CrSRaB/wUOVlUBq3531JgGZEm9Mslbp5syxLEVbYdanz8BfGLc745iQJbUKz6gXpI6omb41mkDsqReWTJDlqRumOWSxchlb0leleQXkrxoRfv+rZuWJG1MVY29dc3QgJzk14G/BX4NeCRJ+17sP9zKiUnSRkzyaW9n26iSxa8AP93cFrgH+Oske6rqT1j91kAAmvvB5wCy7cWcc855k5qvJA3V5zeGbGvdFvhEkisYBOVXMiQgt+8PP3fHztn9dSTNnD4/oP5rSd7w3E4TnN8GXAC8bgvnJUkb0ueSxXXAYruhqhaB65L82ZbNSpI2qIuBdlxDA3JVzQ/p+8LkpyNJm9PF1RPjch2ypF7pbYYsSbOmz6ssJGmmLNWkHsB59hmQJfWKNWRJ6ghryJLUEbNcQ/adepJ6Zblq7G2UJPuTPJ7kVJIbV+l/d5KHmu2eJJe2+p5I8nCSB5PcN87czZAl9cqkMuQk24CbgKsYvEX6RJKjVfVo67CvAG+pqm8kuZrBIyMua/VfWVVPjTumAVlSr0xwlcU+4FRVnQZIchtwAPj/gFxV97SOvxfYtZkBLVlI6pX1lCySzCW5r7XNtU61E3iytT/ftK3leuCzrf0C7kpy/4rzrskMWVKvrKdk0X4y5SpWe6LlqidPciWDgPyzrebLq2ohycuBu5N8qaqOD5uPGbKkXpngRb15YHdrfxewsPKgJK8H/hw4UFVff669qhaaP88AtzMogQxlQJbUK7WOf0Y4AexNckmSHcBB4Gj7gCQXA58B3lNVX261n5fk/Oc+A28FHhk1oCULSb2yVEsTOU9VLSa5AbgT2AYcqaqTSd7f9B8CPgy8DPhkEoDFqnojcCFwe9N2LvCpqrpj1JjZ6tsMfWOI+uBzL33TtKfwfeHNX/vrNd9ENK6LX/q6sWPOfzz98KbHmyQzZEm94q3TktQRPlxIkjpinFuiu8qALKlXZvnhQgZkSb3iA+olqSOsIUtSR1hDlqSOMEOWpI5wHbIkdYQZsiR1hKssJKkjvKgnSR1hyUKSOsI79SSpI8yQJakjZrmGvOUPqJ9FSeaalx9qi/gbbz1/49njO/VWN9Yru7Up/sZbz994xhiQJakjDMiS1BEG5NVZd9t6/sZbz994xnhRT5I6wgxZkjrCgCxJHWFAbkmyP8njSU4luXHa8+mjJEeSnEnyyLTn0ldJdif5XJLHkpxM8sFpz0njsYbcSLIN+DJwFTAPnACurapHpzqxnknyc8AzwC1V9dppz6ePklwEXFRVDyQ5H7gf+CX/LnefGfLz9gGnqup0VT0L3AYcmPKceqeqjgNPT3sefVZVX62qB5rP3wYeA3ZOd1YahwH5eTuBJ1v78/iXWDMuyR7gJ4EvTnkqGoMB+XlZpc16jmZWkhcBnwY+VFXfmvZ8NJoB+XnzwO7W/i5gYUpzkTYlyXYGwfgvq+oz056PxmNAft4JYG+SS5LsAA4CR6c8J2ndkgT4C+CxqvrYtOej8RmQG1W1CNwA3MngIshfVdXJ6c6qf5LcCvwz8ONJ5pNcP+059dDlwHuAn0/yYLNdM+1JaTSXvUlSR5ghS1JHGJAlqSMMyJLUEQZkSeoIA7IkdYQBWZI6woAsSR3xf6cioqejjk07AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best MSE we can have (in our dream):  tensor(0.0082)\n"
     ]
    }
   ],
   "source": [
    "## Here we plot the MLE for Z the latent variables. They are unknown in practice\n",
    "## this is just to see the difference between our result and the best result possible \n",
    "mu_MLE = torch.mean(Z_sampled, dim = 0)\n",
    "m = mu_MLE.shape[0]\n",
    "Sigma_MLE = 1/Z_sampled.shape[0]*((Z_sampled-mu_MLE).T)@(Z_sampled-mu_MLE)\n",
    "print('MLE :')\n",
    "sns.heatmap(Sigma_MLE.to(torch.device('cpu')))\n",
    "plt.show()\n",
    "print('true Sigma')\n",
    "sns.heatmap(true_Sigma.to(torch.device('cpu')))\n",
    "plt.show()\n",
    "best_MSE_Sigma = torch.mean((Sigma_MLE.to(device)-true_Sigma)**2)\n",
    "print('Best MSE we can have (in our dream): ', best_MSE_Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\beta$ MLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE beta in the latent layer:  tensor(2.5869)\n"
     ]
    }
   ],
   "source": [
    "target = Z_sampled+ torch.mm(covariates,true_beta)\n",
    "\n",
    "clf = LinearRegression()\n",
    "clf.fit(covariates,target)\n",
    "beta_dream = torch.from_numpy(clf.coef_.T)\n",
    "best_MSE_beta = torch.mean((beta_dream-true_beta)**2)\n",
    "print('MSE beta in the latent layer: ', best_MSE_beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'q' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-7a994d0762fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#### torch.manual_seed(int(time.time())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m25\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMC_PLNPCA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maverage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_perfect_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrue_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_beta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbest_var_beta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_C\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzero_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'model.fit_IMPS(Y_sampled,O, covariates,N_iter =1,acc= 0.001,lr = 0, class_optimizer = torch.optim.Adamax,optim_beta = True)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#%time model.fit_torch(Y_sampled,O, covariates,N_iter =1,acc= 0.001,lr = 0.02, class_optimizer = torch.optim.Rprop)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'q' is not defined"
     ]
    }
   ],
   "source": [
    "#### torch.manual_seed(int(time.time())\n",
    "batch_size = 25\n",
    "model = MC_PLNPCA(q,batch_size, average = n//4, is_perfect_mean = True,true_value = True, var_beta = best_var_beta, var_C = 1, zero_mean = False)\n",
    "%time model.fit_IMPS(Y_sampled,O, covariates,N_iter =1,acc= 0.001,lr = 0, class_optimizer = torch.optim.Adamax,optim_beta = True)\n",
    "#%time model.fit_torch(Y_sampled,O, covariates,N_iter =1,acc= 0.001,lr = 0.02, class_optimizer = torch.optim.Rprop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\beta^{\\top}\\mathbf{x}_{i} +\\mathbf{C}W_i , \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE Sigma tensor(0.0012, grad_fn=<MeanBackward0>)\n",
      "MSE beta tensor(0.0042, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "from fastPLN import fastPLN\n",
    "fastmodel = fastPLN()\n",
    "fastmodel.fit(Y_sampled,O,covariates, 50)\n",
    "print('MSE Sigma', torch.mean((fastmodel.Sigma-true_Sigma)**2))\n",
    "print('MSE beta', torch.mean((fastmodel.beta-true_beta)**2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First method : direct optimization of the likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta} P_{\\theta}(Y)$$\n",
    "\n",
    "But we need to integrate out $W$ in order to compute the quantity inside the max : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i}\\right) &=\\int P_{\\theta}\\left(Y_{i}, W\\right) d W \\\\\n",
    "&=\\int P_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "This integral being untractable, we are going to approximate it with Monte Carlo methods : \n",
    "\n",
    "$$\n",
    "\\int p_{\\theta}\\left(Y_{i} \\mid W\\right) p(W) d W \\approx \\frac{1}{K} \\sum_{k = 1 }^Kp_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)\n",
    "$$\n",
    "$$W_{i,k} \\sim \\mathcal N (0, I_q)$$\n",
    "\n",
    "The larger the $K$ the better the approximation.  \n",
    "\n",
    "Let's compute $p_{\\theta}\\left(Y_{i} \\mid W_i\\right) p\\left(W_i\\right)$. \n",
    "\n",
    "\n",
    "First, \n",
    "\n",
    "$$\n",
    "P\\left(W_{i}\\right)=\\frac{1}{\\sqrt{2 \\pi}} \\exp \\left(-\\frac{1}{2}\\left\\|W_{i}\\right\\|_{2}^{2}\\right)\n",
    "$$ \n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "P_{\\theta}\\left(Y_{i} \\mid W_{i}\\right) &=\\prod_{j=1}^{p} p_{\\theta}\\left(Y_{i j} \\mid W_{i}\\right) \\\\\n",
    "&=\\prod_{j=1}^{P} \\frac{1}{Y_{i j} !} \\exp \\left(-\\exp \\left(o_{i j}+z_{i j}\\right)\\right) \\exp \\left(o_{i j}+z_{i j}\\right)^{Y_{ij}}\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\n",
    "Z_{i j}=x_{i}^{\\top} \\beta_{:,j}+ C_{j}^{\\top}W_{i}\n",
    "$$\n",
    "\n",
    "Putting all together : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right) p\\left(W_{i}\\right) \n",
    "& =\\operatorname{const} \\times \\exp \\left(-\\frac{1}{2} W_{i}^{\\top} W_{i}\\right) \\exp \\left(\\sum_{j=1}^{p}-\\operatorname{exp}\\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(0_{i j}+Z_{i j}\\right)\\right)\\\\\n",
    "&= \\operatorname{const} \\times \\exp \\left(-\\frac{1}{2}\\left\\|W_{i}\\right\\|^{2}-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+Z_{i}\\right)+Y_{i}^{\\top}\\left(0_{i}+Z_{i}\\right)\\right)\\\\\n",
    "& = \\operatorname{const} \\times \\exp \\left(-\\frac{1}{2}\\left\\|W_{i}\\right\\|^{2}-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CW_{i}\\right)+Y_{i}^{\\top}\\left(0_{i}+\\beta ^{\\top} X_{i}+ CW_{i}\\right)\\right)\\\\\n",
    "& = \\operatorname{const} \\times \\exp \\left(-\\frac{1}{2}\\left\\|W_{i}\\right\\|^{2}-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CW_{i}\\right)+Y_{i}^{\\top} CW_{i}\\right)\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We set $p^{(u)}_{\\theta}(W_i) = \\exp \\left(-\\frac{1}{2}\\left\\|W_{i}\\right\\|^{2}-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CW_{i}\\right)+Y_{i}^{\\top} CW_{i}\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numerical purposes, we may want to use a logarithmic scale and apply the exponential function after. Indeed, $Y_{ij}$ can go up to a thousand, and computing this factorial would give infinite values. \n",
    "\n",
    "$$\n",
    "\\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)=\\sum_{j=1}^{p} - \\ln \\left(Y_{i j} !\\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(o_{i j}+Z_{i j}\\right)\n",
    "$$\n",
    "\n",
    "We are going tu use the Stirling Formula in order to compute the log of the factorial, to avoid computing directly the factorial. \n",
    "\n",
    "\n",
    "If we consider the whole likelihood : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\ln p_{\\theta}\\left(Y \\mid W\\right) &=\\sum_{i=1}^{n} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right) \\\\\n",
    "&=\\sum_{i=1}^{n} \\sum_{j} - \\ln \\left(Y_{ij} ! \\right)-\\exp \\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(0_{i j}+Z_{i j}\\right) \\\\\n",
    "&=1_{n}^{T}\\left[-\\ln (Y !)-\\exp (0+Z)+Y \\odot (0+Z)\\right] 1_{p} \\\\\n",
    "Z=& X \\beta+W C^{\\top}\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "We now need to compute the gradients. Since \n",
    "\n",
    "$$\n",
    "\\nabla_{\\theta} p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)=p_{\\theta}\\left(Y_{i}| W_{i}\\right) \\nabla_{\\theta} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient computation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Let's compute $$\n",
    "\\nabla_{{\\theta}} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)\n",
    "$$\n",
    "\n",
    "\n",
    "We begin with $\\beta$. We see the vectors of size $n$ as a matrice of dimension $(n,1)$ ( a line).\n",
    "\n",
    "We set $A=O+ CW_{i}$\n",
    "\n",
    "We set \n",
    "\n",
    "$$\n",
    "h: \\beta \\mapsto \\mathbb{1}_{p}^{\\top}\\exp \\left( \\beta^{\\top}X_{i}+A\\right) \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial h(\\beta)}{\\partial \\beta_{k l}} &=\\frac{\\partial}{\\partial \\beta_{k l}}\\left(\\sum_j \\exp \\left(X_{i}^{\\top}\\beta_{:, j}+A_{j}\\right)\\right)\\\\\n",
    "&=\\frac{\\partial}{\\partial \\beta_{k,l} } \\exp \\left(X_{i}^{\\top} \\beta_{:,l}+A_{l}\\right) \\\\\n",
    "&=\\frac{\\partial}{\\partial \\beta_{kl} } \\exp \\left(\\sum_{s} X_{i s} B_{s l}+A_{l}\\right)\\\\\n",
    "&=\\left(\\frac{\\partial}{\\partial \\beta_{k l}}\\left(\\sum_{s} x_{i s} \\beta_{s l}+A_{l}\\right)\\right) \\exp \\left(\\sum_{s} X_{i s} \\beta_{sl}+A_l\\right) \\\\\n",
    "&=X_{i k} \\exp \\left(\\sum_{s} X_{i s} \\beta_{s l}+A_{l}\\right)\\\\\n",
    "&=X_{i k} \\exp \\left(X_{i}^{\\top} \\beta_{:,l}+A_{l}\\right) \\\\\n",
    "&=\\left( X_{i}\\exp \\left( \\beta^{\\top}X_{i}+A\\right) ^{T} \\right) _{k 1}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "So that $$\\nabla _{\\beta} h =  X_i\\exp \\left(O_i +  \\beta^{\\top}X_{i}+  CW_{i,k}\\right)^{\\top}$$\n",
    "\n",
    "A similar argument for $$\n",
    "\\tilde h: \\beta \\mapsto  \\sum_{j} Y_{i j}\\left(0_{ij}+X_{i}^{\\top} \\beta_{:,j}+W_{i} C^{\\top}_{:,j}\\right)\n",
    "$$\n",
    "\n",
    "shows that \n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} \\tilde{h}=X_{i} Y_{i}^{\\top}\n",
    "$$\n",
    "\n",
    "So that : \n",
    "<span style=\"color:red\"> \n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_{\\beta} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)= \\nabla _{\\beta} \\tilde h - \\nabla _{\\beta}  h= X_{i} \\left[Y_i - \\exp \\left(O_i +  \\beta^{\\top} X_i+CW_{i}  \\right) \\right]^{\\top}}\n",
    "$$\n",
    "</span>\n",
    "A similar argument for $C$ shows that : \n",
    "<span style=\"color:red\"> \n",
    "$$\n",
    "\\boxed{\n",
    "\\nabla_{C} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)=\\left[Y_{i}- \\exp \\left(O_i +  \\beta^{\\top} X_{i}+CW_{i}{ }\\right)\\right]  W_{i}^{\\top}}\n",
    "$$\n",
    "</span>\n",
    "\n",
    "(Note that if you want to see the vectors as matrices of size (1,n), you only need to  transpose  each vector and it should work fine : \n",
    "\n",
    "If we see the vectors as matrices of size (1,n) : \n",
    "$$\n",
    "\\nabla_{C} \\log p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)=\\left[Y_{i}- \\exp \\left(O_i + X_{i} \\beta+W_{i}C^{\\top}\\right)\\right]^{\\top}  W_{i}\n",
    "$$\n",
    "\n",
    "The first vector has a size $(p,1)$, the second $(1,q)$, which makes it $(p,q)$.\n",
    ")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get : \n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)\n",
    "X_{i} \\left[Y_i - \\exp \\left(O_i +  \\beta^{\\top} X_i+CW_{i}  \\right) \\right]^{\\top}\n",
    "$$\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)=p_{\\theta}\\left(Y_{i} \\mid W_{i}\\right)\n",
    "\\left[Y_{i}- \\exp \\left(O_i +  \\beta^{\\top} X_{i}+CW_{i}{ }\\right)\\right]  W_{i}^{\\top}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### After trying to optimize the likelihood, we have found that the likelihood is very small (the order of the log likelihood is about $-3\\times10^{3}$ for some samples which makes the exponential 0 numerically. We need the likelihood to compute the gradients (see above). We can't optimize the log likelihood directly due to the integral : \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\log p_{\\theta}(Y) &=\\log \\left(\\int p_{\\theta}(Y, W)dW\\right) \\\\\n",
    "& \\approx \\log \\left(\\frac{1}{K}\\sum^K p_{\\theta}\\left(Y, W_{k}\\right)\\right)\\\\\n",
    "& \\neq \\frac{1}{K} \\sum^{K} \\log \\left(p_{\\theta}\\left(Y, W_{k}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importance Sampling "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We thus change the objective function. We want to maximize the log likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta}\\operatorname{log} P_{\\theta}(Y) = \\max _{\\theta} \\sum _i \\operatorname{log} p_{\\theta}(Y_i)$$\n",
    " \n",
    " \n",
    "We need to derive the gradients with respect to theta : \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta} \\log p_{\\theta}(Y_i)&= \\frac{\\nabla_{\\theta} p_{\\theta}(Y_i)}{p_{\\theta}(Y_i)} \\\\\n",
    "&=\\frac{\\nabla_{\\theta} \\int p_{\\theta}(Y_i \\mid W_i) p(W_i) d W}{\\int p_{\\theta}(Y_i \\mid W_i) p(W_i) d W_i}\\\\\n",
    "&=\\frac{\\int \\nabla_{\\theta} p_{\\theta}(Y_i | W_i) p(W_i) d W_i}{\\int  p_{\\theta}(Y_i|W_i) p(W_i) d W_i}\\\\\n",
    "&= \\frac{\\int\\left(\\nabla_{\\theta} \\ln p_{\\theta}(Y \\mid W)\\right) p_{\\theta}(Y| W) p(W) d W}{\\int p_{\\theta}(Y_i \\mid W_i)p(W_i) d W_i}\\\\\n",
    "&=\\int \\nabla_{\\theta} \\ln p_{\\theta}(Y_i \\mid W_i) \\tilde{p}_{\\theta}(W_i) dW_i \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\tilde p_{\\theta}(W_i):=\\frac{p_{\\theta}(Y_i \\mid W_i) p(W_i) d W_i}{\\int p_{\\theta}(Y_i \\mid W_i) p(W_i) dW_i}$$\n",
    "\n",
    "\n",
    "So that : \n",
    "\n",
    "<span style=\"color:red\">\n",
    "$$\n",
    "\\boxed{\\nabla_{\\theta} \\log p_{\\theta}(Y_i) = \\mathbb E_{\\tilde p } \\left[\\nabla_{\\theta} \\ln p_{\\theta}(Y_i \\mid W_i)\\right]}$$ \n",
    "</span>\n",
    "\n",
    "Here $\\tilde p $ is the law of $W_i | Y_i$. Indeed : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "p_{\\theta}(W_i \\mid Y_i) &=\\frac{p_{\\theta}(Y_i|W_i) \\operatorname{p}(W_i)}{p_{\\theta}(Y_i)} \\\\\n",
    "& = \\frac {  {p_{\\theta}}\\left(Y_i \\mid W_{i}\\right)p(W_i)}{\\int p_{\\theta}(Y_i \\mid W_i) p(W_i) dW_i}\\\\\n",
    "&= \\tilde p_{\\theta}(W_i)\n",
    "\\end{aligned}\n",
    "$$\n",
    "Note also that \n",
    "\n",
    "$$\\tilde p_{\\theta}(W_i) = \\frac{p^{(u)}_{\\theta}(W_i)}{\\int p^{(u)}_{\\theta}(W_i) dW_i}$$\n",
    "\n",
    "We only know the numerator of $ p^{(u)}_{\\theta}(W)$. Thus we need to use importance sampling, which consits in the following : \n",
    "\n",
    "Let $\\phi $ be a mesurable function. Let $g$ be a probability density such that $x \\in \\operatorname{supp}(\\tilde p_{\\theta}) \\implies g(x)>0$. We denote $(V_{i,k})_k \\overset{iid}{\\sim} g$. \n",
    "\n",
    "We define : \n",
    "\n",
    "$$\n",
    "w_{k}^{(u)}=\\frac{p^{(u)}_{\\theta}\\left(V_{i,k}\\right)}{g\\left(V_{i,k}\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{w}_{k}^{(u)}=\\frac{w_{k}^{(u)}}{\\sum_{\\ell=1}^{n} w_{\\ell}^{(u)}}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\hat{I}_{n_{\\text{samples}}}^{I S, u}:=\\sum_{k=1}^{n_{\\text{samples}}} \\tilde{w}_{k}^{(u)} \\phi (V_{i,k})\n",
    " \\xrightarrow[n_s \\to + \\infty]{\\mathbb P} \\int \\phi(W) \\tilde p_{\\theta}(W) = \\mathbb E _{\\tilde p _{\\theta}}[\\phi(W)]\n",
    "$$\n",
    "\n",
    "We need to choose carefully the density $g$ i.e. where $ p^{(u)}_{\\theta}\\times \\nabla_{\\theta} \\log p_{\\theta}$ has a lot of mass."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We start with $\\beta$: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{\\beta} \\log {p_{\\theta}}(Y_i)&=E_{\\tilde{p}_{\\theta}}\\left[\\nabla _ { \\beta } \\operatorname {log}  p_{\\theta} \\left(Y_{i}\\left|W_{i k}\\right)\\right]\\right.\\\\\n",
    "&=X_{i}\\left[Y_{i}-\\exp \\left(0_{i}+ \\beta^{\\top} X_i\\right) \\mathbb E_{\\tilde p_{\\theta}}\\left[e^{CW_{i,k} }\\right]\\right]^{\\top}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's look closer at this integral. \n",
    "\n",
    "$$\\mathbb E_{\\tilde p_{\\theta}}\\left[e^{CW_{i,k}}\\right] \n",
    "=\\left(\\begin{array}{l}\n",
    "E_{\\tilde{p}_{\\theta}}\\left[e^{\\left(CW_{i,k}\\right)_{1}}\\right] \\\\\n",
    "\\quad \\quad  \\vdots \\\\\n",
    "E_{\\tilde p_{\\theta}}\\left[e^{\\left(CW_{i,k}\\right)_p}\\right]\n",
    "\\end{array}\\right)\n",
    "=\\left(E_{\\tilde p_{\\theta}}\\left[e^{u_{m}^{\\top}CW_{i} }\\right]\\right)_{1 \\leqslant m\\leqslant p}\n",
    "$$\n",
    "\n",
    "\n",
    "Where $u_m$ is the canonic vector (i.e. a vector full of zeros except $1$ on the $m$ position)\n",
    "\n",
    "Let $ 1 \\leq m \\leq p$.Let $ \\phi _{\\beta,m}(W)=e^{u_{m}^{\\top}CW_{i} } $  Let's look at $E_{\\tilde p_{\\theta}}\\left[\\phi _{\\beta,m}(W)\\right]$\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "(\\star) = E_{\\tilde p_{\\theta}}\\left[\\phi _{\\beta,m}(W) \\right]  &= \\int e^{u_{m}^{\\top}CW_{i,k} } \\frac {p_{\\theta}\\left(Y_{i} \\mid W_{i,k}\\right) p\\left(W_{i,k}\\right)}{{\\int p_{\\theta}(Y_i,W)  dW}}dW_{i,k} = \\int e^{u_{m}^{\\top}CW_{i,k} } \\frac {p^{(u)}_{\\theta}(W_i)}{\\int p^{(u)}_{\\theta}(W_i) dW_i} \\\\\n",
    "& =  \\frac {1}{\\int p^{(u)}_{\\theta}(W) dW}\\int  \\exp \\left(u_{m}^{\\top}CW_{i,k}-\\frac{1}{2}\\left\\|W_{ik}\\right\\|^{2}-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CW_{i,k}\\right)+Y_i^{\\top}CW_{i,k}\\right)d W_{i,k} \\\\\n",
    "& =  \\frac {1}{{\\int p^{(u)}_{\\theta}(W)  dW}}\\int  \\exp \\left((u_{m}+Y_i)^{\\top}CW_{i,k}-\\frac{1}{2}\\left\\|W_{ik}\\right\\|^{2} -\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CW_{i,k}\\right)\\right)d W_{i,k}\\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "We set \n",
    "$$\n",
    "\\mu_{m}:=\\left(u_{m}+Y_{i}\\right)^{\\top} C\n",
    "$$\n",
    "\n",
    "Let $(V_{i,k,m})_k  \\overset{iid}\\sim \\mathcal N(\\mu_{m}, I_q)$ ang $g_m$ the corresponding density. \n",
    "\n",
    "Then, $$ \\begin {aligned}\n",
    "g_m(V) & = \\frac{1}{(2 \\pi)^{q / 2}} \\exp \\left[-\\frac{1}{2}(\\boldsymbol{V}-\\boldsymbol{\\mu_m})^{\\top} (\\boldsymbol{V}-\\boldsymbol{\\mu_m})\\right]\\\\\n",
    "& = \\frac{1}{(2 \\pi)^{q / 2}} e^{-\\frac{\\mu_m ^{\\top} \\mu_m}{2}} e^{- \\frac 1  2 V^{\\top} V+V^{\\top} \\mu_m}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We set $A(\\mu_m) = \\frac{1}{(2 \\pi)^{q / 2}} e^{-\\frac{\\mu_m ^{\\top} \\mu_m}{2}}$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Let's compute the importance weights for $g$. \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "w^{(u)}_k & = \\frac{p^{(u)}_{\\theta}\\left(V_{i,k}\\right)}{g\\left(V_{i,k}\\right)} \\\\\n",
    "& = \\frac { \\exp \\left(-\\frac{1}{2}\\left\\|V_{i,k}\\right\\|^{2}-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CV_{i,k}\\right)+Y_{i}^{\\top} CV_{i,k}\\right)}{A(\\mu _m ) e^{- \\frac 1  2 V_{i,k}^{\\top} V_{i,k}+V_{i,k}^{\\top} \\mu_m}} \\\\\n",
    "& = \\frac 1 {A(\\mu _m)  } \\operatorname{exp}\\left(-\\mathbb 1_{p}^{T} \\exp \\left(0_{i}+\\beta ^{\\top} X_{i}+ CV_{i,k}\\right) - u_m^{\\top}CV_{i,k}\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "Note that in practice, setting $\\mu _m = 0$ works better. We will see why after. \n",
    "\n",
    "\n",
    "Computation for the gradient of $C$ : \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\nabla_{C} \\log {p_{\\theta}}(Y_i)&=E_{\\tilde{p}_{\\theta}}\\left[\\nabla _ { C } \\operatorname {log}  p_{\\theta} \\left(Y_{i}\\left|W_{i k}\\right)\\right]\\right.\\\\\n",
    "&= \\mathbb E_{\\tilde p_{\\theta}}\\left[\\left[Y_{i}- \\exp \\left(O_i +  \\beta^{\\top} X_{i}+CW_{i}{ }\\right)\\right]  W_{i}^{\\top}\\right]\\\\\n",
    "&= Y_{i}\\mathbb E_{\\tilde p_{\\theta}} \\left[ W_i^{\\top} \\right] -\\exp(O_i + \\beta^{\\top}X_i) \\mathbb E_{\\tilde p_{\\theta}}\\left[\\exp \\left(CW_{i}{ }\\right)  W_{i}^{\\top}\\right]\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Let's look closer at the integrals. \n",
    "$$\n",
    "E_{\\tilde p_{\\theta}}\\left[\\exp \\left(CW_{i}{ }\\right)  W_{i}^{\\top}\\right] = (E_{\\tilde p_{\\theta}}\\left[\\exp \\left(u_k^{\\top} CW_{i}\\right)  W_{i}^{\\top}u_m\\right])_{1 \\leq k \\leq p, 1 \\leq m \\leq q}\n",
    "$$\n",
    "\n",
    "$$\\mathbb E_{\\tilde p_{\\theta}} \\left[ W_i^{\\top} \\right] = \\mathbb E_{\\tilde p_{\\theta}} \\left[ W_i^{\\top}u_m \\right]_{1 \\leq m \\leq q}\n",
    "$$\n",
    "\n",
    "We only need to approximate these integrals and we can compute the gradients.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the law g\n",
    "\n",
    "We want to get a good law $g$ for importance sampling. \n",
    "We will try to modify the model so that the law inside the integral are computable and we hope it will help. \n",
    "\n",
    "We look at the following model : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=O_i +  \\beta^{\\top}\\mathbf{x}_{i} +\\mathbf{C}W_i , \\quad i \\in 1, \\ldots, n \\\\\n",
    "\\operatorname{log} (Y_{i}) \\mid Z_{i} & \\sim \\mathcal{N}\\left( Z_{i}, \\operatorname{diag}(\\exp(-|O_i +\\beta ^{\\top} X_i |)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "We denote $D_i:= \\operatorname{diag}(\\exp(-|O_i +\\beta ^{\\top} X_i |)$.\n",
    "\n",
    "Here are the heuristics for choosing this model. \n",
    "First, we approximate the Poisson law with a gaussian: \n",
    "\n",
    "$$\\mathcal P (exp(Z_i)) \\approx \\mathcal N(\\exp(Z_i), \\operatorname{diag} (\\exp(Z_i))$$ \n",
    "\n",
    "\n",
    "\n",
    "Now, with some computation ( see for example the end of the notebook) \n",
    "\n",
    "we can do this approximation : \n",
    "\n",
    "$$ \\exp (\\mathcal N (Z_i, \\operatorname{diag}(\\exp(-|Z_i|)) \\approx \\mathcal N (\\exp(Z_i), \\operatorname{diag}(\\exp(Z_i))$$\n",
    "\n",
    "For computation purposes and to have a law that is computable, we need to remove $CW$ from the variance term inside the absolute value(to get back a gaussian when computing the law of the latent variables given the observation), which gives the heuristic. \n",
    "\n",
    "After some computation, we find that given this model, we can compute the law $W_i|Y_i$ : \n",
    "\n",
    "$$W_i|Y_i \\sim \\mathcal N((\\log(Y_i)-X_i\\beta)^{\\top}D^{-1}C, (C^{\\top}D^{-1}C + I)^{-1})$$ \n",
    "\n",
    "The better $g$ we can take ( to approximate $\\mathbb E _{p_{\\theta}(W_i|Y_i)}\\left[e^{ u_m^{\\top}CW_i}\\right]$ being the law  : $$g^{\\star}(x) = e^{ u_m^{\\top}Cx} p_{W_i|Y_i}(x) , x \\in R^q$$\n",
    "\n",
    "After some little computation we find that : \n",
    "\n",
    "$$ g^{\\star}(x) = \\mathcal N (x; (\\log(Y_i)-X_i\\beta)^{\\top}D^{-1}C + u_m^{\\top}C, (C^{\\top}D^{-1}C + I)^{-1})$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computation of the law of $W_i|Y_i$ for the modified model. \n",
    "\n",
    "We set $$\\mu_i := \\log(Y_i) -X_i\\beta$$\n",
    "\n",
    "Let's compute the law of the latent variables given the data.\n",
    "\n",
    "\\begin{aligned}\n",
    "p_{\\theta}\\left(W_{i} \\mid \\log \\left(Y_{i}\\right) \\right) & = p_{\\theta}\\left(\\log \\left(Y_{i}\\right) \\mid W_{i}\\right) p\\left(W_{i}\\right) \\\\\n",
    "& = C \\times \\exp \\left(-\\frac{1}{2}\\left(\\left(CW_{i}-\\mu_{i}\\right)^{T} D^{-1}\\left(CW_{i}-\\mu_{i}\\right)+W_{i}^{\\top} W_{i}\\right)\\right) \\\\\n",
    "& C \\times \\exp \\left(-\\frac{1}{2}\\left(W_{i}^{\\top}C^{\\top}D^{-1}CW_{i}-2\\mu_{i}^{\\top}D^{-1}CW_i+\\mu_i^{\\top}D^{-1}\\mu_i+W_{i}^{\\top} W_{i}\\right)\\right) \\\\\n",
    "& = C \\times \\exp \\left(-\\frac{1}{2}\\left(W_{i}^{\\top}\\left(C^{\\top}D^{-1}C+ I_q\\right)W_{i}\\right)+\\mu_{i}^{\\top}D^{-1}CW_i\\right)\n",
    "\\end{aligned}\n",
    "\n",
    "By identification : \n",
    "\n",
    "$$W_i |\\log(Y_i) \\sim \\mathcal N \\left(C^{\\top}D^{-1}\\mu_i, (C^{\\top}D^{-1}C+ I_q)^{-1} \\right) $$\n",
    "\n",
    "Then, \n",
    "\n",
    "$$ u_m^{\\top}C W_i |\\log(Y_i) \\sim \\mathcal N\\left(u_m^{\\top}CC^{\\top}D^{-1}\\mu_i, u_m^{\\top}C(C^{\\top}D^{-1}C+ I_q)^{-1}C^{\\top}u_m \\right)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sizes : \n",
    "\n",
    "$ Y : (n,p)$ \n",
    "\n",
    "$O : (n,p)$ \n",
    "\n",
    "$C :  (p,q)$ \n",
    "\n",
    "covariates ($x$) : $(n,d)$\n",
    "\n",
    "$\\beta : (d,p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### some functions I tried, might be useful but not clean. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_2D(f,x_abscisse, y_abscisse):\n",
    "    Z = np.zeros((len(x_abscisse), len(y_abscisse)))\n",
    "    maxi = 0\n",
    "    argmax = (0,0)\n",
    "    def f_(x,y):\n",
    "        return f(torch.tensor([x,y]))\n",
    "    total_mass = scipy.integrate.dblquad(f_,-10,10,-10,10)[0]\n",
    "    for i,x in enumerate(x_abscisse): \n",
    "        for j,y in enumerate(y_abscisse):\n",
    "            xy_ten = torch.tensor([x,y])\n",
    "            value = (f(xy_ten))\n",
    "            if value > maxi : \n",
    "                argmax = (i,j)\n",
    "                maxi = value\n",
    "            Z[i,j]= value\n",
    "            \n",
    "    \n",
    "    best_mean = torch.tensor([x_abscisse[argmax[0]],y_abscisse[argmax[1]]])\n",
    "    def gauss_density(Z):\n",
    "        Sig = (torch.diag(true_Sigma)**(-1))*torch.eye(2)\n",
    "        inv_Sig = torch.inverse(torch.eye(2)/3)\n",
    "        C = 2*math.pi*torch.sqrt(torch.det(Sig))\n",
    "        return 1/C*torch.exp(- 1/2*torch.matmul(torch.matmul(Z-best_mean, inv_Sig), (Z-best_mean).T).squeeze())\n",
    "    \n",
    "    gauss = np.zeros((len(x_abscisse), len(y_abscisse)))\n",
    "    for i,x in enumerate(x_abscisse): \n",
    "        for j,y in enumerate(y_abscisse):\n",
    "            xy_ten = torch.tensor([x,y])\n",
    "            gauss[i,j] = gauss_density(xy_ten)\n",
    "            \n",
    "            \n",
    "    fig = plt.figure(figsize = (19.5,9.5))\n",
    "    ax = fig.add_subplot(1,2,1, projection='3d')\n",
    "    X,Y = np.meshgrid(x_abscisse,y_abscisse)\n",
    "    ax.plot_surface(X, Y, (Z/total_mass).T, cmap = cm.coolwarm)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('z')\n",
    "    ax.set_title('Posterior distribution', fontsize = 15)\n",
    "    \n",
    "    ax_bis = fig.add_subplot(1,2,2, projection='3d')\n",
    "    ax_bis.set_xlabel('x')\n",
    "    ax_bis.set_ylabel('y')\n",
    "    ax_bis.set_zlabel('z')\n",
    "    ax_bis.set_title('Gaussian approximation distribution', fontsize = 15)\n",
    "\n",
    "    ax_bis.plot_surface(X, Y, gauss.T, cmap = cm.coolwarm)\n",
    "    plt.show()\n",
    "    #fig.savefig('Comparison distribution')\n",
    "    return Z,  \n",
    "    \n",
    "\n",
    "    \n",
    "def f_expmoment(Z): \n",
    "    return torch.exp(torch.sum(Z))*p(Z)\n",
    "\n",
    "def Z_density(Z,i):\n",
    "    XB = torch.matmul(covariates[i].unsqueeze(0),true_beta)\n",
    "    \n",
    "    log_ = torch.sum(-torch.exp(Z) + Y_sampled[i,:]*Z) - 1/2*torch.matmul(torch.matmul(Z-XB, torch.inverse(true_Sigma)), (Z-XB).T).squeeze()\n",
    "    return torch.exp(log_)                                               \n",
    "#plot = plot_2D(Z_density, np.linspace(-1,4,100),np.linspace(-3,2,100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "p_{\\theta}(Z_i \\mid Y_i )  & = \\frac{p_{\\theta}\\left(Y_{i} \\mid Z_{i}\\right) p\\left(Z_{i}\\right)}{p_{\\theta}(Y_i)} \\\\\n",
    "& \\propto  ~ p_{\\theta}\\left(Y_{i} \\mid Z_{i}\\right) p\\left(Z_{i}\\right) \\\\\n",
    "& \\propto ~ \\exp \\left(   \\sum_{j=1}^{p}-\\operatorname{exp}\\left(0_{i j}+Z_{i j}\\right)+Y_{i j}\\left(0_{i j}+Z_{i j}\\right)   -\\frac{1}{2} (Z_{i}-\\beta^{\\top}X_i)^{\\top} \\Sigma ^{-1} (Z_{i}-\\beta^{\\top}X_i) \\right) \\\\\n",
    "\\end{aligned}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_1D(f,i,j, abscisse_first, abscisse_second): \n",
    "    values_first  = list()\n",
    "    values_second = list()\n",
    "    def f_first(x): \n",
    "        return f(torch.tensor([x]), i ) \n",
    "    def f_second(x): \n",
    "        return f(torch.tensor([x]), j )\n",
    "    total_mass_first = scipy.integrate.quad(f_first,-10,10)[0]\n",
    "    total_mass_second = scipy.integrate.quad(f_second, -10,10)[0]\n",
    "    \n",
    "    \n",
    "    for w_first, w_second in zip(abscisse_first, abscisse_second) : \n",
    "        values_first.append(np.squeeze((f_first(w_first)).numpy())/total_mass_first)\n",
    "        values_second.append(np.squeeze((f_second(w_second)).numpy())/total_mass_second)\n",
    "    values_first = np.array(values_first)\n",
    "    values_second = np.array(values_second)\n",
    "    \n",
    "    argmax_first = abscisse_first[np.argmax(values_first)]\n",
    "    argmax_second = abscisse_second[np.argmax(values_second)]\n",
    "    gauss_first = []\n",
    "    gauss_second = []\n",
    "    sig_first = 0.1\n",
    "    sig_second = 0.6\n",
    "    def gauss_density(w, sig,argmax): \n",
    "        return 1/math.sqrt(2*math.pi*sig**2)*np.exp(-1/(2*sig**2)*(w-argmax)**2)\n",
    "    \n",
    "    for w_first, w_second in zip(abscisse_first, abscisse_second) :\n",
    "        gauss_first.append(gauss_density(w_first,sig_first, argmax_first))\n",
    "        gauss_second.append(gauss_density(w_second,sig_second, argmax_second))\n",
    "        \n",
    "    gauss_first = np.array(gauss_first)\n",
    "    gauss_second = np.array(gauss_second)\n",
    "    \n",
    "    \n",
    "    #gauss_first*= np.max(values_first)/np.max(gauss_first)\n",
    "    #gauss_second*= np.max(values_second)/np.max(gauss_second)*1.03\n",
    "    fig, axes = plt.subplots(1,2, figsize = (20,10))\n",
    "    axes[0].plot(abscisse_first, gauss_first, label = 'gaussian approximation')\n",
    "    axes[0].plot(abscisse_first,np.abs(values_first), label = 'Posterior distribution')\n",
    "    axes[0].legend()\n",
    "    axes[1].plot(abscisse_second, gauss_second, label = 'gaussian approximation')\n",
    "    axes[1].plot(abscisse_second,np.abs(values_second), label = 'Posterior distribution')\n",
    "    axes[1].legend()\n",
    "    plt.show()\n",
    "    fig.savefig('Comparison 1D')\n",
    "    return argmax_first,argmax_second"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abscisse' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-3345a1dc3481>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_2D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0my\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mabscisse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinspace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'abscisse' is not defined"
     ]
    }
   ],
   "source": [
    "plot_2D(lambda x,y: x*y ,abscisse,torch.linspace(-5,5,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_1D(Z_density,13,28, np.linspace(0,8,200), np.linspace(-4,4,200))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is if we take only one sample $Y_i$. If we take the whole dataset (or a mini-batch), we get (writed in matrix form) :\n",
    "\n",
    "$$\n",
    "\\nabla_{\\beta} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[-X^{\\top} \\exp \\left(0+X\\beta+W_k^{\\top} C^{\\top}\\right)+ X^{\\top}Y\\right]\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\nabla_{C} p_{\\theta}\\left(Y \\mid W_k\\right)=p_{\\theta}\\left(Y \\mid W_k\\right)\\left[Y-\\exp \\left(0+X \\beta+W_k^{\\top} C^{\\top} \\right)\\right]\n",
    "^{\\top}W_k \n",
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
