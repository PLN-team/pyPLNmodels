{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on module utils:\n",
      "\n",
      "NAME\n",
      "    utils - implements all the functions needed for the other modules.\n",
      "\n",
      "DESCRIPTION\n",
      "    Created on Wed Nov  17 09:39:30 2021\n",
      "    \n",
      "    \n",
      "    \n",
      "    @author: Bbatardiere\n",
      "\n",
      "CLASSES\n",
      "    builtins.object\n",
      "        Poisson_reg\n",
      "        sample_PLN\n",
      "    \n",
      "    class Poisson_reg(builtins.object)\n",
      "     |  Poisson regressor class. The purpose of this class is to initialize the variable beta of the PLN model.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  fit(self, Y, O, X, Niter_max=300, tol=0.001, lr=0.005, verbose=False)\n",
      "     |      We run a gradient ascent to maximize the log likelihood, using pytorch autodifferentiation. The log likelihood considered is the one from a poisson regression model. It is the same as PLN without the latent layer Z.We are only trying to have a good guess of beta before doing anything. \n",
      "     |      \n",
      "     |      args : \n",
      "     |              '0' : torch.tensor. Offset, size (n,p)\n",
      "     |              'X' : torch.tensor. Covariates, size (n,d)\n",
      "     |              'Y' : torch.tensor. Samples with size (n,p)\n",
      "     |              'Niter_max' :int  the maximum number of iteration we are ready to do \n",
      "     |              'tol' : non negative float. the tolerance criteria. We will stop if the norm of the gradient is less than \n",
      "     |                     or equal to this threshold\n",
      "     |              'lr' : positive float. Learning rate for the gradient ascent\n",
      "     |              'verbose' : bool. if True, will print some stats.  \n",
      "     |              \n",
      "     |      returns : None but update the parameter beta\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "    \n",
      "    class sample_PLN(builtins.object)\n",
      "     |  sample_PLN(ZI=False)\n",
      "     |  \n",
      "     |  simple class to sample some variables with the PLN model. \n",
      "     |  The main method is the sample one, however we can also plot the data calling the plot_Y method. \n",
      "     |  The method conditional prior should not be used and have not been tested properly.\n",
      "     |  \n",
      "     |  Methods defined here:\n",
      "     |  \n",
      "     |  __init__(self, ZI=False)\n",
      "     |      Initialize self.  See help(type(self)) for accurate signature.\n",
      "     |  \n",
      "     |  sample(self, Sigma, beta, O, covariates, B_zero=None)\n",
      "     |      sample Poisson log Normal variables. \n",
      "     |      The number of samples is the the first size of O, the number of species\n",
      "     |      considered is the second size of O\n",
      "     |      The number of covariates considered is the first size of beta.\n",
      "     |  \n",
      "     |  ----------------------------------------------------------------------\n",
      "     |  Data descriptors defined here:\n",
      "     |  \n",
      "     |  __dict__\n",
      "     |      dictionary for instance variables (if defined)\n",
      "     |  \n",
      "     |  __weakref__\n",
      "     |      list of weak references to the object (if defined)\n",
      "\n",
      "FUNCTIONS\n",
      "    C_from_Sigma(Sigma, q)\n",
      "        get the best matrix of size (p,q) when Sigma is of size (p,p). i.e. reduces norm(Sigma-C@C.T)\n",
      "        args : \n",
      "            Sigma : np.array of size (p,p). Should be positive definite and symmetric.\n",
      "            q : int. The number of columns you want in your matrix C. \n",
      "            \n",
      "        returns : C_reduct : np.array of size (p,q) that contains the q eigenvectors with largest eigenvalues.\n",
      "    \n",
      "    MSE(tens)\n",
      "        computes the Mean Squared Error of a torch.tensor\n",
      "    \n",
      "    M_x(t, mu, Sigma)\n",
      "    \n",
      "    build_block_Sigma(p, block_size)\n",
      "        build a matrix per block of size (p,p). There will be p//block_size+1 blocks of size block_size.\n",
      "        The first p//k ones will be the same size. The last one will be different (size (0,0) if p%block_size = 0)\n",
      "    \n",
      "    compute_l(O, X, Y, beta)\n",
      "    \n",
      "    grad_poiss_beta(O, X, Y, beta)\n",
      "    \n",
      "    init_C(O, X, Y, beta, q)\n",
      "        Inititalization for C for the PLN model. We first get a guess for Sigma that is easier and then takes the q largest eigenvectors to get C.\n",
      "        args : \n",
      "                '0' : torch.tensor. Offset, size (n,p)\n",
      "                'X' : torch.tensor. Covariates, size (n,d)\n",
      "                'Y' : torch.tensor. Samples with size (n,p)\n",
      "                'beta' : torch.tensor of size (d,p)\n",
      "                'q' : int. The dimension of the latent space, i.e. the reducted dimension. \n",
      "        returns : the initialization of C.\n",
      "    \n",
      "    init_Sigma(O, X, Y, beta)\n",
      "        Initialization for Sigma for the PLN model. We take the log of Y ( careful when Y=0), remopved the covariates effects X@beta and then do as a MLE for Gaussians samples. \n",
      "        args : \n",
      "                '0' : torch.tensor. Offset, size (n,p)\n",
      "                'X' : torch.tensor. Covariates, size (n,d)\n",
      "                'Y' : torch.tensor. Samples with size (n,p)\n",
      "                'beta' : torch.tensor of size (d,p)\n",
      "    \n",
      "    log_stirling(n_)\n",
      "        this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
      "        numerical infinite values of n!. It can also take tensors.\n",
      "        \n",
      "        args : \n",
      "             n_ : tensor. \n",
      "        return : an approximation of log(n!)\n",
      "    \n",
      "    logit(x)\n",
      "    \n",
      "    logit_(x)\n",
      "    \n",
      "    sample(O, X, true_beta)\n",
      "    \n",
      "    sigmoid(x)\n",
      "\n",
      "DATA\n",
      "    __authors__ = ('Bastien Batardiere', 'Julien Chiquet', 'Joon Kwon')\n",
      "    device = device(type='cpu')\n",
      "\n",
      "FILE\n",
      "    /home/bbatardiere/Documents/PLNpy/IMPS/utils.py\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import utils\n",
    "help(utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
