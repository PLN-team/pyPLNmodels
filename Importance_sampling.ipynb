{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device :  cpu\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.linalg import toeplitz\n",
    "from utils import sample_PLN\n",
    "import scipy.linalg as SLA\n",
    "import torch.linalg as TLA\n",
    "import scipy\n",
    "from fastPLN import fastPLN\n",
    "from scipy.special import factorial\n",
    "import math\n",
    "import seaborn as sns \n",
    "import torch \n",
    "from pandas import read_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "\\begin{aligned}\n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right), \\text { iid, } \\quad i=1, \\ldots, n \\\\\n",
    "Z_{i} &=\\mathbf{x}_{i}\\beta +W_{i}\\mathbf{C}^{\\top}, \\quad i \\in 1, \\ldots, n \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{i j}+Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to maximize the log likelihood with respect to $\\theta$ : \n",
    "\n",
    "\n",
    "$$\\max _{\\theta}log P_{\\theta}(Y)$$\n",
    "\n",
    "\n",
    "In the MC_PLN.ipynb we tried to do a Monte Carlo to maximize the likelihood but it resulted in numerical 0. We need to derive the gradients with respect to theta : \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\nabla_{\\theta} \\log p_{\\theta}(Y)&= \\frac{\\nabla_{\\theta} p_{\\theta}(Y)}{p_{\\theta}(Y)} \\\\\n",
    "&=\\frac{\\nabla_{\\theta} \\int p_{\\theta}(Y \\mid W) p(W) d W}{\\int p_{\\theta}(Y \\mid W) p(W) d W}\\\\\n",
    "&=\\frac{\\int \\nabla_{\\theta} p_{\\theta}(Y | W) p(W) d W}{\\int  p_{\\theta}(Y|W) p(W) d w}\\\\\n",
    "&= \\frac{\\int\\left(\\nabla_{\\theta} \\ln p_{\\theta}(Y \\mid W)\\right) p_{\\theta}(Y| W) p(W) d W}{\\int p_{\\theta}(Y \\mid W)p(W) d W}\\\\\n",
    "&=\\int \\nabla_{\\theta} \\ln p_{\\theta}(Y \\mid W) \\tilde{p}_{\\theta}(W) dW \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "$$\\tilde p_{\\theta}(W):=\\frac{p_{\\theta}(Y \\mid W) p(W) d W}{\\int p_{\\theta}(Y \\mid W) p(W) dW}$$\n",
    "\n",
    "We only know the numerator of $\\tilde p_{\\theta}(W)$. Thus we need to use importance sampling, which consits in the following : \n",
    "\n",
    "Let $g$ be a probability density such that $x \\in \\operatorname{supp}(\\tilde p_{\\theta}) \\implies g(x)>0$. We denote $(V_i)_i \\overset{iid}{\\sim} g$. \n",
    "\n",
    "We define : \n",
    "\n",
    "$$\n",
    "w_{i}^{(u)}=\\frac{p_{\\theta}\\left(Y|V_i\\right)}{g\\left(V_{i}\\right)}\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\tilde{w}_{i}^{(u)}=\\frac{w_{i}^{(u)}}{\\sum_{\\ell=1}^{n} w_{\\ell}^{(u)}}\n",
    "$$\n",
    "\n",
    "\n",
    "Then, \n",
    "\n",
    "$$\n",
    "\\hat{I}_{n}^{I S, u}:=\\sum_{i=1}^{n} \\tilde{w}_{i}^{(u)} \\nabla_{\\theta} \\ln p_{\\theta}(Y \\mid V_i)\n",
    " \\stackrel{\\text { Proba }}{\\longrightarrow} \\nabla_{\\theta} \\log p_{\\theta}(Y)\n",
    "$$\n",
    "\n",
    "We need to choose carefully the density $g$ i.e. where $\\tilde p_{\\theta}\\times \\nabla_{\\theta} \\log p_{\\theta}$ has a lot of mass. We can see that : \n",
    "\n",
    "$$\\tilde p_{\\theta}(W):=\\frac{p_{\\theta}(Y \\mid W) p(W) d W}{\\int p_{\\theta}(Y \\mid W) p(W) dW}=\\frac{p_{\\theta}\\left(W|Y) P_{\\theta}(Y)\\right.}{\\int p_{\\theta}(W \\mid Y) p_{\\theta}(Y) d W}=\\frac{p_{\\theta}(W \\mid Y)}{\\int p_{\\theta}(W | Y) dW }$$\n",
    "\n",
    "Since $W|Y$ can be well-approximated with a Gaussian, we will choose a Gaussian for $g$.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_block_Sigma(p,k): \n",
    "    '''\n",
    "    build a matrix per block of size (p,p). There will be k+1 blocks of size p//k.\n",
    "    The first k ones will be the same size. The last one will be smaller (size (0,0) if k%p = 0)\n",
    "    '''\n",
    "    np.random.seed(0)\n",
    "    alea = np.random.randn(k+1)**2+1# will multiply each block by some random quantities \n",
    "    Sigma = np.zeros((p,p))\n",
    "    block_size,last_block_size = p//k, p%k\n",
    "    for i in range(k): \n",
    "        Sigma[i*block_size : (i+1)*block_size ,i*block_size : (i+1)*block_size] = alea[i]*toeplitz(0.95**np.arange(block_size))\n",
    "    if last_block_size >0 :\n",
    "        Sigma[-last_block_size:,-last_block_size:] = alea[k]*toeplitz(0.98**np.arange(last_block_size))\n",
    "    return Sigma+0.1*toeplitz(0.95**np.arange(p))\n",
    "\n",
    "\n",
    "def C_from_Sigma(Sigma,q): \n",
    "    w,v = SLA.eigh(Sigma)\n",
    "    C_reduct = v[:,-q:]@np.diag(np.sqrt(w[-q:]))\n",
    "    return C_reduct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_stirling(n_):\n",
    "    '''\n",
    "    this function computes log(n!) even for n large. We use the Stirling formula to avoid \n",
    "    numerical infinite values of n!. It can also take tensors.\n",
    "    \n",
    "    args : \n",
    "         n_ : tensor. \n",
    "    return : an approximation of log(n!)\n",
    "    '''\n",
    "    n = torch.clone(n_) #clone the tensor by precaution\n",
    "    n+= (n==0) # replace the 0 with 1. It changes anything since 0! = 1! \n",
    "    return torch.log(torch.sqrt(2*np.pi*n))+n*torch.log(n/math.exp(1)) #Stirling formula"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMPS_PLNPCA(): \n",
    "    def __init__(self,q, batch_size): \n",
    "        self.q = q\n",
    "        self.batch_size = batch_size \n",
    "        \n",
    "    def get_Sigma(self):\n",
    "        '''\n",
    "        simple function to get Sigma\n",
    "        '''\n",
    "        return self.C@(self.C.T)\n",
    "    \n",
    "    def get_batch(self,batch_size): \n",
    "        '''\n",
    "        get the batches required to do a  minibatch gradient ascent.  \n",
    "        \n",
    "        args : \n",
    "                'batch_size' int.  the batch size you want. \n",
    "                \n",
    "        returns : a generator. Will generate n/batch_size samples of size batch_size (except the last one \n",
    "                    since the rest of the division is not always an integer)\n",
    "                    \n",
    "        '''\n",
    "        #np.random.seed(0)\n",
    "        indices = np.arange(self.n)\n",
    "        np.random.shuffle(indices)\n",
    "        self.batch_size = batch_size \n",
    "        nb_full_batch, last_batch_size  = self.n//self.batch_size, self.n % self.batch_size  \n",
    "        for i in range(nb_full_batch): \n",
    "            yield   (self.Y[indices[i*self.batch_size: (i+1)*self.batch_size]], \n",
    "                    self.covariates[indices[i*self.batch_size: (i+1)*self.batch_size]],\n",
    "                    self.O[indices[i*self.batch_size: (i+1)*self.batch_size]]) \n",
    "                        \n",
    "                  \n",
    "        if last_batch_size != 0 : \n",
    "            self.batch_size = last_batch_size\n",
    "            yield   (self.Y[indices[-last_batch_size:]], \n",
    "                    self.covariates[indices[-last_batch_size:]],\n",
    "                    self.O[indices[-last_batch_size:]])\n",
    "            \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
