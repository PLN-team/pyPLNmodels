{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage and main fitting functions\n",
    "\n",
    "The package comes with an ecological dataset to present the functionality. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick mathematical description of the package. \n",
    "\n",
    "The package tries to infer the parameters of two models: \n",
    "\n",
    "- Poisson Log Normal-Principal Composent Analysis model (PLN-PCA)  \n",
    "- Zero Inflated Poisson Log-Normal model (ZIPLN) \n",
    "\n",
    "\n",
    "### PLN-PCA\n",
    "\n",
    "We consider a PLN-PCA model described as the following : \n",
    "\n",
    "- Consider $n$ samples $(i=1 \\ldots n)$\n",
    "\n",
    "- Measure $x_{i}=\\left(x_{i h}\\right)_{1 \\leq h \\leq d}$ :\n",
    "$x_{i h}=$ (covariate) for sample $i$\n",
    "(altitude, temperature, categorical covariate, ...)\n",
    "\n",
    "- Consider $p$ features (genes) $(j=1 \\ldots p)$ Measure $Y=\\left(Y_{i j}\\right)_{1 \\leq i \\leq n, 1 \\leq j \\leq p}$ :\n",
    "\n",
    "- Measure $Y = Y_{i j}=$ number of times the feature $j$ is observed in sample $i$. \n",
    "\n",
    "- Associate a random vector $Z_{i}$ with each sample.\n",
    "- Assume that the unknown $\\left(W_{i}\\right)_{1 \\leq i \\leq n}$ are independant and living in a space of dimension $q\\leq p$  such that:\n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right)  \\\\\n",
    "Z_{i} &=\\beta^{\\top}\\mathbf{x}_{i} +\\mathbf{C}W_i  \\in \\mathbb R^p \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim \\mathcal{P}\\left(\\exp \\left(o_{ij} + Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "and $C\\in \\mathbb R^{p\\times q}$, $\\beta \\in \\mathbb R^{d\\times p}$ . \n",
    "\n",
    "Where $O = (o_{ij})_{1\\leq i\\leq n, 1\\leq j\\leq p}$ are known offsets. \n",
    "\n",
    "The unknown parameter is $\\theta = (\\Sigma,\\beta)$. The latent variable of the model can be seen as $Z$ or $W$. \n",
    "\n",
    "The PCA (Principal Component Analysis) term in the name of the model comes from a ranq $q$ approximation of $\\Sigma$ with $CC^{\\top}$.\n",
    "When $p=q$, we are not doing PCA, so that we wil call this model PLN. \n",
    "\n",
    "The goal of this package is to retrieve $\\theta$ from the observed data $(Y, O, X)$. To do so, we will try to maximize the log likelihood of the model:\n",
    "$$p_{\\theta}(Y_i)  = \\int_{\\mathbb R^q} p_{\\theta}(Y_i,W)dW $$\n",
    "\n",
    "However, almost any integrals involving the law of the complete data is unreachable, so that we can't perform either gradient ascent algorithms or EM algorithm.   \n",
    "We adopt two different approaches to circumvent this problem: \n",
    "- Variational approximation of the latent layer (Variational EM)\n",
    "- Importance sampling based algorithm, using a gradient ascent method.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Variational approach\n",
    "\n",
    "We want here to use the EM algorithm, but the E step is unreachable, since the law $W|Y_i$ or $Z|Y_i$ is unknown and can't be integrated out. We thus choose to approximate the law of $W|Y_i$ with a law $q_i(Z)$, where $q_i$ is taken among a family of law. We thus change the objective function: \n",
    "\n",
    "$$\\begin{align} J_Y(\\theta,q) & = \\frac 1 n \\sum _{i = 1}^n J_{Y_i}(\\theta, q_i) \\\\ \n",
    "J_{Y_i}(\\theta, q_i)& =\\log p_{\\theta}(Y_i)-K L\\left[q_i(Z_i) \\|p_{\\theta}(Z_i \\mid Y_i)\\right]\\\\ \n",
    "& = \\mathbb{E}_{q_i}\\left[\\log p_{\\theta}(Y_i, Z_i)\\right] \\underbrace{-\\mathbb{E}_{q_i}[\\log q_i(Z_i)]}_{\\text {entropy } \\mathcal{H}(q_i)} \\\\\n",
    "\\end{align}$$\n",
    "\n",
    "\n",
    "We choose $q_i$ in a family distribution : \n",
    "\n",
    "$$\n",
    "q_i \\in \\mathcal{Q}_{\\text {diag}}=\\{\n",
    " \\mathcal{N}\\left(M_{i}, \\operatorname{diag} (S_{i}\\odot S_i ))\n",
    ", M_i \\in \\mathbb{M} ^p, S_i \\in \\mathbb{R} ^p\\right\\}\n",
    "$$\n",
    "\n",
    "We choose such a family distribution since $W$ is gaussian, so that $W|Y_i$ may be well approximated with a gaussian.However, taking a diagonal matrix as covariance breaks the dependecy induced by Y_i. \n",
    "\n",
    "We can prove that $J_{Y_i}(\\theta, q_i) \\leq p_{\\theta} (Y_i) \\quad \\forall q_i$. The quantity $J_{Y}(\\theta, q)$ is called the ELBO (Evidence Lower BOund).  \n",
    "\n",
    "##### Variational EM \n",
    "\n",
    "Given an intialisation $(\\theta^0, q^0)$, the variational EM aims at maximizing the ELBO alternating between two steps: \n",
    "\n",
    "-  VE step: update  $q$\n",
    "$$\n",
    "q^{t+1}=\\underset{q \\in \\mathcal{Q}_{gauss}}{\\arg \\max } J_Y(\\theta^{t}, q)\n",
    "$$\n",
    "- M step : update $\\theta$\n",
    "$$\n",
    "\\theta^{t+1}=\\underset{\\theta}{\\arg \\max } J_Y(\\theta, q^{t+1})\n",
    "$$\n",
    "Each step is an optimisation problem that needs to be solved using analytical forms or gradient ascent. Note that $q$ is completely determined by $M = (M_i)_{1 \\leq i \\leq n } \\in \\mathbb R ^{n\\times q}$ and $S = (S_i)_{1 \\leq i \\leq n } \\in \\mathbb R ^{n\\times q}$, so that $J$ is a function of $(M, S, \\beta, \\Sigma)$. $M$ and $S$ are the variational parameters, $\\beta$ and $\\Sigma$ are the model parameters.  \n",
    "\n",
    "\n",
    "##### Case $p = q$\n",
    "The case $p=q$ is not doing any reduction dimension, but is very fast to compute. \n",
    "When $ p =q $, computations show that the M-step is straightforward as we can update $\\Sigma$ and $\\beta$ with an analytical form : \n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\Sigma^{(t+1)} & = \\frac{1}{n} \\sum_{i}\\left(\\left((M^{(t)}-X\\beta)_{i} (M^{(t)}-X\\beta)_{i}\\right)^{\\top}+S^{(t)}_{i}\\right)\\\\\n",
    "\\beta^{(t+1)} &= (X^{\\top}X)^{-1}X^{\\top}M^{(t)} \\\\ \n",
    "\\end{aligned}\n",
    "$$\n",
    "This results in a fast algorithm, since we only need to go a gradient ascent on the variational parameters $M$ and $S$. Practice shows that we only need to do one gradient step of $M$ and $S$, update $\\beta$ and $\\Sigma$ with their closed form, then re-perform a gradient step on $M$ and $S$ ... \n",
    "\n",
    "\n",
    "##### Case $p <q$\n",
    "\n",
    "When $p<q$, we do not have any analytical form, and we are forced to perform gradient ascent on all the parameters. Practice shows that we can perform a gradient ascent on all the parameters at a time (doing each VE step and M step perfectly is quite inefficient). \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importance sampling based algorithm \n",
    "\n",
    "In this section, we try to estimate the gradients with respect to $\\theta = (C, \\beta) $. \n",
    "\n",
    "\n",
    "We can use importance sampling to estimate the likelihood: \n",
    "\n",
    " $$p_{\\theta}(Y_i) = \\int \\tilde p_{\\theta}^{(u)}(W) \\mathrm dW \\approx \\frac 1 {n_s} \\sum_{k=1}^{n_s} \\frac {\\tilde p_{\\theta}^{(u)}(V_k)}{g(V_k)}, ~ ~ ~(V_{k})_{1 \\leq k \\leq n_s} \\overset{iid}{\\sim} g$$\n",
    " \n",
    "where $g$ is the importance law, $n_s$ is the sampling effort and  \n",
    "\n",
    "\n",
    "$$\\begin{array}{ll}\n",
    "\\tilde p_{\\theta}^{(u)}\\ :& \\mathbb R^{q}  \\to  \\mathbb R^+  \\\\\n",
    " & W \\mapsto p_{\\theta}(Y_i| W) p(W) \\\\\n",
    "\\end{array}$$\n",
    "\n",
    "To learn more about the (crucial) choice of $g$, please see REF.\n",
    "\n",
    "One can do the following approximation:\n",
    "\n",
    "\n",
    "  $$\\begin{equation}\\label{one integral}\n",
    "  \\nabla _{\\theta} \\operatorname{log} p_{\\theta}(Y_i) \\approx \\nabla_{\\theta} \\operatorname{log}\\left(\\frac 1 {n_s} \\sum_{k=1}^{n_s} \\frac {\\tilde p_{\\theta}^{(u)}(V_k)}{g(V_k)}\\right)\\end{equation}$$\n",
    "  \n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ZIPLN \n",
    "\n",
    "ZIPLN model is a modified PLN model that tries to explain the zero inflated datasets. Basically, we add a latent variable $\\xi$ parametrized by $B^0$ that will force some components of $Y$ to be zero. The model is the following : \n",
    "\n",
    "$$\n",
    "\\begin{aligned} \n",
    "W_{i} & \\sim \\mathcal{N}\\left(0, I_{q}\\right)  \\\\\n",
    "Z_{i} &=\\beta^{\\top}\\mathbf{x}_{i} +\\mathbf{C}W_i  \\in \\mathbb R^p \\\\\n",
    "\\xi _{ij} &   \\sim \\mathcal{B}\\left(\\operatorname{logit}^{-1}\\left(\\mathbf x_{i}^{\\top} B_{j}^{0}\\right)\\right) \\in \\mathbb R \\\\\n",
    "Y_{i j} \\mid Z_{i j} & \\sim (1-\\xi_{ij})\\mathcal{P}\\left(\\exp \\left(o_{ij} + Z_{i j}\\right)\\right)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "\n",
    "$\n",
    "\\text { We are interested in inferring } \\theta=\\left(\\boldsymbol{\\Sigma}, \\boldsymbol{\\beta}, \\boldsymbol{B}^{0}\\right) \\in \\mathbb{S}_{p}^{++} \\times \\mathcal{M}_{p, d}(\\mathbb{R}) \\times \\mathcal{M}_{p, d}(\\mathbb{R}) \\text {,   where }\\Sigma = CC^{\\top}\n",
    "$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
